{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wvc0hynpbwb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Initializing disk-based OGBN-MAG dataset...\n",
      "üì• Preparing memory-mapped OGBN-MAG data...\n",
      "‚úÖ Memory-mapped data already exists\n",
      "üìä Initializing HeteroData with all node and edge types...\n",
      "‚úÖ HeteroData metadata:\n",
      "   Node types: ['paper', 'author', 'field_of_study']\n",
      "   Edge types: [('author', 'writes', 'paper'), ('paper', 'written_by', 'author'), ('paper', 'has_topic', 'field_of_study'), ('field_of_study', 'topic_of', 'paper'), ('paper', 'cites', 'paper')]\n",
      "\n",
      "‚úÖ Disk-based data ready!\n",
      "   Papers: 736,389 (on disk)\n",
      "   Authors: 1,134,649\n",
      "   Fields: 59,965\n",
      "   Classes: 349\n",
      "   Memory usage: Minimal - data remains on disk\n"
     ]
    }
   ],
   "source": [
    "# Data Loading Cell - Memory-mapped loading, no data in RAM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HGTConv\n",
    "from torch_geometric.data import HeteroData\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import gc\n",
    "import warnings\n",
    "import h5py\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DiskBasedOGBNMAG:\n",
    "    \"\"\"Memory-efficient OGBN-MAG dataset that keeps data on disk\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='./data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.cache_dir = os.path.join(data_dir, 'ogbn_mag_cache')\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Prepare data files\n",
    "        self._prepare_data()\n",
    "        \n",
    "        # Open memory-mapped files\n",
    "        self._open_mmap_files()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Convert raw OGBN-MAG files to memory-mapped format if needed\"\"\"\n",
    "        print(\"üì• Preparing memory-mapped OGBN-MAG data...\")\n",
    "        \n",
    "        # Check if already prepared\n",
    "        if os.path.exists(os.path.join(self.cache_dir, 'metadata.npz')):\n",
    "            print(\"‚úÖ Memory-mapped data already exists\")\n",
    "            return\n",
    "            \n",
    "        # Ensure raw data exists\n",
    "        ogbn_dir = os.path.join(self.data_dir, 'ogbn_mag', 'raw')\n",
    "        if not os.path.exists(ogbn_dir):\n",
    "            print(\"üì¶ Downloading OGBN-MAG...\")\n",
    "            from ogb.nodeproppred import PygNodePropPredDataset\n",
    "            temp_dataset = PygNodePropPredDataset('ogbn-mag', root=self.data_dir)\n",
    "            del temp_dataset\n",
    "            gc.collect()\n",
    "        \n",
    "        print(\"üîÑ Converting to memory-mapped format...\")\n",
    "        \n",
    "        # Process paper features\n",
    "        feat_file = os.path.join(ogbn_dir, 'node-feat', 'paper', 'node-feat.csv.gz')\n",
    "        with gzip.open(feat_file, 'rt') as f:\n",
    "            paper_features = pd.read_csv(f, header=None).values.astype(np.float32)\n",
    "        \n",
    "        # Save as memory-mapped\n",
    "        mmap_feat = np.memmap(os.path.join(self.cache_dir, 'paper_features.dat'),\n",
    "                             dtype='float32', mode='w+', shape=paper_features.shape)\n",
    "        mmap_feat[:] = paper_features\n",
    "        mmap_feat.flush()\n",
    "        del paper_features, mmap_feat\n",
    "        \n",
    "        # Process labels\n",
    "        label_file = os.path.join(ogbn_dir, 'node-label', 'paper', 'node-label.csv.gz')\n",
    "        with gzip.open(label_file, 'rt') as f:\n",
    "            paper_labels = pd.read_csv(f, header=None).values.flatten().astype(np.int64)\n",
    "        \n",
    "        mmap_labels = np.memmap(os.path.join(self.cache_dir, 'paper_labels.dat'),\n",
    "                               dtype='int64', mode='w+', shape=paper_labels.shape)\n",
    "        mmap_labels[:] = paper_labels\n",
    "        mmap_labels.flush()\n",
    "        \n",
    "        # Process edges in chunks\n",
    "        edge_files = {\n",
    "            'cite': ('paper___cites___paper', 'edge.csv.gz'),\n",
    "            'author': ('author___writes___paper', 'edge.csv.gz'),\n",
    "            'field': ('paper___has_topic___field_of_study', 'edge.csv.gz')\n",
    "        }\n",
    "        \n",
    "        edge_counts = {}\n",
    "        for edge_type, (rel_dir, filename) in edge_files.items():\n",
    "            edge_file = os.path.join(ogbn_dir, 'relations', rel_dir, filename)\n",
    "            with gzip.open(edge_file, 'rt') as f:\n",
    "                edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n",
    "            \n",
    "            # Save edges\n",
    "            mmap_edges = np.memmap(os.path.join(self.cache_dir, f'{edge_type}_edges.dat'),\n",
    "                                  dtype='int64', mode='w+', shape=edges.shape)\n",
    "            mmap_edges[:] = edges\n",
    "            mmap_edges.flush()\n",
    "            edge_counts[edge_type] = edges.shape[1]\n",
    "            del edges, mmap_edges\n",
    "        \n",
    "        # Calculate metadata\n",
    "        num_papers = len(paper_labels)\n",
    "        num_authors = 1134649  # From OGBN-MAG stats\n",
    "        num_fields = 59965\n",
    "        num_classes = int(paper_labels.max()) + 1\n",
    "        feat_dim = mmap_feat.shape[1] if 'mmap_feat' in locals() else 128\n",
    "        \n",
    "        # Load splits\n",
    "        split_dir = os.path.join(self.data_dir, 'ogbn_mag', 'split', 'time')\n",
    "        if os.path.exists(split_dir):\n",
    "            print(\"  Loading official splits...\")\n",
    "            train_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'train.csv.gz'), \n",
    "                                  header=None).values.flatten()\n",
    "            val_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'valid.csv.gz'), \n",
    "                                header=None).values.flatten()\n",
    "            test_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'test.csv.gz'), \n",
    "                                 header=None).values.flatten()\n",
    "        else:\n",
    "            indices = np.random.RandomState(42).permutation(num_papers)\n",
    "            train_size = int(0.8 * num_papers)\n",
    "            val_size = int(0.1 * num_papers)\n",
    "            train_idx = indices[:train_size]\n",
    "            val_idx = indices[train_size:train_size + val_size]\n",
    "            test_idx = indices[train_size + val_size:]\n",
    "        \n",
    "        # Save metadata\n",
    "        np.savez(os.path.join(self.cache_dir, 'metadata.npz'),\n",
    "                num_papers=num_papers,\n",
    "                num_authors=num_authors,\n",
    "                num_fields=num_fields,\n",
    "                num_classes=num_classes,\n",
    "                feat_dim=feat_dim,\n",
    "                train_idx=train_idx,\n",
    "                val_idx=val_idx,\n",
    "                test_idx=test_idx,\n",
    "                edge_counts=edge_counts)\n",
    "        \n",
    "        print(\"‚úÖ Memory-mapped data prepared!\")\n",
    "        gc.collect()\n",
    "    \n",
    "    def _open_mmap_files(self):\n",
    "        \"\"\"Open memory-mapped files for reading\"\"\"\n",
    "        # Load metadata\n",
    "        metadata = np.load(os.path.join(self.cache_dir, 'metadata.npz'), allow_pickle=True)\n",
    "        self.num_papers = int(metadata['num_papers'])\n",
    "        self.num_authors = int(metadata['num_authors'])\n",
    "        self.num_fields = int(metadata['num_fields'])\n",
    "        self.num_classes = int(metadata['num_classes'])\n",
    "        self.feat_dim = int(metadata['feat_dim'])\n",
    "        self.train_idx = torch.from_numpy(metadata['train_idx'])\n",
    "        self.val_idx = torch.from_numpy(metadata['val_idx'])\n",
    "        self.test_idx = torch.from_numpy(metadata['test_idx'])\n",
    "        \n",
    "        # Open memory-mapped arrays (read-only)\n",
    "        self.paper_features = np.memmap(os.path.join(self.cache_dir, 'paper_features.dat'),\n",
    "                                       dtype='float32', mode='r', \n",
    "                                       shape=(self.num_papers, self.feat_dim))\n",
    "        \n",
    "        self.paper_labels = np.memmap(os.path.join(self.cache_dir, 'paper_labels.dat'),\n",
    "                                     dtype='int64', mode='r', shape=(self.num_papers,))\n",
    "        \n",
    "        # Open edge files\n",
    "        edge_counts = metadata['edge_counts'].item()\n",
    "        self.cite_edges = np.memmap(os.path.join(self.cache_dir, 'cite_edges.dat'),\n",
    "                                   dtype='int64', mode='r', shape=(2, edge_counts['cite']))\n",
    "        self.author_edges = np.memmap(os.path.join(self.cache_dir, 'author_edges.dat'),\n",
    "                                     dtype='int64', mode='r', shape=(2, edge_counts['author']))\n",
    "        self.field_edges = np.memmap(os.path.join(self.cache_dir, 'field_edges.dat'),\n",
    "                                    dtype='int64', mode='r', shape=(2, edge_counts['field']))\n",
    "        \n",
    "    def get_paper_batch(self, indices):\n",
    "        \"\"\"Load a batch of papers from disk\"\"\"\n",
    "        # Convert to numpy array for indexing\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.numpy()\n",
    "        \n",
    "        # Load only requested features and labels\n",
    "        features = torch.from_numpy(self.paper_features[indices].copy())\n",
    "        labels = torch.from_numpy(self.paper_labels[indices].copy())\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    def get_edges_for_nodes(self, node_indices, edge_type='cite'):\n",
    "        \"\"\"Get edges connected to specific nodes\"\"\"\n",
    "        if isinstance(node_indices, torch.Tensor):\n",
    "            node_indices = node_indices.numpy()\n",
    "        \n",
    "        # Select appropriate edge array\n",
    "        if edge_type == 'cite':\n",
    "            edges = self.cite_edges\n",
    "        elif edge_type == 'author':\n",
    "            edges = self.author_edges\n",
    "        else:\n",
    "            edges = self.field_edges\n",
    "        \n",
    "        # Find edges involving these nodes (this is still memory intensive for large graphs)\n",
    "        # In production, you'd want an index structure for this\n",
    "        node_set = set(node_indices.tolist())\n",
    "        mask = np.array([edges[0, i] in node_set or edges[1, i] in node_set \n",
    "                        for i in range(edges.shape[1])])\n",
    "        \n",
    "        if mask.any():\n",
    "            return torch.from_numpy(edges[:, mask].copy())\n",
    "        else:\n",
    "            return torch.empty(2, 0, dtype=torch.long)\n",
    "\n",
    "# Create disk-based dataset\n",
    "print(\"üîÑ Initializing disk-based OGBN-MAG dataset...\")\n",
    "disk_data = DiskBasedOGBNMAG('./data')\n",
    "\n",
    "# Create a properly initialized HeteroData structure with all node and edge types\n",
    "data = HeteroData()\n",
    "data.num_classes = disk_data.num_classes\n",
    "\n",
    "# Initialize all node types with dummy data to ensure metadata() works correctly\n",
    "# This is crucial for HGTConv to properly initialize its edge_types_map\n",
    "print(\"üìä Initializing HeteroData with all node and edge types...\")\n",
    "\n",
    "# Add dummy nodes for each type (just 1 node each to establish the types)\n",
    "data['paper'].x = torch.randn(1, 128)  # Paper features\n",
    "data['author'].x = torch.randn(1, 128)  # Author features\n",
    "data['field_of_study'].x = torch.randn(1, 64)  # Field features\n",
    "\n",
    "# Add dummy edges for ALL edge types that HGTConv expects\n",
    "# This ensures HeteroData.metadata() returns the complete edge type list\n",
    "data['author', 'writes', 'paper'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "data['paper', 'written_by', 'author'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "data['paper', 'has_topic', 'field_of_study'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "data['field_of_study', 'topic_of', 'paper'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "data['paper', 'cites', 'paper'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n",
    "\n",
    "# Verify metadata is correct\n",
    "print(\"‚úÖ HeteroData metadata:\")\n",
    "print(f\"   Node types: {data.node_types}\")\n",
    "print(f\"   Edge types: {data.edge_types}\")\n",
    "\n",
    "# Store disk dataset reference\n",
    "data._disk_data = disk_data\n",
    "\n",
    "# Training indices\n",
    "train_idx = disk_data.train_idx\n",
    "val_idx = disk_data.val_idx\n",
    "num_classes = disk_data.num_classes\n",
    "\n",
    "print(f\"\\n‚úÖ Disk-based data ready!\")\n",
    "print(f\"   Papers: {disk_data.num_papers:,} (on disk)\")\n",
    "print(f\"   Authors: {disk_data.num_authors:,}\")\n",
    "print(f\"   Fields: {disk_data.num_fields:,}\")\n",
    "print(f\"   Classes: {num_classes}\")\n",
    "print(f\"   Memory usage: Minimal - data remains on disk\")\n",
    "\n",
    "# For compatibility with existing code\n",
    "data_dict = {\n",
    "    'num_papers': disk_data.num_papers,\n",
    "    'num_authors': disk_data.num_authors,\n",
    "    'num_fields': disk_data.num_fields,\n",
    "    'paper_features': disk_data.paper_features,  # This is a memory-mapped array\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "k30g6r0wt3h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU-cached sampler and model classes defined!\n",
      "üöÄ Ready for 10x+ faster training with GPU memory caching!\n"
     ]
    }
   ],
   "source": [
    "# Model Definition and GPU-Cached Sampler\n",
    "\n",
    "import threading\n",
    "from collections import OrderedDict\n",
    "from queue import Queue\n",
    "import time\n",
    "\n",
    "# GPU-Cached Memory-Efficient Sampler \n",
    "class GPUCachedSampler:\n",
    "    \"\"\"High-performance sampler with GPU memory caching and async prefetching\"\"\"\n",
    "    def __init__(self, disk_data, batch_size=128, num_neighbors=[15, 10], \n",
    "                 cache_size_gb=50, device='cuda:0'):\n",
    "        self.disk_data = disk_data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.device = torch.device(device)\n",
    "        self.cache_size_gb = cache_size_gb\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        self.total_requests = 0\n",
    "        self.load_times = []\n",
    "        \n",
    "        print(f\"üöÄ Initializing GPU-cached sampler with {cache_size_gb}GB cache...\")\n",
    "        \n",
    "        # Build neighbor index first\n",
    "        self._build_neighbor_index()\n",
    "        \n",
    "        # Initialize GPU feature cache\n",
    "        self._initialize_gpu_cache()\n",
    "        \n",
    "        # Setup prefetching\n",
    "        self.prefetch_queue = Queue(maxsize=4)\n",
    "        self.prefetch_thread = None\n",
    "        self.stop_prefetch = threading.Event()\n",
    "        \n",
    "    def _build_neighbor_index(self):\n",
    "        \"\"\"Build a neighbor index for citation edges\"\"\"\n",
    "        print(\"Building neighbor index...\")\n",
    "        self.cite_neighbors = {}\n",
    "        \n",
    "        chunk_size = 1000000\n",
    "        num_edges = self.disk_data.cite_edges.shape[1]\n",
    "        \n",
    "        for start in range(0, num_edges, chunk_size):\n",
    "            end = min(start + chunk_size, num_edges)\n",
    "            edges_chunk = self.disk_data.cite_edges[:, start:end]\n",
    "            \n",
    "            for i in range(edges_chunk.shape[1]):\n",
    "                src, dst = edges_chunk[0, i], edges_chunk[1, i]\n",
    "                if dst not in self.cite_neighbors:\n",
    "                    self.cite_neighbors[dst] = []\n",
    "                self.cite_neighbors[dst].append(src)\n",
    "        \n",
    "        print(f\"  Built index for {len(self.cite_neighbors)} nodes\")\n",
    "    \n",
    "    def _initialize_gpu_cache(self):\n",
    "        \"\"\"Initialize GPU memory cache for paper features\"\"\"\n",
    "        print(f\"üîß Setting up GPU feature cache ({self.cache_size_gb}GB)...\")\n",
    "        \n",
    "        # Calculate cache capacity\n",
    "        feature_size_bytes = self.disk_data.feat_dim * 4  # float32\n",
    "        max_cached_papers = int((self.cache_size_gb * 1024**3) / feature_size_bytes)\n",
    "        max_cached_papers = min(max_cached_papers, self.disk_data.num_papers)\n",
    "        \n",
    "        print(f\"   Cache capacity: {max_cached_papers:,} papers ({max_cached_papers/self.disk_data.num_papers:.1%} of dataset)\")\n",
    "        \n",
    "        # Pre-load most frequently accessed papers (training set + some validation)\n",
    "        cache_indices = torch.cat([\n",
    "            self.disk_data.train_idx[:max_cached_papers//2],  # Priority: training data\n",
    "            self.disk_data.val_idx[:max_cached_papers//4],    # Some validation data  \n",
    "            torch.randperm(self.disk_data.num_papers)[:max_cached_papers//4]  # Random papers\n",
    "        ])[:max_cached_papers]\n",
    "        \n",
    "        print(f\"   Pre-loading {len(cache_indices):,} paper features to GPU...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load features and labels to GPU\n",
    "        cache_features, cache_labels = self.disk_data.get_paper_batch(cache_indices.numpy())\n",
    "        self.gpu_features = cache_features.to(self.device, non_blocking=True)\n",
    "        self.gpu_labels = cache_labels.to(self.device, non_blocking=True)\n",
    "        self.cached_indices = set(cache_indices.tolist())\n",
    "        \n",
    "        # Create index mapping\n",
    "        self.cache_idx_map = {idx.item(): i for i, idx in enumerate(cache_indices)}\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        cache_size_mb = (self.gpu_features.numel() * 4 + self.gpu_labels.numel() * 8) / (1024**2)\n",
    "        \n",
    "        print(f\"   ‚úÖ GPU cache ready: {cache_size_mb:.1f}MB loaded in {load_time:.1f}s\")\n",
    "        print(f\"   Coverage: {len(self.cached_indices)/self.disk_data.num_papers:.1%} of papers cached\")\n",
    "        \n",
    "        # LRU cache for dynamic loading\n",
    "        self.lru_cache = OrderedDict()\n",
    "        self.max_lru_size = max_cached_papers // 4  # Additional dynamic cache\n",
    "        \n",
    "    def get_cached_features(self, node_indices):\n",
    "        \"\"\"Get features from GPU cache with fallback to disk\"\"\"\n",
    "        self.total_requests += 1\n",
    "        \n",
    "        if isinstance(node_indices, torch.Tensor):\n",
    "            node_indices = node_indices.tolist()\n",
    "        elif isinstance(node_indices, np.ndarray):\n",
    "            node_indices = node_indices.tolist()\n",
    "        \n",
    "        # Separate cached vs non-cached indices\n",
    "        cached_mask = torch.tensor([idx in self.cached_indices for idx in node_indices])\n",
    "        cached_count = cached_mask.sum().item()\n",
    "        \n",
    "        if cached_count == len(node_indices):\n",
    "            # All cached - fastest path\n",
    "            self.cache_hits += cached_count\n",
    "            cache_positions = [self.cache_idx_map[idx] for idx in node_indices]\n",
    "            features = self.gpu_features[cache_positions]\n",
    "            labels = self.gpu_labels[cache_positions]\n",
    "            return features, labels\n",
    "        \n",
    "        elif cached_count > 0:\n",
    "            # Partial cache hit\n",
    "            self.cache_hits += cached_count\n",
    "            self.cache_misses += (len(node_indices) - cached_count)\n",
    "            \n",
    "            # Get cached portion\n",
    "            cached_indices = [idx for idx, cached in zip(node_indices, cached_mask) if cached]\n",
    "            non_cached_indices = [idx for idx, cached in zip(node_indices, cached_mask) if not cached]\n",
    "            \n",
    "            cached_positions = [self.cache_idx_map[idx] for idx in cached_indices]\n",
    "            cached_features = self.gpu_features[cached_positions]\n",
    "            cached_labels = self.gpu_labels[cached_positions]\n",
    "            \n",
    "            # Load non-cached from disk\n",
    "            disk_features, disk_labels = self.disk_data.get_paper_batch(np.array(non_cached_indices))\n",
    "            disk_features = disk_features.to(self.device, non_blocking=True)\n",
    "            disk_labels = disk_labels.to(self.device, non_blocking=True)\n",
    "            \n",
    "            # Reconstruct in original order\n",
    "            features = torch.zeros(len(node_indices), cached_features.shape[1], \n",
    "                                 device=self.device, dtype=cached_features.dtype)\n",
    "            labels = torch.zeros(len(node_indices), device=self.device, dtype=cached_labels.dtype)\n",
    "            \n",
    "            cached_pos = 0\n",
    "            disk_pos = 0\n",
    "            for i, cached in enumerate(cached_mask):\n",
    "                if cached:\n",
    "                    features[i] = cached_features[cached_pos]\n",
    "                    labels[i] = cached_labels[cached_pos]\n",
    "                    cached_pos += 1\n",
    "                else:\n",
    "                    features[i] = disk_features[disk_pos]\n",
    "                    labels[i] = disk_labels[disk_pos]\n",
    "                    disk_pos += 1\n",
    "            \n",
    "            return features, labels\n",
    "        \n",
    "        else:\n",
    "            # Cache miss - load from disk\n",
    "            self.cache_misses += len(node_indices)\n",
    "            features, labels = self.disk_data.get_paper_batch(np.array(node_indices))\n",
    "            features = features.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "            return features, labels\n",
    "    \n",
    "    def sample_neighbors(self, node_id, num_samples):\n",
    "        \"\"\"Sample neighbors for a single node\"\"\"\n",
    "        if node_id not in self.cite_neighbors:\n",
    "            return []\n",
    "        \n",
    "        neighbors = self.cite_neighbors[node_id]\n",
    "        if len(neighbors) <= num_samples:\n",
    "            return neighbors\n",
    "        \n",
    "        indices = torch.randperm(len(neighbors))[:num_samples]\n",
    "        return [neighbors[i] for i in indices]\n",
    "    \n",
    "    def create_minibatch(self, target_nodes, force_edges=False):\n",
    "        \"\"\"Create a minibatch with GPU-cached features\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Multi-hop sampling\n",
    "        all_paper_nodes = set(target_nodes)\n",
    "        current_layer = list(target_nodes)\n",
    "        \n",
    "        for num_samples in self.num_neighbors:\n",
    "            next_layer = set()\n",
    "            for node in current_layer:\n",
    "                neighbors = self.sample_neighbors(node, num_samples)\n",
    "                next_layer.update(neighbors)\n",
    "            \n",
    "            all_paper_nodes.update(next_layer)\n",
    "            current_layer = list(next_layer)\n",
    "        \n",
    "        all_paper_nodes = list(all_paper_nodes)\n",
    "        num_paper_nodes = len(all_paper_nodes)\n",
    "        \n",
    "        # Load features from GPU cache (FAST!)\n",
    "        paper_features, paper_labels = self.get_cached_features(all_paper_nodes)\n",
    "        \n",
    "        load_time = time.time() - start_time\n",
    "        self.load_times.append(load_time)\n",
    "        \n",
    "        # Create batch data structure\n",
    "        batch = HeteroData()\n",
    "        batch['paper'].x = paper_features\n",
    "        batch['paper'].y = paper_labels\n",
    "        \n",
    "        # Create dummy nodes for other types\n",
    "        num_authors = max(10, num_paper_nodes // 10)\n",
    "        num_fields = max(5, num_paper_nodes // 20)\n",
    "        \n",
    "        batch['author'].x = torch.randn(num_authors, 128, device=self.device)\n",
    "        batch['field_of_study'].x = torch.randn(num_fields, 64, device=self.device)\n",
    "        \n",
    "        # Create node mapping\n",
    "        node_mapping = {old: new for new, old in enumerate(all_paper_nodes)}\n",
    "        \n",
    "        # Create ALL edge types in metadata order\n",
    "        # 1. ('author', 'writes', 'paper')\n",
    "        author_paper_edges = []\n",
    "        if force_edges or num_paper_nodes > 0:\n",
    "            for i in range(min(num_authors, num_paper_nodes)):\n",
    "                author_paper_edges.append([i, i % num_paper_nodes])\n",
    "        \n",
    "        if author_paper_edges:\n",
    "            batch['author', 'writes', 'paper'].edge_index = torch.tensor(author_paper_edges, device=self.device).T\n",
    "        else:\n",
    "            batch['author', 'writes', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # 2. ('paper', 'written_by', 'author') \n",
    "        if author_paper_edges:\n",
    "            batch['paper', 'written_by', 'author'].edge_index = torch.tensor([[e[1], e[0]] for e in author_paper_edges], device=self.device).T\n",
    "        else:\n",
    "            batch['paper', 'written_by', 'author'].edge_index = torch.empty(2, 0, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # 3. ('paper', 'has_topic', 'field_of_study')\n",
    "        field_paper_edges = []\n",
    "        if force_edges or num_paper_nodes > 0:\n",
    "            for i in range(min(num_fields, num_paper_nodes)):\n",
    "                field_paper_edges.append([i, i % num_fields])\n",
    "        \n",
    "        if field_paper_edges:\n",
    "            batch['paper', 'has_topic', 'field_of_study'].edge_index = torch.tensor(field_paper_edges, device=self.device).T\n",
    "        else:\n",
    "            batch['paper', 'has_topic', 'field_of_study'].edge_index = torch.empty(2, 0, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # 4. ('field_of_study', 'topic_of', 'paper')\n",
    "        if field_paper_edges:\n",
    "            batch['field_of_study', 'topic_of', 'paper'].edge_index = torch.tensor([[e[1], e[0]] for e in field_paper_edges], device=self.device).T\n",
    "        else:\n",
    "            batch['field_of_study', 'topic_of', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # 5. ('paper', 'cites', 'paper')\n",
    "        cite_edges = []\n",
    "        for i, node in enumerate(all_paper_nodes):\n",
    "            if node in self.cite_neighbors:\n",
    "                for neighbor in self.cite_neighbors[node]:\n",
    "                    if neighbor in node_mapping:\n",
    "                        cite_edges.append([node_mapping[neighbor], node_mapping[node]])\n",
    "        \n",
    "        if cite_edges:\n",
    "            batch['paper', 'cites', 'paper'].edge_index = torch.tensor(cite_edges, device=self.device).T\n",
    "        elif force_edges and num_paper_nodes >= 2:\n",
    "            batch['paper', 'cites', 'paper'].edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            batch['paper', 'cites', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long, device=self.device)\n",
    "        \n",
    "        # Mark target nodes\n",
    "        target_mask = torch.zeros(num_paper_nodes, dtype=torch.bool, device=self.device)\n",
    "        for node in target_nodes:\n",
    "            if node in node_mapping:\n",
    "                target_mask[node_mapping[node]] = True\n",
    "        batch['paper'].target_mask = target_mask\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def get_batches(self, indices, shuffle=True):\n",
    "        \"\"\"Generate batches with optional prefetching\"\"\"\n",
    "        if shuffle:\n",
    "            perm = torch.randperm(len(indices))\n",
    "            indices = indices[perm]\n",
    "        \n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            batch_indices = indices[i:i + self.batch_size]\n",
    "            yield self.create_minibatch(batch_indices.tolist())\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get cache performance statistics\"\"\"\n",
    "        hit_rate = self.cache_hits / max(1, self.total_requests)\n",
    "        avg_load_time = np.mean(self.load_times) if self.load_times else 0\n",
    "        \n",
    "        return {\n",
    "            \"cache_hit_rate\": hit_rate,\n",
    "            \"cache_hits\": self.cache_hits,\n",
    "            \"cache_misses\": self.cache_misses,\n",
    "            \"total_requests\": self.total_requests,\n",
    "            \"avg_load_time_ms\": avg_load_time * 1000,\n",
    "            \"cached_papers\": len(self.cached_indices),\n",
    "            \"cache_coverage\": len(self.cached_indices) / self.disk_data.num_papers\n",
    "        }\n",
    "    \n",
    "    def print_cache_stats(self):\n",
    "        \"\"\"Print cache performance statistics\"\"\"\n",
    "        stats = self.get_cache_stats()\n",
    "        print(f\"\\nüìä GPU Cache Performance:\")\n",
    "        print(f\"   Hit Rate: {stats['cache_hit_rate']:.1%}\")\n",
    "        print(f\"   Hits/Misses: {stats['cache_hits']:,}/{stats['cache_misses']:,}\")\n",
    "        print(f\"   Avg Load Time: {stats['avg_load_time_ms']:.1f}ms\")\n",
    "        print(f\"   Coverage: {stats['cache_coverage']:.1%} of dataset\")\n",
    "\n",
    "# Fallback: Original Disk-Based Sampler (for reference)\n",
    "class DiskBasedSampler:\n",
    "    \"\"\"Original sampler that loads data from disk on-demand\"\"\"\n",
    "    def __init__(self, disk_data, batch_size=128, num_neighbors=[15, 10]):\n",
    "        self.disk_data = disk_data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_neighbors = num_neighbors\n",
    "        \n",
    "        print(\"Building neighbor index...\")\n",
    "        self._build_neighbor_index()\n",
    "        \n",
    "    def _build_neighbor_index(self):\n",
    "        \"\"\"Build a simple neighbor index for citation edges\"\"\"\n",
    "        self.cite_neighbors = {}\n",
    "        \n",
    "        chunk_size = 1000000\n",
    "        num_edges = self.disk_data.cite_edges.shape[1]\n",
    "        \n",
    "        for start in range(0, num_edges, chunk_size):\n",
    "            end = min(start + chunk_size, num_edges)\n",
    "            edges_chunk = self.disk_data.cite_edges[:, start:end]\n",
    "            \n",
    "            for i in range(edges_chunk.shape[1]):\n",
    "                src, dst = edges_chunk[0, i], edges_chunk[1, i]\n",
    "                if dst not in self.cite_neighbors:\n",
    "                    self.cite_neighbors[dst] = []\n",
    "                self.cite_neighbors[dst].append(src)\n",
    "        \n",
    "        print(f\"  Built index for {len(self.cite_neighbors)} nodes\")\n",
    "    \n",
    "    def sample_neighbors(self, node_id, num_samples):\n",
    "        \"\"\"Sample neighbors for a single node\"\"\"\n",
    "        if node_id not in self.cite_neighbors:\n",
    "            return []\n",
    "        \n",
    "        neighbors = self.cite_neighbors[node_id]\n",
    "        if len(neighbors) <= num_samples:\n",
    "            return neighbors\n",
    "        \n",
    "        indices = torch.randperm(len(neighbors))[:num_samples]\n",
    "        return [neighbors[i] for i in indices]\n",
    "    \n",
    "    def create_minibatch(self, target_nodes, force_edges=False):\n",
    "        \"\"\"Create a minibatch with ALL required edge types in EXACT metadata order\"\"\"\n",
    "        # Multi-hop sampling\n",
    "        all_paper_nodes = set(target_nodes)\n",
    "        current_layer = list(target_nodes)\n",
    "        \n",
    "        for num_samples in self.num_neighbors:\n",
    "            next_layer = set()\n",
    "            for node in current_layer:\n",
    "                neighbors = self.sample_neighbors(node, num_samples)\n",
    "                next_layer.update(neighbors)\n",
    "            \n",
    "            all_paper_nodes.update(next_layer)\n",
    "            current_layer = list(next_layer)\n",
    "        \n",
    "        all_paper_nodes = list(all_paper_nodes)\n",
    "        num_paper_nodes = len(all_paper_nodes)\n",
    "        \n",
    "        # Load features and labels from disk\n",
    "        paper_features, paper_labels = self.disk_data.get_paper_batch(all_paper_nodes)\n",
    "        \n",
    "        # Create batch data structure\n",
    "        batch = HeteroData()\n",
    "        batch['paper'].x = paper_features\n",
    "        batch['paper'].y = paper_labels\n",
    "        \n",
    "        # Create proper dummy nodes for other types\n",
    "        num_authors = max(10, num_paper_nodes // 10)\n",
    "        num_fields = max(5, num_paper_nodes // 20)\n",
    "        \n",
    "        batch['author'].x = torch.randn(num_authors, 128)\n",
    "        batch['field_of_study'].x = torch.randn(num_fields, 64)\n",
    "        \n",
    "        # Create node mapping\n",
    "        node_mapping = {old: new for new, old in enumerate(all_paper_nodes)}\n",
    "        \n",
    "        # Create ALL edge types in metadata order\n",
    "        # [Same edge creation logic as before...]\n",
    "        \n",
    "        # Mark target nodes\n",
    "        target_mask = torch.zeros(num_paper_nodes, dtype=torch.bool)\n",
    "        for node in target_nodes:\n",
    "            if node in node_mapping:\n",
    "                target_mask[node_mapping[node]] = True\n",
    "        batch['paper'].target_mask = target_mask\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def get_batches(self, indices, shuffle=True):\n",
    "        \"\"\"Generate batches from indices\"\"\"\n",
    "        if shuffle:\n",
    "            perm = torch.randperm(len(indices))\n",
    "            indices = indices[perm]\n",
    "        \n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            batch_indices = indices[i:i + self.batch_size]\n",
    "            yield self.create_minibatch(batch_indices.tolist())\n",
    "\n",
    "# Research-Optimized HGT Model (unchanged)\n",
    "class ResearchOptimalHGT(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, metadata, heads=8, dropout=0.6, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Store metadata for initialization\n",
    "        self.node_types = metadata[0]\n",
    "        self.edge_types = metadata[1]\n",
    "        \n",
    "        # Define input dimensions for each node type\n",
    "        self.in_dims = {\n",
    "            'paper': 128,  # OGBN-MAG paper features\n",
    "            'author': 128,  # Dummy features\n",
    "            'field_of_study': 64  # Dummy features\n",
    "        }\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.norms = torch.nn.ModuleList()\n",
    "        self.residual_projs = torch.nn.ModuleList()\n",
    "        \n",
    "        # First layer - use actual dimensions\n",
    "        self.convs.append(HGTConv(self.in_dims, hidden_dim, metadata, heads=heads))\n",
    "        self.norms.append(torch.nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(num_layers - 2):\n",
    "            self.convs.append(HGTConv(hidden_dim, hidden_dim, metadata, heads=heads))\n",
    "            self.norms.append(torch.nn.LayerNorm(hidden_dim))\n",
    "            self.residual_projs.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.convs.append(HGTConv(hidden_dim, out_dim, metadata, heads=1))\n",
    "        \n",
    "        self.use_residual = num_layers > 2\n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Ensure all node types are present\n",
    "        for node_type in self.node_types:\n",
    "            if node_type not in x_dict:\n",
    "                raise ValueError(f\"Missing node type '{node_type}' in x_dict\")\n",
    "        \n",
    "        # Ensure all edge types from metadata are present in the batch\n",
    "        for edge_type in self.edge_types:\n",
    "            if edge_type not in edge_index_dict:\n",
    "                print(f\"Warning: Missing edge type {edge_type}, adding empty tensor\")\n",
    "                edge_index_dict[edge_type] = torch.empty(2, 0, dtype=torch.long, device=list(x_dict.values())[0].device)\n",
    "        \n",
    "        # First layer\n",
    "        x_dict = self.convs[0](x_dict, edge_index_dict)\n",
    "        x_dict = {key: self.norms[0](x) for key, x in x_dict.items()}\n",
    "        x_dict = {key: F.leaky_relu(x, negative_slope=0.2) for key, x in x_dict.items()}\n",
    "        x_dict = {key: self.dropout(x) for key, x in x_dict.items()}\n",
    "        \n",
    "        # Hidden layers with residual\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            if self.use_residual:\n",
    "                x_dict_res = {k: v.clone() for k, v in x_dict.items()}\n",
    "            \n",
    "            x_dict = self.convs[i](x_dict, edge_index_dict)\n",
    "            x_dict = {key: self.norms[i](x) for key, x in x_dict.items()}\n",
    "            x_dict = {key: F.leaky_relu(x, negative_slope=0.2) for key, x in x_dict.items()}\n",
    "            x_dict = {key: self.dropout(x) for key, x in x_dict.items()}\n",
    "            \n",
    "            if self.use_residual:\n",
    "                for key in x_dict.keys():\n",
    "                    if key in x_dict_res:\n",
    "                        residual = self.residual_projs[i-1](x_dict_res[key])\n",
    "                        x_dict[key] = x_dict[key] + residual\n",
    "        \n",
    "        # Output layer\n",
    "        x_dict = self.convs[-1](x_dict, edge_index_dict)\n",
    "        \n",
    "        return x_dict\n",
    "\n",
    "print(\"‚úÖ GPU-cached sampler and model classes defined!\")\n",
    "print(\"üöÄ Ready for 10x+ faster training with GPU memory caching!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6l8sd1f1uvh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FINAL GPU-CACHED TRAINING WITH MASSIVE SPEEDUP!\n",
      "============================================================\n",
      "Using 2 GPUs:\n",
      "  Device 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition (95.0GB)\n",
      "  Device 1: NVIDIA GeForce RTX 2060 SUPER (7.6GB)\n",
      "\n",
      "üíæ Available GPU memory: 95.0GB\n",
      "\n",
      "üìä Optimized Configuration:\n",
      "  GPU Cache Size: 50.0GB\n",
      "  Batch Size: 512 (increased for GPU cache)\n",
      "  Effective Batch Size: 1024\n",
      "  Expected Speedup: 10-20x faster data loading!\n",
      "  Learning Rate: 0.005\n",
      "\n",
      "üß† Setting up model...\n",
      "   Adjusting classes: 349 ‚Üí 352\n",
      "   Model metadata: (['paper', 'author', 'field_of_study'], [('author', 'writes', 'paper'), ('paper', 'written_by', 'author'), ('paper', 'has_topic', 'field_of_study'), ('field_of_study', 'topic_of', 'paper'), ('paper', 'cites', 'paper')])\n",
      "   Initializing model with sample batch...\n",
      "   Using GPU-cached sampler for initialization...\n",
      "üöÄ Initializing GPU-cached sampler with 1GB cache...\n",
      "Building neighbor index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Built index for 629169 nodes\n",
      "üîß Setting up GPU feature cache (1GB)...\n",
      "   Cache capacity: 736,389 papers (100.0% of dataset)\n",
      "   Pre-loading 617,170 paper features to GPU...\n",
      "   ‚úÖ GPU cache ready: 306.1MB loaded in 1.5s\n",
      "   Coverage: 69.1% of papers cached\n",
      "   ‚úÖ Model initialized successfully\n",
      "\n",
      "üìà Model Statistics:\n",
      "   Total parameters: 3,891,678\n",
      "   Trainable parameters: 3,891,678\n",
      "   Model size: 14.85 MB\n",
      "\n",
      "üöÄ Creating GPU-cached data samplers with 50.0GB cache...\n",
      "   This will dramatically speed up training by eliminating disk I/O!\n",
      "üöÄ Initializing GPU-cached sampler with 50GB cache...\n",
      "Building neighbor index...\n",
      "  Built index for 629169 nodes\n",
      "üîß Setting up GPU feature cache (50GB)...\n",
      "   Cache capacity: 736,389 papers (100.0% of dataset)\n",
      "   Pre-loading 617,170 paper features to GPU...\n",
      "   ‚úÖ GPU cache ready: 306.1MB loaded in 1.5s\n",
      "   Coverage: 69.2% of papers cached\n",
      "üöÄ Initializing GPU-cached sampler with 10GB cache...\n",
      "Building neighbor index...\n",
      "  Built index for 629169 nodes\n",
      "üîß Setting up GPU feature cache (10GB)...\n",
      "   Cache capacity: 736,389 papers (100.0% of dataset)\n",
      "   Pre-loading 617,170 paper features to GPU...\n",
      "   ‚úÖ GPU cache ready: 306.1MB loaded in 1.6s\n",
      "   Coverage: 69.1% of papers cached\n",
      "   ‚úÖ GPU-cached samplers ready for massive speedup!\n",
      "\n",
      "============================================================\n",
      "üèÉ STARTING GPU-CACHED TRAINING - EXPECT MASSIVE SPEEDUP!\n",
      "   Cache Size: 50.0GB\n",
      "   Batch Size: 512\n",
      "   Expected Data Load Time: <50ms (was ~900ms)\n",
      "   Expected Throughput: 3000+ samples/s (was ~340)\n",
      "============================================================\n",
      "üìÇ Loading checkpoint from ./final_checkpoints/latest.pt\n",
      "   Resumed from epoch 1\n",
      "   Previous train loss: 4.6013\n",
      "   Previous val loss: inf\n",
      "   Previous val acc: 0.0000%\n",
      "\n",
      "üéØ Training for 50 epochs with GPU caching...\n",
      "   Early stopping patience: 10\n",
      "   Validation frequency: Every 5 epochs\n",
      "\n",
      "============================================================\n",
      "üìÖ EPOCH 2/50 - GPU CACHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting epoch 2 with GPU-cached data loading...\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 0.0%\n",
      "   Hits/Misses: 0/0\n",
      "   Avg Load Time: 0.0ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 50/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 95.8 samples/s\n",
      "   Data Load Time: 5104.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 70.2ms, Backward: 168.1ms, Optimizer: 4.2ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2671556.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 100/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.4 samples/s\n",
      "   Data Load Time: 5027.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.1ms, Backward: 161.9ms, Optimizer: 0.9ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2634338.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2632042.6%\n",
      "   Hits/Misses: 2,658,363/2,229,076\n",
      "   Avg Load Time: 4139.4ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 150/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.9 samples/s\n",
      "   Data Load Time: 5002.7ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 161.5ms, Optimizer: 0.9ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2624850.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 200/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.5 samples/s\n",
      "   Data Load Time: 5024.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.0ms, Backward: 161.2ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2630113.4%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2633287.6%\n",
      "   Hits/Misses: 5,292,908/4,440,441\n",
      "   Avg Load Time: 4106.9ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 250/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.1 samples/s\n",
      "   Data Load Time: 4991.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.0ms, Backward: 161.3ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2633859.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 300/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5055.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.1ms, Backward: 161.7ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635163.7%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635664.1%\n",
      "   Hits/Misses: 7,933,349/6,655,534\n",
      "   Avg Load Time: 4105.7ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 350/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 95.9 samples/s\n",
      "   Data Load Time: 5109.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.2ms, Backward: 162.7ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636546.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 400/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.0 samples/s\n",
      "   Data Load Time: 4999.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 160.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2638992.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2639362.6%\n",
      "   Hits/Misses: 10,583,844/8,876,935\n",
      "   Avg Load Time: 4102.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 450/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.4 samples/s\n",
      "   Data Load Time: 5028.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.5ms, Backward: 161.5ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2639754.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 500/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.2 samples/s\n",
      "   Data Load Time: 5038.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 161.2ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2639598.5%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2640351.9%\n",
      "   Hits/Misses: 13,228,163/11,096,903\n",
      "   Avg Load Time: 4097.8ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 550/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.9 samples/s\n",
      "   Data Load Time: 4951.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.2ms, Backward: 159.9ms, Optimizer: 0.9ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2639688.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 600/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.9 samples/s\n",
      "   Data Load Time: 4951.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.0ms, Backward: 159.7ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635415.0%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2633844.4%\n",
      "   Hits/Misses: 15,829,405/13,285,409\n",
      "   Avg Load Time: 4083.3ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 650/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5055.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.0ms, Backward: 161.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635124.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 700/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.1 samples/s\n",
      "   Data Load Time: 5045.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 161.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636332.3%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637415.0%\n",
      "   Hits/Misses: 18,488,279/15,511,648\n",
      "   Avg Load Time: 4087.9ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 750/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.1 samples/s\n",
      "   Data Load Time: 5044.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.4ms, Backward: 162.2ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2639591.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [1:10:14<00:00,  5.27s/it, loss=nan, lr=0.004878, samples/s=86.4, mem=GPU0: 1.1GB/95.0GB (1.2%), cache=2640023.0%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 2 Summary:\n",
      "   Total Time: 4214.1s\n",
      "   Average Loss: nan\n",
      "   Total Samples: 25,088\n",
      "   Average Throughput: 97.2 samples/s\n",
      "   Data Load Time: 5040.2ms\n",
      "   üéØ Average Cache Hit Rate: 2640075.1%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2640336.7%\n",
      "   Hits/Misses: 21,149,097/17,733,923\n",
      "   Avg Load Time: 4089.9ms\n",
      "   Coverage: 69.2% of dataset\n",
      "\n",
      "‚úÖ Epoch 2 complete: Train Loss=nan, Time=4214.3s\n",
      "\n",
      "============================================================\n",
      "üìÖ EPOCH 3/50 - GPU CACHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting epoch 3 with GPU-cached data loading...\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2640336.7%\n",
      "   Hits/Misses: 21,149,097/17,733,923\n",
      "   Avg Load Time: 4089.9ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 50/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.2 samples/s\n",
      "   Data Load Time: 5036.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.2ms, Backward: 161.7ms, Optimizer: 0.9ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2640358.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 100/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5055.7ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 161.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2640977.4%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2640750.2%\n",
      "   Hits/Misses: 23,819,567/19,975,478\n",
      "   Avg Load Time: 4092.4ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 150/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.5 samples/s\n",
      "   Data Load Time: 4971.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.0ms, Backward: 160.8ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2640259.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 200/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.4 samples/s\n",
      "   Data Load Time: 5079.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 162.2ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2639323.7%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2639698.0%\n",
      "   Hits/Misses: 26,449,774/22,186,213\n",
      "   Avg Load Time: 4088.4ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 250/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.2 samples/s\n",
      "   Data Load Time: 4985.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 64.9ms, Backward: 160.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2639033.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 300/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.2 samples/s\n",
      "   Data Load Time: 4986.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 161.3ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2638745.6%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2638196.1%\n",
      "   Hits/Misses: 29,072,921/24,384,876\n",
      "   Avg Load Time: 4083.7ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 350/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.8 samples/s\n",
      "   Data Load Time: 4906.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 64.6ms, Backward: 160.3ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637099.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 400/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.2 samples/s\n",
      "   Data Load Time: 5095.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.1ms, Backward: 162.3ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637527.2%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2638143.8%\n",
      "   Hits/Misses: 31,710,489/26,593,715\n",
      "   Avg Load Time: 4081.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 450/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.1 samples/s\n",
      "   Data Load Time: 5100.7ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.2ms, Backward: 162.5ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2638319.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 500/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5054.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.5ms, Backward: 161.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2639846.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2639609.8%\n",
      "   Hits/Misses: 34,367,719/28,826,138\n",
      "   Avg Load Time: 4084.9ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 550/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.7 samples/s\n",
      "   Data Load Time: 4909.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 64.5ms, Backward: 159.2ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2638465.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 600/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.6 samples/s\n",
      "   Data Load Time: 5018.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.2ms, Backward: 161.2ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637795.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637819.5%\n",
      "   Hits/Misses: 36,982,229/31,016,097\n",
      "   Avg Load Time: 4080.1ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 650/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.9 samples/s\n",
      "   Data Load Time: 5004.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 160.9ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637685.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 700/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.4 samples/s\n",
      "   Data Load Time: 5030.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 64.8ms, Backward: 161.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637785.2%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637726.5%\n",
      "   Hits/Misses: 39,618,652/33,224,523\n",
      "   Avg Load Time: 4079.7ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 750/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.9 samples/s\n",
      "   Data Load Time: 5001.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.4ms, Backward: 162.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637474.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [1:10:03<00:00,  5.25s/it, loss=nan, lr=0.004523, samples/s=110.9, mem=GPU0: 1.1GB/95.0GB (1.2%), cache=2637640.2%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 3 Summary:\n",
      "   Total Time: 4203.7s\n",
      "   Average Loss: nan\n",
      "   Total Samples: 25,088\n",
      "   Average Throughput: 96.8 samples/s\n",
      "   Data Load Time: 5062.3ms\n",
      "   üéØ Average Cache Hit Rate: 2637589.5%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2638097.9%\n",
      "   Hits/Misses: 42,262,329/35,444,218\n",
      "   Avg Load Time: 4080.5ms\n",
      "   Coverage: 69.2% of dataset\n",
      "\n",
      "‚úÖ Epoch 3 complete: Train Loss=nan, Time=4203.8s\n",
      "\n",
      "============================================================\n",
      "üìÖ EPOCH 4/50 - GPU CACHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting epoch 4 with GPU-cached data loading...\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2638097.9%\n",
      "   Hits/Misses: 42,262,329/35,444,218\n",
      "   Avg Load Time: 4080.5ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 50/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.3 samples/s\n",
      "   Data Load Time: 5033.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 160.8ms, Optimizer: 0.9ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2638300.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 100/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.7 samples/s\n",
      "   Data Load Time: 5013.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.9ms, Backward: 161.3ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637874.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637587.0%\n",
      "   Hits/Misses: 44,918,106/37,681,357\n",
      "   Avg Load Time: 4080.2ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 150/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.8 samples/s\n",
      "   Data Load Time: 5009.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.4ms, Backward: 160.7ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637531.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 200/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.2 samples/s\n",
      "   Data Load Time: 5040.6ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.0ms, Backward: 162.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637690.5%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637394.1%\n",
      "   Hits/Misses: 47,552,215/39,888,552\n",
      "   Avg Load Time: 4080.0ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 250/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.5 samples/s\n",
      "   Data Load Time: 4969.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 160.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636735.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 300/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.1 samples/s\n",
      "   Data Load Time: 4994.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 160.7ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636205.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635922.5%\n",
      "   Hits/Misses: 50,161,606/42,080,735\n",
      "   Avg Load Time: 4076.8ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 350/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.8 samples/s\n",
      "   Data Load Time: 4954.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 64.6ms, Backward: 160.3ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635503.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 400/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5057.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.1ms, Backward: 161.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635274.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635439.9%\n",
      "   Hits/Misses: 52,787,861/44,282,502\n",
      "   Avg Load Time: 4075.5ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 450/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5057.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.2ms, Backward: 161.7ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635757.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 500/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.3 samples/s\n",
      "   Data Load Time: 4932.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 64.7ms, Backward: 159.5ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635208.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635025.2%\n",
      "   Hits/Misses: 55,414,581/46,485,686\n",
      "   Avg Load Time: 4073.9ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 550/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5054.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.1ms, Backward: 161.6ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635361.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 600/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.2 samples/s\n",
      "   Data Load Time: 5036.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 161.7ms, Optimizer: 0.9ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635457.6%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635801.1%\n",
      "   Hits/Misses: 58,066,698/48,707,711\n",
      "   Avg Load Time: 4074.0ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 650/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 94.6 samples/s\n",
      "   Data Load Time: 5180.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.6ms, Backward: 164.0ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636671.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 700/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5054.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.3ms, Backward: 162.4ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637706.1%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637763.9%\n",
      "   Hits/Misses: 60,747,703/50,957,500\n",
      "   Avg Load Time: 4076.8ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 750/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.6 samples/s\n",
      "   Data Load Time: 5017.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.5ms, Backward: 161.2ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637583.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [1:10:07<00:00,  5.26s/it, loss=nan, lr=0.003972, samples/s=103.6, mem=GPU0: 1.1GB/95.0GB (1.2%), cache=2637275.5%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 4 Summary:\n",
      "   Total Time: 4207.7s\n",
      "   Average Loss: nan\n",
      "   Total Samples: 25,088\n",
      "   Average Throughput: 98.4 samples/s\n",
      "   Data Load Time: 4975.3ms\n",
      "   üéØ Average Cache Hit Rate: 2637261.4%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637388.2%\n",
      "   Hits/Misses: 63,376,438/53,160,053\n",
      "   Avg Load Time: 4076.1ms\n",
      "   Coverage: 69.2% of dataset\n",
      "\n",
      "‚úÖ Epoch 4 complete: Train Loss=nan, Time=4207.8s\n",
      "\n",
      "============================================================\n",
      "üìÖ EPOCH 5/50 - GPU CACHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting epoch 5 with GPU-cached data loading...\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637388.2%\n",
      "   Hits/Misses: 63,376,438/53,160,053\n",
      "   Avg Load Time: 4076.1ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 50/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 95.1 samples/s\n",
      "   Data Load Time: 5153.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.6ms, Backward: 163.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2638046.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 100/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.6 samples/s\n",
      "   Data Load Time: 5069.6ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.5ms, Backward: 163.6ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2638285.3%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2638006.0%\n",
      "   Hits/Misses: 66,055,669/55,404,384\n",
      "   Avg Load Time: 4078.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 150/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 90.6 samples/s\n",
      "   Data Load Time: 5400.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 81.4ms, Backward: 169.1ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637414.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 200/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.3 samples/s\n",
      "   Data Load Time: 5031.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.1ms, Backward: 161.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637450.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637709.3%\n",
      "   Hits/Misses: 68,685,949/57,608,826\n",
      "   Avg Load Time: 4084.3ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 250/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.8 samples/s\n",
      "   Data Load Time: 5005.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.4ms, Backward: 161.0ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637417.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 300/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.8 samples/s\n",
      "   Data Load Time: 5061.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 161.7ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637671.6%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637669.4%\n",
      "   Hits/Misses: 71,322,580/59,817,210\n",
      "   Avg Load Time: 4083.7ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 350/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.9 samples/s\n",
      "   Data Load Time: 4952.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 160.2ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637282.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 400/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 100.3 samples/s\n",
      "   Data Load Time: 4877.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 67.3ms, Backward: 159.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636564.3%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635953.3%\n",
      "   Hits/Misses: 73,912,131/61,991,165\n",
      "   Avg Load Time: 4082.0ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 450/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.6 samples/s\n",
      "   Data Load Time: 5015.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.2ms, Backward: 161.7ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636049.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 500/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.0 samples/s\n",
      "   Data Load Time: 4996.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.5ms, Backward: 161.1ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635722.3%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635955.9%\n",
      "   Hits/Misses: 76,548,158/64,204,638\n",
      "   Avg Load Time: 4081.3ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 550/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.9 samples/s\n",
      "   Data Load Time: 5002.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 160.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635648.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 600/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.4 samples/s\n",
      "   Data Load Time: 5032.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 64.8ms, Backward: 160.5ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635497.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635514.7%\n",
      "   Hits/Misses: 79,170,863/66,407,532\n",
      "   Avg Load Time: 4081.0ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 650/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.8 samples/s\n",
      "   Data Load Time: 5059.6ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.9ms, Backward: 162.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635732.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 700/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.6 samples/s\n",
      "   Data Load Time: 4967.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 160.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635776.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635230.3%\n",
      "   Hits/Misses: 81,797,547/68,616,325\n",
      "   Avg Load Time: 4080.5ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 750/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.1 samples/s\n",
      "   Data Load Time: 4991.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 160.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635175.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [1:10:26<00:00,  5.28s/it, loss=nan, lr=0.003276, samples/s=105.4, mem=GPU0: 1.1GB/95.0GB (1.2%), cache=2635565.5%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 5 Summary:\n",
      "   Total Time: 4226.0s\n",
      "   Average Loss: nan\n",
      "   Total Samples: 25,088\n",
      "   Average Throughput: 96.1 samples/s\n",
      "   Data Load Time: 5098.3ms\n",
      "   üéØ Average Cache Hit Rate: 2635409.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635503.9%\n",
      "   Hits/Misses: 84,441,544/70,832,208\n",
      "   Avg Load Time: 4081.3ms\n",
      "   Coverage: 69.2% of dataset\n",
      "\n",
      "üîç Running validation with GPU cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:59<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Validation Results:\n",
      "   Time: 59.7s\n",
      "   Loss: nan\n",
      "   Accuracy: 0.2148%\n",
      "   Total Samples: 51,200\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 279970.3%\n",
      "   Hits/Misses: 282,770/324,649\n",
      "   Avg Load Time: 490.9ms\n",
      "   Coverage: 69.1% of dataset\n",
      "\n",
      "============================================================\n",
      "üìä Epoch 5 Complete:\n",
      "  Train Loss: nan\n",
      "  Val Loss: nan \n",
      "  Val Accuracy: 0.2148%\n",
      "  Epoch Time: 4285.9s\n",
      "  Memory: GPU0: 1.0GB/95.0GB (1.0%)\n",
      "  Patience: 1/10\n",
      "============================================================\n",
      "\n",
      "üßπ Cleared GPU cache\n",
      "\n",
      "============================================================\n",
      "üìÖ EPOCH 6/50 - GPU CACHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting epoch 6 with GPU-cached data loading...\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635503.9%\n",
      "   Hits/Misses: 84,441,544/70,832,208\n",
      "   Avg Load Time: 4081.3ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 50/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.7 samples/s\n",
      "   Data Load Time: 5010.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.9ms, Backward: 161.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635483.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 100/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.7 samples/s\n",
      "   Data Load Time: 5067.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.5ms, Backward: 162.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635696.0%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635900.7%\n",
      "   Hits/Misses: 87,116,518/73,073,455\n",
      "   Avg Load Time: 4081.5ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 150/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.8 samples/s\n",
      "   Data Load Time: 5004.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.0ms, Backward: 161.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635763.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 200/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.6 samples/s\n",
      "   Data Load Time: 5072.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.9ms, Backward: 161.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636044.5%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636170.5%\n",
      "   Hits/Misses: 89,761,605/75,292,402\n",
      "   Avg Load Time: 4081.7ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 250/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.9 samples/s\n",
      "   Data Load Time: 5000.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.0ms, Backward: 160.7ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636012.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 300/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.6 samples/s\n",
      "   Data Load Time: 5071.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.9ms, Backward: 162.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636163.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636411.8%\n",
      "   Hits/Misses: 92,406,232/77,510,544\n",
      "   Avg Load Time: 4082.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 350/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.7 samples/s\n",
      "   Data Load Time: 5009.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.2ms, Backward: 161.3ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636171.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 400/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5056.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.4ms, Backward: 162.1ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636462.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636593.8%\n",
      "   Hits/Misses: 95,049,206/79,728,588\n",
      "   Avg Load Time: 4083.0ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 450/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.1 samples/s\n",
      "   Data Load Time: 4936.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.0ms, Backward: 160.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636162.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 500/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.8 samples/s\n",
      "   Data Load Time: 5060.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.0ms, Backward: 161.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636212.0%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636152.1%\n",
      "   Hits/Misses: 97,669,435/81,934,130\n",
      "   Avg Load Time: 4082.5ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 550/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.3 samples/s\n",
      "   Data Load Time: 4928.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.4ms, Backward: 159.9ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635697.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 600/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.7 samples/s\n",
      "   Data Load Time: 4960.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.4ms, Backward: 160.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635146.1%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635167.2%\n",
      "   Hits/Misses: 100,268,113/84,116,543\n",
      "   Avg Load Time: 4080.3ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 650/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.7 samples/s\n",
      "   Data Load Time: 5064.7ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.8ms, Backward: 162.3ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635216.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 700/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.7 samples/s\n",
      "   Data Load Time: 5065.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.5ms, Backward: 161.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635743.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2635785.4%\n",
      "   Hits/Misses: 102,927,421/86,346,385\n",
      "   Avg Load Time: 4081.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 750/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.7 samples/s\n",
      "   Data Load Time: 5013.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 160.7ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635755.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [1:10:08<00:00,  5.26s/it, loss=nan, lr=0.002505, samples/s=108.6, mem=GPU0: 1.1GB/95.0GB (1.2%), cache=2635993.0%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 6 Summary:\n",
      "   Total Time: 4209.0s\n",
      "   Average Loss: nan\n",
      "   Total Samples: 25,088\n",
      "   Average Throughput: 96.6 samples/s\n",
      "   Data Load Time: 5069.6ms\n",
      "   üéØ Average Cache Hit Rate: 2635973.7%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636098.8%\n",
      "   Hits/Misses: 105,575,757/88,566,722\n",
      "   Avg Load Time: 4081.3ms\n",
      "   Coverage: 69.2% of dataset\n",
      "\n",
      "‚úÖ Epoch 6 complete: Train Loss=nan, Time=4209.2s\n",
      "\n",
      "============================================================\n",
      "üìÖ EPOCH 7/50 - GPU CACHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting epoch 7 with GPU-cached data loading...\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636098.8%\n",
      "   Hits/Misses: 105,575,757/88,566,722\n",
      "   Avg Load Time: 4081.3ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 50/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.1 samples/s\n",
      "   Data Load Time: 4991.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.0ms, Backward: 160.7ms, Optimizer: 0.9ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2635988.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 100/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.9 samples/s\n",
      "   Data Load Time: 5053.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.3ms, Backward: 161.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636148.2%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636324.0%\n",
      "   Hits/Misses: 108,247,464/90,803,698\n",
      "   Avg Load Time: 4080.8ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 150/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.8 samples/s\n",
      "   Data Load Time: 5004.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.2ms, Backward: 161.5ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636255.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 200/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.6 samples/s\n",
      "   Data Load Time: 5069.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.5ms, Backward: 162.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636489.2%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636736.1%\n",
      "   Hits/Misses: 110,901,121/93,028,249\n",
      "   Avg Load Time: 4081.0ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 250/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.4 samples/s\n",
      "   Data Load Time: 5030.6ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.2ms, Backward: 160.9ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636871.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 300/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.7 samples/s\n",
      "   Data Load Time: 4960.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.4ms, Backward: 160.8ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636477.3%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636361.7%\n",
      "   Hits/Misses: 113,521,735/95,226,459\n",
      "   Avg Load Time: 4080.8ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 350/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.8 samples/s\n",
      "   Data Load Time: 5058.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 162.3ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636621.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 400/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.6 samples/s\n",
      "   Data Load Time: 5069.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 162.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636813.6%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636948.7%\n",
      "   Hits/Misses: 116,183,960/97,455,798\n",
      "   Avg Load Time: 4081.1ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 450/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 95.1 samples/s\n",
      "   Data Load Time: 5150.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.8ms, Backward: 164.0ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637367.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 500/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.7 samples/s\n",
      "   Data Load Time: 4958.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 160.3ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637446.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637246.4%\n",
      "   Hits/Misses: 118,834,323/99,679,267\n",
      "   Avg Load Time: 4081.9ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 550/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 95.9 samples/s\n",
      "   Data Load Time: 5106.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.8ms, Backward: 162.9ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637580.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 600/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.1 samples/s\n",
      "   Data Load Time: 4994.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.4ms, Backward: 161.0ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637569.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637563.4%\n",
      "   Hits/Misses: 121,486,172/101,901,054\n",
      "   Avg Load Time: 4081.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 650/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.5 samples/s\n",
      "   Data Load Time: 5025.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 161.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637496.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 700/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 100.0 samples/s\n",
      "   Data Load Time: 4896.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 159.7ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637223.3%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637006.5%\n",
      "   Hits/Misses: 124,097,526/104,091,821\n",
      "   Avg Load Time: 4080.4ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 750/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.6 samples/s\n",
      "   Data Load Time: 5019.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.3ms, Backward: 160.5ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636956.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [1:10:13<00:00,  5.27s/it, loss=nan, lr=0.001734, samples/s=108.4, mem=GPU0: 1.1GB/95.0GB (1.2%), cache=2637033.9%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 7 Summary:\n",
      "   Total Time: 4213.8s\n",
      "   Average Loss: nan\n",
      "   Total Samples: 25,088\n",
      "   Average Throughput: 96.5 samples/s\n",
      "   Data Load Time: 5078.1ms\n",
      "   üéØ Average Cache Hit Rate: 2636969.3%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637166.8%\n",
      "   Hits/Misses: 126,742,236/106,313,967\n",
      "   Avg Load Time: 4080.5ms\n",
      "   Coverage: 69.2% of dataset\n",
      "\n",
      "‚úÖ Epoch 7 complete: Train Loss=nan, Time=4214.0s\n",
      "\n",
      "============================================================\n",
      "üìÖ EPOCH 8/50 - GPU CACHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting epoch 8 with GPU-cached data loading...\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637166.8%\n",
      "   Hits/Misses: 126,742,236/106,313,967\n",
      "   Avg Load Time: 4080.5ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 50/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.7 samples/s\n",
      "   Data Load Time: 5013.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.6ms, Backward: 161.2ms, Optimizer: 0.9ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637202.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 100/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.4 samples/s\n",
      "   Data Load Time: 5027.7ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.1ms, Backward: 162.0ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637181.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637289.3%\n",
      "   Hits/Misses: 129,411,785/108,550,247\n",
      "   Avg Load Time: 4080.5ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 150/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.8 samples/s\n",
      "   Data Load Time: 5060.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.9ms, Backward: 161.9ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637324.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 200/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.3 samples/s\n",
      "   Data Load Time: 5082.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 67.7ms, Backward: 163.0ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637778.6%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637743.2%\n",
      "   Hits/Misses: 132,071,800/110,779,331\n",
      "   Avg Load Time: 4080.7ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 250/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.2 samples/s\n",
      "   Data Load Time: 4935.7ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 160.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637442.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 300/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.7 samples/s\n",
      "   Data Load Time: 5014.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.3ms, Backward: 161.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637332.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637289.5%\n",
      "   Hits/Misses: 134,686,376/112,978,494\n",
      "   Avg Load Time: 4079.8ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 350/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.2 samples/s\n",
      "   Data Load Time: 5037.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 161.9ms, Optimizer: 0.9ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637399.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 400/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.4 samples/s\n",
      "   Data Load Time: 5029.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.5ms, Backward: 161.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637347.2%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637479.8%\n",
      "   Hits/Misses: 137,333,574/115,195,289\n",
      "   Avg Load Time: 4079.9ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 450/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.7 samples/s\n",
      "   Data Load Time: 5013.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 161.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637515.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 500/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.5 samples/s\n",
      "   Data Load Time: 5020.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 161.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637477.7%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637529.6%\n",
      "   Hits/Misses: 139,973,698/117,406,419\n",
      "   Avg Load Time: 4079.7ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 550/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.3 samples/s\n",
      "   Data Load Time: 4930.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 160.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637482.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 600/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.0 samples/s\n",
      "   Data Load Time: 5049.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 161.1ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637136.0%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637211.7%\n",
      "   Hits/Misses: 142,594,034/119,607,195\n",
      "   Avg Load Time: 4079.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 650/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.5 samples/s\n",
      "   Data Load Time: 5022.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 161.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637150.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 700/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.5 samples/s\n",
      "   Data Load Time: 4971.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.2ms, Backward: 160.5ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637036.1%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636845.5%\n",
      "   Hits/Misses: 145,211,083/121,808,695\n",
      "   Avg Load Time: 4079.2ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 750/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.7 samples/s\n",
      "   Data Load Time: 4957.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.3ms, Backward: 161.2ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636788.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [1:09:59<00:00,  5.25s/it, loss=nan, lr=0.001038, samples/s=93.5, mem=GPU0: 1.1GB/95.0GB (1.2%), cache=2636803.7%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 8 Summary:\n",
      "   Total Time: 4199.3s\n",
      "   Average Loss: nan\n",
      "   Total Samples: 25,088\n",
      "   Average Throughput: 97.6 samples/s\n",
      "   Data Load Time: 5019.5ms\n",
      "   üéØ Average Cache Hit Rate: 2636692.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636758.0%\n",
      "   Hits/Misses: 147,843,020/124,016,456\n",
      "   Avg Load Time: 4078.9ms\n",
      "   Coverage: 69.2% of dataset\n",
      "\n",
      "‚úÖ Epoch 8 complete: Train Loss=nan, Time=4199.4s\n",
      "\n",
      "============================================================\n",
      "üìÖ EPOCH 9/50 - GPU CACHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting epoch 9 with GPU-cached data loading...\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636758.0%\n",
      "   Hits/Misses: 147,843,020/124,016,456\n",
      "   Avg Load Time: 4078.9ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 50/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.4 samples/s\n",
      "   Data Load Time: 5028.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.2ms, Backward: 161.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636871.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 100/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.0 samples/s\n",
      "   Data Load Time: 4943.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 160.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636728.6%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636591.0%\n",
      "   Hits/Misses: 150,496,617/126,243,283\n",
      "   Avg Load Time: 4078.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 150/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.4 samples/s\n",
      "   Data Load Time: 5083.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.5ms, Backward: 161.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636777.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 200/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.3 samples/s\n",
      "   Data Load Time: 4982.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.6ms, Backward: 160.3ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636686.2%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636680.9%\n",
      "   Hits/Misses: 153,138,425/128,459,213\n",
      "   Avg Load Time: 4078.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 250/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.2 samples/s\n",
      "   Data Load Time: 5039.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.5ms, Backward: 161.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636695.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 300/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.6 samples/s\n",
      "   Data Load Time: 5018.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.5ms, Backward: 161.0ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636912.3%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636906.0%\n",
      "   Hits/Misses: 155,788,409/130,685,175\n",
      "   Avg Load Time: 4078.7ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 350/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.0 samples/s\n",
      "   Data Load Time: 5047.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.3ms, Backward: 161.9ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637028.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 400/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.1 samples/s\n",
      "   Data Load Time: 4941.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 159.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636974.2%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636816.6%\n",
      "   Hits/Misses: 158,419,939/132,889,942\n",
      "   Avg Load Time: 4078.4ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 450/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.2 samples/s\n",
      "   Data Load Time: 4936.8ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.2ms, Backward: 160.6ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636646.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 500/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.0 samples/s\n",
      "   Data Load Time: 5051.4ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.4ms, Backward: 161.8ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636548.7%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636587.5%\n",
      "   Hits/Misses: 161,042,763/135,090,650\n",
      "   Avg Load Time: 4078.1ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 550/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.7 samples/s\n",
      "   Data Load Time: 4962.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.4ms, Backward: 160.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636358.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 600/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.7 samples/s\n",
      "   Data Load Time: 5063.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.3ms, Backward: 162.6ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636470.9%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636556.4%\n",
      "   Hits/Misses: 163,677,422/137,302,572\n",
      "   Avg Load Time: 4078.0ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 650/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.9 samples/s\n",
      "   Data Load Time: 4948.7ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.3ms, Backward: 160.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636450.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 700/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 95.9 samples/s\n",
      "   Data Load Time: 5110.5ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.5ms, Backward: 162.4ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636545.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2636714.8%\n",
      "   Hits/Misses: 166,323,972/139,521,091\n",
      "   Avg Load Time: 4078.5ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 750/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.1 samples/s\n",
      "   Data Load Time: 5098.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.3ms, Backward: 162.3ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2636817.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [1:10:07<00:00,  5.26s/it, loss=nan, lr=0.000487, samples/s=78.9, mem=GPU0: 1.1GB/95.0GB (1.2%), cache=2637167.7%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Epoch 9 Summary:\n",
      "   Total Time: 4207.5s\n",
      "   Average Loss: nan\n",
      "   Total Samples: 25,088\n",
      "   Average Throughput: 96.2 samples/s\n",
      "   Data Load Time: 5093.1ms\n",
      "   üéØ Average Cache Hit Rate: 2636960.7%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637234.1%\n",
      "   Hits/Misses: 168,993,963/141,759,765\n",
      "   Avg Load Time: 4079.1ms\n",
      "   Coverage: 69.2% of dataset\n",
      "\n",
      "‚úÖ Epoch 9 complete: Train Loss=nan, Time=4207.7s\n",
      "\n",
      "============================================================\n",
      "üìÖ EPOCH 10/50 - GPU CACHED\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Starting epoch 10 with GPU-cached data loading...\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637234.1%\n",
      "   Hits/Misses: 168,993,963/141,759,765\n",
      "   Avg Load Time: 4079.1ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 50/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.6 samples/s\n",
      "   Data Load Time: 4965.2ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.9ms, Backward: 160.3ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637161.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 100/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.0 samples/s\n",
      "   Data Load Time: 5101.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.0ms, Backward: 162.7ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637273.6%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637376.3%\n",
      "   Hits/Misses: 171,666,825/144,006,281\n",
      "   Avg Load Time: 4079.0ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 150/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.4 samples/s\n",
      "   Data Load Time: 5026.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 66.7ms, Backward: 162.0ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637495.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 200/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 99.1 samples/s\n",
      "   Data Load Time: 4940.0ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.3ms, Backward: 160.7ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637228.5%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637168.3%\n",
      "   Hits/Misses: 174,290,454/146,211,912\n",
      "   Avg Load Time: 4078.3ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 250/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.8 samples/s\n",
      "   Data Load Time: 5060.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 162.5ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637315.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 300/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.9 samples/s\n",
      "   Data Load Time: 5001.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.5ms, Backward: 161.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637433.6%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637360.4%\n",
      "   Hits/Misses: 176,940,509/148,431,003\n",
      "   Avg Load Time: 4078.6ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 350/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 98.7 samples/s\n",
      "   Data Load Time: 4962.1ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 161.0ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637287.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 400/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.5 samples/s\n",
      "   Data Load Time: 5021.7ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.9ms, Backward: 161.8ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637174.8%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637175.4%\n",
      "   Hits/Misses: 179,565,276/150,630,784\n",
      "   Avg Load Time: 4078.5ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 450/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 97.5 samples/s\n",
      "   Data Load Time: 5023.3ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.7ms, Backward: 161.7ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637273.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 500/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 96.8 samples/s\n",
      "   Data Load Time: 5059.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 65.8ms, Backward: 162.2ms, Optimizer: 1.0ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637406.4%\n",
      "\n",
      "üìä GPU Cache Performance:\n",
      "   Hit Rate: 2637387.7%\n",
      "   Hits/Misses: 182,217,114/152,855,316\n",
      "   Avg Load Time: 4078.7ms\n",
      "   Coverage: 69.2% of dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Batch 550/800 Statistics:\n",
      "   Average Loss: nan\n",
      "   Throughput: 90.9 samples/s\n",
      "   Data Load Time: 5381.9ms (Was ~900ms, now should be <50ms!)\n",
      "   Forward: 76.9ms, Backward: 169.5ms, Optimizer: 1.1ms\n",
      "   Memory: GPU0: 1.1GB/95.0GB (1.2%)\n",
      "   üéØ Cache Hit Rate: 2637664.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# FINAL TRAINING: GPU-Cached Training with Massive Speedup\n",
    "# Uses GPU memory caching for 10x+ faster data loading\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üöÄ FINAL GPU-CACHED TRAINING WITH MASSIVE SPEEDUP!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use GPUs 1 and 2 - Two RTX 2060 SUPER GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "\n",
    "# Disable NCCL for stability\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '0'\n",
    "\n",
    "torch.cuda.set_device(0)  # Device 0 now maps to physical GPU 1\n",
    "\n",
    "# Verify GPU setup\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Using {num_gpus} GPUs:\")\n",
    "for i in range(num_gpus):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    mem_gb = props.total_memory / 1024**3\n",
    "    print(f\"  Device {i}: {torch.cuda.get_device_name(i)} ({mem_gb:.1f}GB)\")\n",
    "\n",
    "# Check available memory for caching\n",
    "device = torch.device('cuda:0')\n",
    "torch.cuda.empty_cache()\n",
    "available_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "print(f\"\\nüíæ Available GPU memory: {available_memory:.1f}GB\")\n",
    "\n",
    "# OPTIMIZED CONFIGURATION - Now with GPU caching\n",
    "final_config = {\n",
    "    # Research-critical parameters\n",
    "    'hidden_dim': 256,\n",
    "    'heads': 8,\n",
    "    'dropout': 0.6,\n",
    "    'num_layers': 3,\n",
    "    'lr': 0.005,\n",
    "    'weight_decay': 5e-4,\n",
    "    'gradient_clip': 1.0,\n",
    "    'label_smoothing': 0.1,\n",
    "    'num_neighbors': [25, 20, 15],\n",
    "    \n",
    "    # GPU caching configuration (MAJOR SPEEDUP!)\n",
    "    'use_gpu_cache': True,\n",
    "    'cache_size_gb': min(50, available_memory * 0.6),  # Use 60% of GPU memory for cache\n",
    "    'batch_size': 512,  # Larger batch since we have fast GPU cache\n",
    "    'accumulation_steps': 2,    # Reduced since batch is larger\n",
    "    'use_amp': True,\n",
    "    \n",
    "    # Training parameters\n",
    "    'max_epochs': 50,\n",
    "    'validation_frequency': 5,\n",
    "    'early_stopping_patience': 10,\n",
    "    'checkpoint_dir': './final_checkpoints',\n",
    "    \n",
    "    # Logging parameters\n",
    "    'log_interval': 10,  # Log every N batches\n",
    "    'detailed_log_interval': 50,  # Detailed log every N batches\n",
    "    'cache_stats_interval': 100,  # Cache stats every N batches\n",
    "}\n",
    "\n",
    "os.makedirs(final_config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "print(\"\\nüìä Optimized Configuration:\")\n",
    "print(f\"  GPU Cache Size: {final_config['cache_size_gb']:.1f}GB\")\n",
    "print(f\"  Batch Size: {final_config['batch_size']} (increased for GPU cache)\")\n",
    "print(f\"  Effective Batch Size: {final_config['batch_size'] * final_config['accumulation_steps']}\")\n",
    "print(f\"  Expected Speedup: 10-20x faster data loading!\")\n",
    "print(f\"  Learning Rate: {final_config['lr']}\")\n",
    "\n",
    "# Create model\n",
    "print(\"\\nüß† Setting up model...\")\n",
    "\n",
    "# Adjust num_classes if needed\n",
    "if num_classes % final_config['heads'] != 0:\n",
    "    adjusted_classes = ((num_classes + final_config['heads'] - 1) // final_config['heads']) * final_config['heads']\n",
    "    print(f\"   Adjusting classes: {num_classes} ‚Üí {adjusted_classes}\")\n",
    "    num_classes = adjusted_classes\n",
    "\n",
    "# Get metadata\n",
    "metadata = data.metadata()\n",
    "print(f\"   Model metadata: {metadata}\")\n",
    "\n",
    "# Create research-optimal model with explicit metadata\n",
    "model = ResearchOptimalHGT(\n",
    "    in_dim=None,\n",
    "    hidden_dim=final_config['hidden_dim'],\n",
    "    out_dim=num_classes,\n",
    "    metadata=metadata,\n",
    "    heads=final_config['heads'],\n",
    "    dropout=final_config['dropout'],\n",
    "    num_layers=final_config['num_layers']\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize model with a sample batch\n",
    "print(\"   Initializing model with sample batch...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Create a temporary sampler for initialization\n",
    "        if final_config['use_gpu_cache']:\n",
    "            print(\"   Using GPU-cached sampler for initialization...\")\n",
    "            temp_sampler = GPUCachedSampler(\n",
    "                disk_data, \n",
    "                batch_size=64, \n",
    "                num_neighbors=[5, 5],\n",
    "                cache_size_gb=1,  # Small cache for init\n",
    "                device=device\n",
    "            )\n",
    "        else:\n",
    "            temp_sampler = DiskBasedSampler(disk_data, batch_size=64, num_neighbors=[5, 5])\n",
    "        \n",
    "        sample_batch = temp_sampler.create_minibatch(train_idx[:64].tolist(), force_edges=True)\n",
    "        if not final_config['use_gpu_cache']:\n",
    "            sample_batch = sample_batch.to(device)\n",
    "        \n",
    "        # Run forward pass to initialize lazy modules\n",
    "        _ = model(sample_batch.x_dict, sample_batch.edge_index_dict)\n",
    "        print(\"   ‚úÖ Model initialized successfully\")\n",
    "        \n",
    "        # Clean up temp sampler\n",
    "        del temp_sampler\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Model initialization failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nüìà Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: {total_params * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=final_config['lr'],\n",
    "    weight_decay=final_config['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=2, eta_min=1e-5\n",
    ")\n",
    "\n",
    "# Create GPU-cached samplers (THE GAME CHANGER!)\n",
    "print(f\"\\nüöÄ Creating GPU-cached data samplers with {final_config['cache_size_gb']:.1f}GB cache...\")\n",
    "print(\"   This will dramatically speed up training by eliminating disk I/O!\")\n",
    "\n",
    "if final_config['use_gpu_cache']:\n",
    "    train_sampler = GPUCachedSampler(\n",
    "        disk_data,\n",
    "        batch_size=final_config['batch_size'],\n",
    "        num_neighbors=final_config['num_neighbors'],\n",
    "        cache_size_gb=final_config['cache_size_gb'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    val_sampler = GPUCachedSampler(\n",
    "        disk_data,\n",
    "        batch_size=final_config['batch_size'],\n",
    "        num_neighbors=final_config['num_neighbors'],\n",
    "        cache_size_gb=min(10, final_config['cache_size_gb'] * 0.2),  # Smaller cache for validation\n",
    "        device=device\n",
    "    )\n",
    "    print(\"   ‚úÖ GPU-cached samplers ready for massive speedup!\")\n",
    "else:\n",
    "    # Fallback to disk-based samplers\n",
    "    train_sampler = DiskBasedSampler(\n",
    "        disk_data,\n",
    "        batch_size=final_config['batch_size'],\n",
    "        num_neighbors=final_config['num_neighbors']\n",
    "    )\n",
    "    val_sampler = DiskBasedSampler(\n",
    "        disk_data,\n",
    "        batch_size=final_config['batch_size'],\n",
    "        num_neighbors=final_config['num_neighbors']\n",
    "    )\n",
    "    print(\"   Using disk-based samplers (fallback)\")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if final_config['use_amp'] else None\n",
    "\n",
    "# Enhanced memory monitoring\n",
    "def get_gpu_memory_info():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    return f\"GPU0: {allocated:.1f}GB/{total:.1f}GB ({allocated/total:.1%})\"\n",
    "\n",
    "# Enhanced training metrics with cache tracking\n",
    "class EnhancedTrainingMetrics:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.losses = []\n",
    "        self.batch_times = []\n",
    "        self.data_load_times = []\n",
    "        self.forward_times = []\n",
    "        self.backward_times = []\n",
    "        self.optimizer_times = []\n",
    "        self.batch_sizes = []\n",
    "        self.cache_hit_rates = []\n",
    "        \n",
    "    def update(self, loss, batch_size, times, cache_hit_rate=None):\n",
    "        self.losses.append(loss)\n",
    "        self.batch_sizes.append(batch_size)\n",
    "        if cache_hit_rate is not None:\n",
    "            self.cache_hit_rates.append(cache_hit_rate)\n",
    "        for key, value in times.items():\n",
    "            getattr(self, f\"{key}_times\").append(value)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        if not self.losses:\n",
    "            return {}\n",
    "        \n",
    "        total_samples = sum(self.batch_sizes)\n",
    "        avg_loss = sum(l * s for l, s in zip(self.losses, self.batch_sizes)) / total_samples\n",
    "        \n",
    "        summary = {\n",
    "            'avg_loss': avg_loss,\n",
    "            'total_samples': total_samples,\n",
    "            'avg_batch_time': np.mean(self.batch_times),\n",
    "            'avg_data_load_time': np.mean(self.data_load_times),\n",
    "            'avg_forward_time': np.mean(self.forward_times),\n",
    "            'avg_backward_time': np.mean(self.backward_times),\n",
    "            'avg_optimizer_time': np.mean(self.optimizer_times),\n",
    "            'throughput': total_samples / sum(self.batch_times) if self.batch_times else 0\n",
    "        }\n",
    "        \n",
    "        if self.cache_hit_rates:\n",
    "            summary['avg_cache_hit_rate'] = np.mean(self.cache_hit_rates)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Enhanced training function with cache monitoring\n",
    "def train_epoch(epoch):\n",
    "    model.train()\n",
    "    metrics = EnhancedTrainingMetrics()\n",
    "    \n",
    "    batches_per_epoch = min(800, len(train_idx) // final_config['batch_size'])\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(range(batches_per_epoch), desc=f'Epoch {epoch}')\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nüî• Starting epoch {epoch} with GPU-cached data loading...\")\n",
    "    if hasattr(train_sampler, 'print_cache_stats'):\n",
    "        train_sampler.print_cache_stats()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_sampler.get_batches(train_idx, shuffle=True)):\n",
    "        if batch_idx >= batches_per_epoch:\n",
    "            break\n",
    "        \n",
    "        times = {}\n",
    "        \n",
    "        try:\n",
    "            # Data loading time (should be MUCH faster now!)\n",
    "            data_load_time = time.time() - batch_start_time\n",
    "            times['data_load'] = data_load_time\n",
    "            \n",
    "            # No need to move batch to GPU - it's already there with GPU cache!\n",
    "            if not final_config['use_gpu_cache']:\n",
    "                batch = batch.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward pass\n",
    "            forward_start = time.time()\n",
    "            with autocast(enabled=final_config['use_amp']):\n",
    "                out_dict = model(batch.x_dict, batch.edge_index_dict)\n",
    "                \n",
    "                target_mask = batch['paper'].target_mask\n",
    "                if target_mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                paper_out = out_dict['paper'][target_mask][:, :num_classes]\n",
    "                paper_labels = batch['paper'].y[target_mask]\n",
    "                \n",
    "                loss = F.cross_entropy(\n",
    "                    paper_out, \n",
    "                    paper_labels, \n",
    "                    label_smoothing=final_config['label_smoothing']\n",
    "                )\n",
    "                loss = loss / final_config['accumulation_steps']\n",
    "            \n",
    "            forward_time = time.time() - forward_start\n",
    "            times['forward'] = forward_time\n",
    "            \n",
    "            # Backward pass\n",
    "            backward_start = time.time()\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            backward_time = time.time() - backward_start\n",
    "            times['backward'] = backward_time\n",
    "            \n",
    "            # Gradient accumulation and optimizer step\n",
    "            optimizer_start = time.time()\n",
    "            if (batch_idx + 1) % final_config['accumulation_steps'] == 0:\n",
    "                if scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), final_config['gradient_clip'])\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), final_config['gradient_clip'])\n",
    "                    optimizer.step()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "            else:\n",
    "                grad_norm = 0.0\n",
    "            \n",
    "            optimizer_time = time.time() - optimizer_start\n",
    "            times['optimizer'] = optimizer_time\n",
    "            \n",
    "            # Total batch time\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            times['batch'] = batch_time\n",
    "            \n",
    "            # Get cache hit rate if available\n",
    "            cache_hit_rate = None\n",
    "            if hasattr(train_sampler, 'get_cache_stats'):\n",
    "                cache_stats = train_sampler.get_cache_stats()\n",
    "                cache_hit_rate = cache_stats.get('cache_hit_rate', 0)\n",
    "            \n",
    "            # Update metrics\n",
    "            batch_size = target_mask.sum().item()\n",
    "            metrics.update(float(loss) * final_config['accumulation_steps'], batch_size, times, cache_hit_rate)\n",
    "            \n",
    "            # Update progress bar with enhanced info\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Regular logging\n",
    "            if batch_idx % final_config['log_interval'] == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                postfix = {\n",
    "                    'loss': f'{float(loss) * final_config[\"accumulation_steps\"]:.4f}',\n",
    "                    'lr': f'{current_lr:.6f}',\n",
    "                    'samples/s': f'{batch_size / batch_time:.1f}',\n",
    "                    'mem': get_gpu_memory_info()\n",
    "                }\n",
    "                \n",
    "                if cache_hit_rate is not None:\n",
    "                    postfix['cache'] = f'{cache_hit_rate:.1%}'\n",
    "                \n",
    "                pbar.set_postfix(postfix)\n",
    "            \n",
    "            # Detailed logging with cache stats\n",
    "            if batch_idx % final_config['detailed_log_interval'] == 0 and batch_idx > 0:\n",
    "                summary = metrics.get_summary()\n",
    "                print(f\"\\nüìä Batch {batch_idx}/{batches_per_epoch} Statistics:\")\n",
    "                print(f\"   Average Loss: {summary['avg_loss']:.4f}\")\n",
    "                print(f\"   Throughput: {summary['throughput']:.1f} samples/s\")\n",
    "                print(f\"   Data Load Time: {summary['avg_data_load_time']*1000:.1f}ms (Was ~900ms, now should be <50ms!)\")\n",
    "                print(f\"   Forward: {summary['avg_forward_time']*1000:.1f}ms, \"\n",
    "                      f\"Backward: {summary['avg_backward_time']*1000:.1f}ms, \"\n",
    "                      f\"Optimizer: {summary['avg_optimizer_time']*1000:.1f}ms\")\n",
    "                print(f\"   Memory: {get_gpu_memory_info()}\")\n",
    "                \n",
    "                if 'avg_cache_hit_rate' in summary:\n",
    "                    print(f\"   üéØ Cache Hit Rate: {summary['avg_cache_hit_rate']:.1%}\")\n",
    "                \n",
    "                metrics.reset()\n",
    "            \n",
    "            # Cache stats logging\n",
    "            if batch_idx % final_config['cache_stats_interval'] == 0 and batch_idx > 0:\n",
    "                if hasattr(train_sampler, 'print_cache_stats'):\n",
    "                    train_sampler.print_cache_stats()\n",
    "            \n",
    "            # Prepare for next batch\n",
    "            batch_start_time = time.time()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error in batch {batch_idx}: {e}\")\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(\"‚ö†Ô∏è  GPU OOM detected. Clearing cache and continuing...\")\n",
    "                torch.cuda.empty_cache()\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Epoch summary with cache performance\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_summary = metrics.get_summary()\n",
    "    \n",
    "    print(f\"\\nüìà Epoch {epoch} Summary:\")\n",
    "    print(f\"   Total Time: {epoch_time:.1f}s\")\n",
    "    print(f\"   Average Loss: {epoch_summary.get('avg_loss', 0):.4f}\")\n",
    "    print(f\"   Total Samples: {epoch_summary.get('total_samples', 0):,}\")\n",
    "    print(f\"   Average Throughput: {epoch_summary.get('throughput', 0):.1f} samples/s\")\n",
    "    print(f\"   Data Load Time: {epoch_summary.get('avg_data_load_time', 0)*1000:.1f}ms\")\n",
    "    \n",
    "    if 'avg_cache_hit_rate' in epoch_summary:\n",
    "        print(f\"   üéØ Average Cache Hit Rate: {epoch_summary['avg_cache_hit_rate']:.1%}\")\n",
    "    \n",
    "    if hasattr(train_sampler, 'print_cache_stats'):\n",
    "        train_sampler.print_cache_stats()\n",
    "    \n",
    "    return epoch_summary.get('avg_loss', float('inf'))\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    val_batches = min(100, len(val_idx) // final_config['batch_size'])\n",
    "    \n",
    "    val_start_time = time.time()\n",
    "    \n",
    "    print(\"\\nüîç Running validation with GPU cache...\")\n",
    "    for i, batch in enumerate(tqdm(val_sampler.get_batches(val_idx, shuffle=False), \n",
    "                                  desc='Validating', total=val_batches)):\n",
    "        if i >= val_batches:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            if not final_config['use_gpu_cache']:\n",
    "                batch = batch.to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast(enabled=final_config['use_amp']):\n",
    "                out_dict = model(batch.x_dict, batch.edge_index_dict)\n",
    "                    \n",
    "                target_mask = batch['paper'].target_mask\n",
    "                \n",
    "                if target_mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                paper_out = out_dict['paper'][target_mask][:, :num_classes]\n",
    "                paper_labels = batch['paper'].y[target_mask]\n",
    "                loss = F.cross_entropy(paper_out, paper_labels)\n",
    "                \n",
    "                pred = paper_out.argmax(dim=-1)\n",
    "                correct = (pred == paper_labels).sum().item()\n",
    "            \n",
    "            batch_size = target_mask.sum().item()\n",
    "            total_loss += float(loss) * batch_size\n",
    "            total_correct += correct\n",
    "            total_examples += batch_size\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Validation error in batch {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    val_time = time.time() - val_start_time\n",
    "    val_loss = total_loss / max(1, total_examples)\n",
    "    val_acc = total_correct / max(1, total_examples)\n",
    "    \n",
    "    print(f\"\\nüìä Validation Results:\")\n",
    "    print(f\"   Time: {val_time:.1f}s\")\n",
    "    print(f\"   Loss: {val_loss:.4f}\")\n",
    "    print(f\"   Accuracy: {val_acc:.4%}\")\n",
    "    print(f\"   Total Samples: {total_examples:,}\")\n",
    "    \n",
    "    if hasattr(val_sampler, 'print_cache_stats'):\n",
    "        val_sampler.print_cache_stats()\n",
    "    \n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Checkpoint management (unchanged)\n",
    "def save_checkpoint(epoch, train_loss, val_loss, val_acc, is_best=False):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'config': final_config,\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, os.path.join(final_config['checkpoint_dir'], 'latest.pt'))\n",
    "    \n",
    "    if is_best:\n",
    "        torch.save(checkpoint, os.path.join(final_config['checkpoint_dir'], 'best.pt'))\n",
    "        print(f\"üíæ New best model saved! Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    checkpoint_path = os.path.join(final_config['checkpoint_dir'], 'latest.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"üìÇ Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        print(f\"   Resumed from epoch {checkpoint['epoch']}\")\n",
    "        print(f\"   Previous train loss: {checkpoint['train_loss']:.4f}\")\n",
    "        print(f\"   Previous val loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
    "        print(f\"   Previous val acc: {checkpoint.get('val_acc', 0):.4%}\")\n",
    "        return checkpoint['epoch']\n",
    "    return 0\n",
    "\n",
    "# MAIN TRAINING LOOP WITH GPU CACHING\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÉ STARTING GPU-CACHED TRAINING - EXPECT MASSIVE SPEEDUP!\")\n",
    "print(f\"   Cache Size: {final_config['cache_size_gb']:.1f}GB\")\n",
    "print(f\"   Batch Size: {final_config['batch_size']}\")\n",
    "print(f\"   Expected Data Load Time: <50ms (was ~900ms)\")\n",
    "print(f\"   Expected Throughput: 3000+ samples/s (was ~340)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to resume from checkpoint\n",
    "start_epoch = load_checkpoint()\n",
    "\n",
    "# Clear GPU cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Training history\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "training_start = datetime.now()\n",
    "\n",
    "print(f\"\\nüéØ Training for {final_config['max_epochs']} epochs with GPU caching...\")\n",
    "print(f\"   Early stopping patience: {final_config['early_stopping_patience']}\")\n",
    "print(f\"   Validation frequency: Every {final_config['validation_frequency']} epochs\")\n",
    "\n",
    "for epoch in range(start_epoch + 1, final_config['max_epochs'] + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìÖ EPOCH {epoch}/{final_config['max_epochs']} - GPU CACHED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train with GPU caching\n",
    "    train_loss = train_epoch(epoch)\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validate periodically\n",
    "    if epoch % final_config['validation_frequency'] == 0:\n",
    "        val_loss, val_acc = validate()\n",
    "        \n",
    "        # Check if best\n",
    "        is_best = val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, train_loss, val_loss, val_acc, is_best)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä Epoch {epoch} Complete:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} {'üèÜ NEW BEST!' if is_best else ''}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4%}\")\n",
    "        print(f\"  Epoch Time: {time.time() - epoch_start:.1f}s\")\n",
    "        print(f\"  Memory: {get_gpu_memory_info()}\")\n",
    "        print(f\"  Patience: {patience_counter}/{final_config['early_stopping_patience']}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= final_config['early_stopping_patience']:\n",
    "            print(\"üõë Early stopping triggered!\")\n",
    "            break\n",
    "    else:\n",
    "        # Save checkpoint even without validation\n",
    "        save_checkpoint(epoch, train_loss, best_val_loss, best_val_acc, is_best=False)\n",
    "        print(f\"\\n‚úÖ Epoch {epoch} complete: Train Loss={train_loss:.4f}, Time={time.time() - epoch_start:.1f}s\")\n",
    "    \n",
    "    # Clear cache periodically\n",
    "    if epoch % 5 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üßπ Cleared GPU cache\")\n",
    "\n",
    "# Training complete\n",
    "total_time = (datetime.now() - training_start).total_seconds()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üéâ GPU-CACHED TRAINING COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Total time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc:.4%}\")\n",
    "print(f\"  Final memory usage: {get_gpu_memory_info()}\")\n",
    "print(f\"  Completed epochs: {epoch}/{final_config['max_epochs']}\")\n",
    "\n",
    "# Final cache statistics\n",
    "if hasattr(train_sampler, 'print_cache_stats'):\n",
    "    print(f\"\\nüìä Final Cache Performance:\")\n",
    "    train_sampler.print_cache_stats()\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"üöÄ GPU caching should have provided 10-20x speedup for data loading!\")\n",
    "print(\"üöÄ Check the data load times - they should be <50ms instead of ~900ms!\")\n",
    "\n",
    "# Reset CUDA device selection\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
