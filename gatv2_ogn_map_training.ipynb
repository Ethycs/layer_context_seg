{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wvc0hynpbwb",
   "metadata": {},
   "outputs": [],
   "source": "# Data Loading Cell - Memory-mapped loading, no data in RAM\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import HGTConv\nfrom torch_geometric.data import HeteroData\nimport os\nimport numpy as np\nimport pandas as pd\nimport gzip\nimport gc\nimport warnings\nimport h5py\n\nwarnings.filterwarnings('ignore')\n\nclass DiskBasedOGBNMAG:\n    \"\"\"Memory-efficient OGBN-MAG dataset that keeps data on disk\"\"\"\n    \n    def __init__(self, data_dir='./data'):\n        self.data_dir = data_dir\n        self.cache_dir = os.path.join(data_dir, 'ogbn_mag_cache')\n        os.makedirs(self.cache_dir, exist_ok=True)\n        \n        # Prepare data files\n        self._prepare_data()\n        \n        # Open memory-mapped files\n        self._open_mmap_files()\n        \n    def _prepare_data(self):\n        \"\"\"Convert raw OGBN-MAG files to memory-mapped format if needed\"\"\"\n        print(\"📥 Preparing memory-mapped OGBN-MAG data...\")\n        \n        # Check if already prepared\n        if os.path.exists(os.path.join(self.cache_dir, 'metadata.npz')):\n            print(\"✅ Memory-mapped data already exists\")\n            return\n            \n        # Ensure raw data exists\n        ogbn_dir = os.path.join(self.data_dir, 'ogbn_mag', 'raw')\n        if not os.path.exists(ogbn_dir):\n            print(\"📦 Downloading OGBN-MAG...\")\n            from ogb.nodeproppred import PygNodePropPredDataset\n            temp_dataset = PygNodePropPredDataset('ogbn-mag', root=self.data_dir)\n            del temp_dataset\n            gc.collect()\n        \n        print(\"🔄 Converting to memory-mapped format...\")\n        \n        # Process paper features\n        feat_file = os.path.join(ogbn_dir, 'node-feat', 'paper', 'node-feat.csv.gz')\n        with gzip.open(feat_file, 'rt') as f:\n            paper_features = pd.read_csv(f, header=None).values.astype(np.float32)\n        \n        # Save as memory-mapped\n        mmap_feat = np.memmap(os.path.join(self.cache_dir, 'paper_features.dat'),\n                             dtype='float32', mode='w+', shape=paper_features.shape)\n        mmap_feat[:] = paper_features\n        mmap_feat.flush()\n        del paper_features, mmap_feat\n        \n        # Process labels\n        label_file = os.path.join(ogbn_dir, 'node-label', 'paper', 'node-label.csv.gz')\n        with gzip.open(label_file, 'rt') as f:\n            paper_labels = pd.read_csv(f, header=None).values.flatten().astype(np.int64)\n        \n        mmap_labels = np.memmap(os.path.join(self.cache_dir, 'paper_labels.dat'),\n                               dtype='int64', mode='w+', shape=paper_labels.shape)\n        mmap_labels[:] = paper_labels\n        mmap_labels.flush()\n        \n        # Process edges in chunks\n        edge_files = {\n            'cite': ('paper___cites___paper', 'edge.csv.gz'),\n            'author': ('author___writes___paper', 'edge.csv.gz'),\n            'field': ('paper___has_topic___field_of_study', 'edge.csv.gz')\n        }\n        \n        edge_counts = {}\n        for edge_type, (rel_dir, filename) in edge_files.items():\n            edge_file = os.path.join(ogbn_dir, 'relations', rel_dir, filename)\n            with gzip.open(edge_file, 'rt') as f:\n                edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n            \n            # Save edges\n            mmap_edges = np.memmap(os.path.join(self.cache_dir, f'{edge_type}_edges.dat'),\n                                  dtype='int64', mode='w+', shape=edges.shape)\n            mmap_edges[:] = edges\n            mmap_edges.flush()\n            edge_counts[edge_type] = edges.shape[1]\n            del edges, mmap_edges\n        \n        # Calculate metadata\n        num_papers = len(paper_labels)\n        num_authors = 1134649  # From OGBN-MAG stats\n        num_fields = 59965\n        num_classes = int(paper_labels.max()) + 1\n        feat_dim = mmap_feat.shape[1] if 'mmap_feat' in locals() else 128\n        \n        # Load splits\n        split_dir = os.path.join(self.data_dir, 'ogbn_mag', 'split', 'time')\n        if os.path.exists(split_dir):\n            print(\"  Loading official splits...\")\n            train_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'train.csv.gz'), \n                                  header=None).values.flatten()\n            val_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'valid.csv.gz'), \n                                header=None).values.flatten()\n            test_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'test.csv.gz'), \n                                 header=None).values.flatten()\n        else:\n            indices = np.random.RandomState(42).permutation(num_papers)\n            train_size = int(0.8 * num_papers)\n            val_size = int(0.1 * num_papers)\n            train_idx = indices[:train_size]\n            val_idx = indices[train_size:train_size + val_size]\n            test_idx = indices[train_size + val_size:]\n        \n        # Save metadata\n        np.savez(os.path.join(self.cache_dir, 'metadata.npz'),\n                num_papers=num_papers,\n                num_authors=num_authors,\n                num_fields=num_fields,\n                num_classes=num_classes,\n                feat_dim=feat_dim,\n                train_idx=train_idx,\n                val_idx=val_idx,\n                test_idx=test_idx,\n                edge_counts=edge_counts)\n        \n        print(\"✅ Memory-mapped data prepared!\")\n        gc.collect()\n    \n    def _open_mmap_files(self):\n        \"\"\"Open memory-mapped files for reading\"\"\"\n        # Load metadata\n        metadata = np.load(os.path.join(self.cache_dir, 'metadata.npz'), allow_pickle=True)\n        self.num_papers = int(metadata['num_papers'])\n        self.num_authors = int(metadata['num_authors'])\n        self.num_fields = int(metadata['num_fields'])\n        self.num_classes = int(metadata['num_classes'])\n        self.feat_dim = int(metadata['feat_dim'])\n        self.train_idx = torch.from_numpy(metadata['train_idx'])\n        self.val_idx = torch.from_numpy(metadata['val_idx'])\n        self.test_idx = torch.from_numpy(metadata['test_idx'])\n        \n        # Open memory-mapped arrays (read-only)\n        self.paper_features = np.memmap(os.path.join(self.cache_dir, 'paper_features.dat'),\n                                       dtype='float32', mode='r', \n                                       shape=(self.num_papers, self.feat_dim))\n        \n        self.paper_labels = np.memmap(os.path.join(self.cache_dir, 'paper_labels.dat'),\n                                     dtype='int64', mode='r', shape=(self.num_papers,))\n        \n        # Open edge files\n        edge_counts = metadata['edge_counts'].item()\n        self.cite_edges = np.memmap(os.path.join(self.cache_dir, 'cite_edges.dat'),\n                                   dtype='int64', mode='r', shape=(2, edge_counts['cite']))\n        self.author_edges = np.memmap(os.path.join(self.cache_dir, 'author_edges.dat'),\n                                     dtype='int64', mode='r', shape=(2, edge_counts['author']))\n        self.field_edges = np.memmap(os.path.join(self.cache_dir, 'field_edges.dat'),\n                                    dtype='int64', mode='r', shape=(2, edge_counts['field']))\n        \n    def get_paper_batch(self, indices):\n        \"\"\"Load a batch of papers from disk\"\"\"\n        # Convert to numpy array for indexing\n        if isinstance(indices, torch.Tensor):\n            indices = indices.numpy()\n        \n        # Load only requested features and labels\n        features = torch.from_numpy(self.paper_features[indices].copy())\n        labels = torch.from_numpy(self.paper_labels[indices].copy())\n        \n        return features, labels\n    \n    def get_edges_for_nodes(self, node_indices, edge_type='cite'):\n        \"\"\"Get edges connected to specific nodes\"\"\"\n        if isinstance(node_indices, torch.Tensor):\n            node_indices = node_indices.numpy()\n        \n        # Select appropriate edge array\n        if edge_type == 'cite':\n            edges = self.cite_edges\n        elif edge_type == 'author':\n            edges = self.author_edges\n        else:\n            edges = self.field_edges\n        \n        # Find edges involving these nodes (this is still memory intensive for large graphs)\n        # In production, you'd want an index structure for this\n        node_set = set(node_indices.tolist())\n        mask = np.array([edges[0, i] in node_set or edges[1, i] in node_set \n                        for i in range(edges.shape[1])])\n        \n        if mask.any():\n            return torch.from_numpy(edges[:, mask].copy())\n        else:\n            return torch.empty(2, 0, dtype=torch.long)\n\n# Create disk-based dataset\nprint(\"🔄 Initializing disk-based OGBN-MAG dataset...\")\ndisk_data = DiskBasedOGBNMAG('./data')\n\n# Create a properly initialized HeteroData structure with all node and edge types\ndata = HeteroData()\ndata.num_classes = disk_data.num_classes\n\n# Initialize all node types with dummy data to ensure metadata() works correctly\n# This is crucial for HGTConv to properly initialize its edge_types_map\nprint(\"📊 Initializing HeteroData with all node and edge types...\")\n\n# Add dummy nodes for each type (just 1 node each to establish the types)\ndata['paper'].x = torch.randn(1, 128)  # Paper features\ndata['author'].x = torch.randn(1, 128)  # Author features\ndata['field_of_study'].x = torch.randn(1, 64)  # Field features\n\n# Add dummy edges for ALL edge types that HGTConv expects\n# This ensures HeteroData.metadata() returns the complete edge type list\ndata['author', 'writes', 'paper'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\ndata['paper', 'written_by', 'author'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\ndata['paper', 'has_topic', 'field_of_study'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\ndata['field_of_study', 'topic_of', 'paper'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\ndata['paper', 'cites', 'paper'].edge_index = torch.tensor([[0], [0]], dtype=torch.long)\n\n# Verify metadata is correct\nprint(\"✅ HeteroData metadata:\")\nprint(f\"   Node types: {data.node_types}\")\nprint(f\"   Edge types: {data.edge_types}\")\n\n# Store disk dataset reference\ndata._disk_data = disk_data\n\n# Training indices\ntrain_idx = disk_data.train_idx\nval_idx = disk_data.val_idx\nnum_classes = disk_data.num_classes\n\nprint(f\"\\n✅ Disk-based data ready!\")\nprint(f\"   Papers: {disk_data.num_papers:,} (on disk)\")\nprint(f\"   Authors: {disk_data.num_authors:,}\")\nprint(f\"   Fields: {disk_data.num_fields:,}\")\nprint(f\"   Classes: {num_classes}\")\nprint(f\"   Memory usage: Minimal - data remains on disk\")\n\n# For compatibility with existing code\ndata_dict = {\n    'num_papers': disk_data.num_papers,\n    'num_authors': disk_data.num_authors,\n    'num_fields': disk_data.num_fields,\n    'paper_features': disk_data.paper_features,  # This is a memory-mapped array\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k30g6r0wt3h",
   "metadata": {},
   "outputs": [],
   "source": "# Model Definition and Disk-Based Sampler\n\n# Disk-Based Memory-Efficient Sampler\nclass DiskBasedSampler:\n    \"\"\"Sampler that loads data from disk on-demand\"\"\"\n    def __init__(self, disk_data, batch_size=128, num_neighbors=[15, 10]):\n        self.disk_data = disk_data\n        self.batch_size = batch_size\n        self.num_neighbors = num_neighbors\n        \n        # Create edge index for fast neighbor lookup (this does use some memory)\n        # In production, you'd use a graph database or specialized index\n        print(\"Building neighbor index...\")\n        self._build_neighbor_index()\n        \n    def _build_neighbor_index(self):\n        \"\"\"Build a simple neighbor index for citation edges\"\"\"\n        # For true disk-based, this would be saved to disk too\n        # Here we just build a dict for the citation network\n        self.cite_neighbors = {}\n        \n        # Process in chunks to limit memory\n        chunk_size = 1000000\n        num_edges = self.disk_data.cite_edges.shape[1]\n        \n        for start in range(0, num_edges, chunk_size):\n            end = min(start + chunk_size, num_edges)\n            edges_chunk = self.disk_data.cite_edges[:, start:end]\n            \n            for i in range(edges_chunk.shape[1]):\n                src, dst = edges_chunk[0, i], edges_chunk[1, i]\n                if dst not in self.cite_neighbors:\n                    self.cite_neighbors[dst] = []\n                self.cite_neighbors[dst].append(src)\n        \n        print(f\"  Built index for {len(self.cite_neighbors)} nodes\")\n    \n    def sample_neighbors(self, node_id, num_samples):\n        \"\"\"Sample neighbors for a single node\"\"\"\n        if node_id not in self.cite_neighbors:\n            return []\n        \n        neighbors = self.cite_neighbors[node_id]\n        if len(neighbors) <= num_samples:\n            return neighbors\n        \n        # Random sample\n        indices = torch.randperm(len(neighbors))[:num_samples]\n        return [neighbors[i] for i in indices]\n    \n    def create_minibatch(self, target_nodes, force_edges=False):\n        \"\"\"Create a minibatch with ALL required edge types in EXACT metadata order\"\"\"\n        # Multi-hop sampling\n        all_paper_nodes = set(target_nodes)\n        current_layer = list(target_nodes)\n        \n        for num_samples in self.num_neighbors:\n            next_layer = set()\n            for node in current_layer:\n                neighbors = self.sample_neighbors(node, num_samples)\n                next_layer.update(neighbors)\n            \n            all_paper_nodes.update(next_layer)\n            current_layer = list(next_layer)\n        \n        # Convert to list for indexing\n        all_paper_nodes = list(all_paper_nodes)\n        num_paper_nodes = len(all_paper_nodes)\n        \n        # Load features and labels from disk\n        paper_features, paper_labels = self.disk_data.get_paper_batch(all_paper_nodes)\n        \n        # Create batch data structure\n        batch = HeteroData()\n        \n        # Paper data\n        batch['paper'].x = paper_features\n        batch['paper'].y = paper_labels\n        \n        # Create proper dummy nodes for other types - HGTConv needs all node types\n        num_authors = max(10, num_paper_nodes // 10)  # At least 10 authors\n        num_fields = max(5, num_paper_nodes // 20)    # At least 5 fields\n        \n        batch['author'].x = torch.randn(num_authors, 128)\n        batch['field_of_study'].x = torch.randn(num_fields, 64)\n        \n        # Create node mapping\n        node_mapping = {old: new for new, old in enumerate(all_paper_nodes)}\n        \n        # CRITICAL: Create ALL edge types in EXACT SAME ORDER as metadata\n        # HeteroData.metadata() will return edge types in the order they were added\n        # So we must match that order: [('author', 'writes', 'paper'), ('paper', 'written_by', 'author'),\n        #                               ('paper', 'has_topic', 'field_of_study'), ('field_of_study', 'topic_of', 'paper'), \n        #                               ('paper', 'cites', 'paper')]\n        \n        # 1. ('author', 'writes', 'paper') - FIRST in metadata\n        author_paper_edges = []\n        if force_edges or num_paper_nodes > 0:\n            for i in range(min(num_authors, num_paper_nodes)):\n                author_paper_edges.append([i, i % num_paper_nodes])\n        \n        if author_paper_edges:\n            batch['author', 'writes', 'paper'].edge_index = torch.tensor(author_paper_edges).T\n        else:\n            batch['author', 'writes', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long)\n        \n        # 2. ('paper', 'written_by', 'author') - SECOND in metadata\n        if author_paper_edges:\n            batch['paper', 'written_by', 'author'].edge_index = torch.tensor([[e[1], e[0]] for e in author_paper_edges]).T\n        else:\n            batch['paper', 'written_by', 'author'].edge_index = torch.empty(2, 0, dtype=torch.long)\n        \n        # 3. ('paper', 'has_topic', 'field_of_study') - THIRD in metadata\n        field_paper_edges = []\n        if force_edges or num_paper_nodes > 0:\n            for i in range(min(num_fields, num_paper_nodes)):\n                field_paper_edges.append([i, i % num_fields])  # paper -> field\n        \n        if field_paper_edges:\n            batch['paper', 'has_topic', 'field_of_study'].edge_index = torch.tensor(field_paper_edges).T\n        else:\n            batch['paper', 'has_topic', 'field_of_study'].edge_index = torch.empty(2, 0, dtype=torch.long)\n        \n        # 4. ('field_of_study', 'topic_of', 'paper') - FOURTH in metadata  \n        if field_paper_edges:\n            batch['field_of_study', 'topic_of', 'paper'].edge_index = torch.tensor([[e[1], e[0]] for e in field_paper_edges]).T\n        else:\n            batch['field_of_study', 'topic_of', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long)\n        \n        # 5. ('paper', 'cites', 'paper') - FIFTH in metadata (LAST!)\n        cite_edges = []\n        for i, node in enumerate(all_paper_nodes):\n            if node in self.cite_neighbors:\n                for neighbor in self.cite_neighbors[node]:\n                    if neighbor in node_mapping:\n                        cite_edges.append([node_mapping[neighbor], node_mapping[node]])\n        \n        if cite_edges:\n            batch['paper', 'cites', 'paper'].edge_index = torch.tensor(cite_edges).T\n        elif force_edges and num_paper_nodes >= 2:\n            batch['paper', 'cites', 'paper'].edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n        else:\n            batch['paper', 'cites', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long)\n        \n        # Mark target nodes\n        target_mask = torch.zeros(num_paper_nodes, dtype=torch.bool)\n        for node in target_nodes:\n            if node in node_mapping:\n                target_mask[node_mapping[node]] = True\n        batch['paper'].target_mask = target_mask\n        \n        return batch\n    \n    def get_batches(self, indices, shuffle=True):\n        \"\"\"Generate batches from indices\"\"\"\n        if shuffle:\n            perm = torch.randperm(len(indices))\n            indices = indices[perm]\n        \n        for i in range(0, len(indices), self.batch_size):\n            batch_indices = indices[i:i + self.batch_size]\n            yield self.create_minibatch(batch_indices.tolist())\n\n# Research-Optimized HGT Model with better initialization\nclass ResearchOptimalHGT(torch.nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, metadata, heads=8, dropout=0.6, num_layers=3):\n        super().__init__()\n        self.num_layers = num_layers\n        self.dropout = torch.nn.Dropout(dropout)\n        \n        # Store metadata for initialization\n        self.node_types = metadata[0]\n        self.edge_types = metadata[1]\n        \n        # Define input dimensions for each node type\n        self.in_dims = {\n            'paper': 128,  # OGBN-MAG paper features\n            'author': 128,  # Dummy features\n            'field_of_study': 64  # Dummy features\n        }\n        \n        self.convs = torch.nn.ModuleList()\n        self.norms = torch.nn.ModuleList()\n        self.residual_projs = torch.nn.ModuleList()\n        \n        # First layer - use actual dimensions\n        self.convs.append(HGTConv(self.in_dims, hidden_dim, metadata, heads=heads))\n        self.norms.append(torch.nn.LayerNorm(hidden_dim))\n        \n        # Hidden layers\n        for i in range(num_layers - 2):\n            self.convs.append(HGTConv(hidden_dim, hidden_dim, metadata, heads=heads))\n            self.norms.append(torch.nn.LayerNorm(hidden_dim))\n            self.residual_projs.append(torch.nn.Linear(hidden_dim, hidden_dim))\n        \n        # Output layer\n        self.convs.append(HGTConv(hidden_dim, out_dim, metadata, heads=1))\n        \n        self.use_residual = num_layers > 2\n        \n    def forward(self, x_dict, edge_index_dict):\n        # Ensure all node types are present\n        for node_type in self.node_types:\n            if node_type not in x_dict:\n                raise ValueError(f\"Missing node type '{node_type}' in x_dict\")\n        \n        # Ensure all edge types from metadata are present in the batch\n        for edge_type in self.edge_types:\n            if edge_type not in edge_index_dict:\n                print(f\"Warning: Missing edge type {edge_type}, adding empty tensor\")\n                edge_index_dict[edge_type] = torch.empty(2, 0, dtype=torch.long, device=list(x_dict.values())[0].device)\n        \n        # First layer\n        x_dict = self.convs[0](x_dict, edge_index_dict)\n        x_dict = {key: self.norms[0](x) for key, x in x_dict.items()}\n        x_dict = {key: F.leaky_relu(x, negative_slope=0.2) for key, x in x_dict.items()}\n        x_dict = {key: self.dropout(x) for key, x in x_dict.items()}\n        \n        # Hidden layers with residual\n        for i in range(1, self.num_layers - 1):\n            if self.use_residual:\n                x_dict_res = {k: v.clone() for k, v in x_dict.items()}\n            \n            x_dict = self.convs[i](x_dict, edge_index_dict)\n            x_dict = {key: self.norms[i](x) for key, x in x_dict.items()}\n            x_dict = {key: F.leaky_relu(x, negative_slope=0.2) for key, x in x_dict.items()}\n            x_dict = {key: self.dropout(x) for key, x in x_dict.items()}\n            \n            if self.use_residual:\n                for key in x_dict.keys():\n                    if key in x_dict_res:\n                        residual = self.residual_projs[i-1](x_dict_res[key])\n                        x_dict[key] = x_dict[key] + residual\n        \n        # Output layer\n        x_dict = self.convs[-1](x_dict, edge_index_dict)\n        \n        return x_dict\n\nprint(\"✅ Model and disk-based sampler classes defined!\")\n\n# The HeteroData object 'data' now has proper metadata built-in from the dummy edges we added\n# No need to override data.metadata anymore - it will return the correct metadata automatically"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6l8sd1f1uvh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 FINAL MULTI-GPU TRAINING WITH DISK-BASED LOADING\n",
      "============================================================\n",
      "Using 2 GPUs:\n",
      "  Device 0: NVIDIA RTX PRO 6000 Blackwell Workstation Edition (95.0GB)\n",
      "  Device 1: NVIDIA GeForce RTX 2060 SUPER (7.6GB)\n",
      "\n",
      "📊 Configuration:\n",
      "  Total batch size: 384\n",
      "  Data loading: From disk on-demand\n",
      "  Memory usage: Minimal\n",
      "  GPU setup: Two RTX 2060 SUPER GPUs\n",
      "\n",
      "🧠 Setting up model...\n",
      "   Adjusting classes: 349 → 352\n",
      "   Model metadata: ([], [])\n",
      "   Node types: []\n",
      "   Edge types: []\n",
      "   Initializing model with sample batch...\n",
      "Building neighbor index...\n",
      "  Built index for 629169 nodes\n",
      "   Sample batch edge types: [('author', 'writes', 'paper'), ('paper', 'written_by', 'author'), ('paper', 'has_topic', 'field_of_study'), ('field_of_study', 'topic_of', 'paper'), ('paper', 'cites', 'paper')]\n",
      "   Expected edge types: []\n",
      "   HGTConv edge_types_map: {}\n",
      "   ❌ Model initialization failed: ('author', 'writes', 'paper')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_58053/2799755780.py\", line 112, in <module>\n",
      "    _ = model(sample_batch.x_dict, sample_batch.edge_index_dict)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_58053/4268810785.py\", line 213, in forward\n",
      "    x_dict = self.convs[0](x_dict, edge_index_dict)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/hgt_conv.py\", line 192, in forward\n",
      "    k, v, src_offset = self._construct_src_node_feat(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/hgt_conv.py\", line 139, in _construct_src_node_feat\n",
      "    edge_type_offset = self.edge_types_map[edge_type]\n",
      "                       ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "KeyError: ('author', 'writes', 'paper')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('author', 'writes', 'paper')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    109\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   HGTConv edge_types_map: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhgt_conv.edge_types_map\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m         \u001b[38;5;66;03m# Run forward pass to initialize lazy modules\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m         _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   ✅ Model initialized successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 213\u001b[39m, in \u001b[36mResearchOptimalHGT.forward\u001b[39m\u001b[34m(self, x_dict, edge_index_dict)\u001b[39m\n\u001b[32m    210\u001b[39m         edge_index_dict[edge_type] = torch.empty(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, dtype=torch.long, device=\u001b[38;5;28mlist\u001b[39m(x_dict.values())[\u001b[32m0\u001b[39m].device)\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# First layer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m x_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m x_dict = {key: \u001b[38;5;28mself\u001b[39m.norms[\u001b[32m0\u001b[39m](x) \u001b[38;5;28;01mfor\u001b[39;00m key, x \u001b[38;5;129;01min\u001b[39;00m x_dict.items()}\n\u001b[32m    215\u001b[39m x_dict = {key: F.leaky_relu(x, negative_slope=\u001b[32m0.2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key, x \u001b[38;5;129;01min\u001b[39;00m x_dict.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/hgt_conv.py:192\u001b[39m, in \u001b[36mHGTConv.forward\u001b[39m\u001b[34m(self, x_dict, edge_index_dict)\u001b[39m\n\u001b[32m    189\u001b[39m     v_dict[key] = v.view(-\u001b[32m1\u001b[39m, H, D)\n\u001b[32m    191\u001b[39m q, dst_offset = \u001b[38;5;28mself\u001b[39m._cat(q_dict)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m k, v, src_offset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_construct_src_node_feat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m edge_index, edge_attr = construct_bipartite_edge_index(\n\u001b[32m    196\u001b[39m     edge_index_dict, src_offset, dst_offset, edge_attr_dict=\u001b[38;5;28mself\u001b[39m.p_rel,\n\u001b[32m    197\u001b[39m     num_nodes=k.size(\u001b[32m0\u001b[39m))\n\u001b[32m    199\u001b[39m out = \u001b[38;5;28mself\u001b[39m.propagate(edge_index, k=k, q=q, v=v, edge_attr=edge_attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch_geometric/nn/conv/hgt_conv.py:139\u001b[39m, in \u001b[36mHGTConv._construct_src_node_feat\u001b[39m\u001b[34m(self, k_dict, v_dict, edge_index_dict)\u001b[39m\n\u001b[32m    136\u001b[39m cumsum += N\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m# construct type_vec for curr edge_type with shape [H, D]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m edge_type_offset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43medge_types_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_type\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    140\u001b[39m type_vec = torch.arange(H, dtype=torch.long).view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m).repeat(\n\u001b[32m    141\u001b[39m     \u001b[32m1\u001b[39m, N) * num_edge_types + edge_type_offset\n\u001b[32m    143\u001b[39m type_list.append(type_vec)\n",
      "\u001b[31mKeyError\u001b[39m: ('author', 'writes', 'paper')"
     ]
    }
   ],
   "source": [
    "# FINAL TRAINING: Multi-GPU with Disk-Based Loading\n",
    "# Uses GPUs 1 and 2 - Two RTX 2060 SUPER GPUs\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"🚀 FINAL MULTI-GPU TRAINING WITH DISK-BASED LOADING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use GPUs 1 and 2 - Two RTX 2060 SUPER GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "\n",
    "# Disable NCCL for stability\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '0'\n",
    "\n",
    "torch.cuda.set_device(0)  # Device 0 now maps to physical GPU 1\n",
    "\n",
    "# Verify GPU setup\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Using {num_gpus} GPUs:\")\n",
    "for i in range(num_gpus):\n",
    "    print(f\"  Device {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB)\")\n",
    "\n",
    "# Force multi-GPU usage\n",
    "use_multi_gpu = True\n",
    "\n",
    "# FINAL CONFIGURATION - Optimized for two RTX 2060 SUPER GPUs\n",
    "final_config = {\n",
    "    # Research-critical parameters\n",
    "    'hidden_dim': 256,\n",
    "    'heads': 8,\n",
    "    'dropout': 0.6,\n",
    "    'num_layers': 3,\n",
    "    'lr': 0.005,\n",
    "    'weight_decay': 5e-4,\n",
    "    'gradient_clip': 1.0,\n",
    "    'label_smoothing': 0.1,\n",
    "    'num_neighbors': [25, 20, 15],\n",
    "    \n",
    "    # GPU optimization for two RTX 2060 SUPER GPUs\n",
    "    'batch_size_per_gpu': 192,  # Conservative for ~7.6GB GPUs\n",
    "    'accumulation_steps': 4,    # Maintain effective batch size\n",
    "    'use_amp': True,\n",
    "    \n",
    "    # Training parameters\n",
    "    'max_epochs': 50,\n",
    "    'validation_frequency': 5,\n",
    "    'early_stopping_patience': 10,\n",
    "    'checkpoint_dir': './final_checkpoints',\n",
    "}\n",
    "\n",
    "os.makedirs(final_config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "print(\"\\n📊 Configuration:\")\n",
    "print(f\"  Total batch size: {final_config['batch_size_per_gpu'] * num_gpus}\")\n",
    "print(f\"  Data loading: From disk on-demand\")\n",
    "print(f\"  Memory usage: Minimal\")\n",
    "print(f\"  GPU setup: Two RTX 2060 SUPER GPUs\")\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda:0')\n",
    "print(\"\\n🧠 Setting up model...\")\n",
    "\n",
    "# Adjust num_classes if needed\n",
    "if num_classes % final_config['heads'] != 0:\n",
    "    adjusted_classes = ((num_classes + final_config['heads'] - 1) // final_config['heads']) * final_config['heads']\n",
    "    print(f\"   Adjusting classes: {num_classes} → {adjusted_classes}\")\n",
    "    num_classes = adjusted_classes\n",
    "\n",
    "# DEBUG: Verify metadata before model creation\n",
    "metadata = data.metadata()\n",
    "print(f\"   Model metadata: {metadata}\")\n",
    "print(f\"   Node types: {metadata[0]}\")\n",
    "print(f\"   Edge types: {metadata[1]}\")\n",
    "\n",
    "# Create research-optimal model with explicit metadata\n",
    "model = ResearchOptimalHGT(\n",
    "    in_dim=None,  # Not used anymore, dimensions are hardcoded in the model\n",
    "    hidden_dim=final_config['hidden_dim'],\n",
    "    out_dim=num_classes,\n",
    "    metadata=metadata,  # Pass the actual metadata tuple\n",
    "    heads=final_config['heads'],\n",
    "    dropout=final_config['dropout'],\n",
    "    num_layers=final_config['num_layers']\n",
    ")\n",
    "\n",
    "# Move model to GPU first\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize model with a batch that has edges\n",
    "print(\"   Initializing model with sample batch...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Get a sample batch with forced edges for initialization\n",
    "        sample_sampler = DiskBasedSampler(disk_data, batch_size=64, num_neighbors=[5, 5])\n",
    "        sample_batch = sample_sampler.create_minibatch(train_idx[:64].tolist(), force_edges=True)\n",
    "        sample_batch = sample_batch.to(device)\n",
    "        \n",
    "        # Debug: Check edge types in batch\n",
    "        print(f\"   Sample batch edge types: {list(sample_batch.edge_index_dict.keys())}\")\n",
    "        print(f\"   Expected edge types: {metadata[1]}\")\n",
    "        \n",
    "        # Debug: Check HGTConv edge types map\n",
    "        hgt_conv = model.convs[0]\n",
    "        print(f\"   HGTConv edge_types_map: {hgt_conv.edge_types_map}\")\n",
    "        \n",
    "        # Run forward pass to initialize lazy modules\n",
    "        _ = model(sample_batch.x_dict, sample_batch.edge_index_dict)\n",
    "        print(\"   ✅ Model initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Model initialization failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Set up DataParallel for two RTX 2060 SUPER GPUs\n",
    "if use_multi_gpu and num_gpus > 1:\n",
    "    try:\n",
    "        print(\"\\n🔧 Setting up DataParallel for two RTX 2060 SUPER GPUs...\")\n",
    "        \n",
    "        # Use all available GPUs\n",
    "        device_ids = list(range(num_gpus))\n",
    "        print(f\"   Using device IDs: {device_ids}\")\n",
    "        \n",
    "        # Wrap model with DataParallel\n",
    "        model = DataParallel(model, device_ids=device_ids)\n",
    "        \n",
    "        # Quick test forward pass\n",
    "        print(\"   Testing DataParallel setup...\")\n",
    "        with torch.no_grad():\n",
    "            test_sampler = DiskBasedSampler(disk_data, batch_size=32, num_neighbors=[3, 3])\n",
    "            test_batch = test_sampler.create_minibatch(train_idx[:32].tolist(), force_edges=True)\n",
    "            test_batch = test_batch.to(device)\n",
    "            _ = model(test_batch.x_dict, test_batch.edge_index_dict)\n",
    "        \n",
    "        print(f\"✅ Model distributed across {num_gpus} RTX 2060 SUPER GPUs\")\n",
    "        actual_batch_size = final_config['batch_size_per_gpu'] * num_gpus\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  DataParallel failed: {e}\")\n",
    "        print(\"📌 Falling back to single GPU\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Remove DataParallel wrapper if it was added\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "        \n",
    "        use_multi_gpu = False\n",
    "        actual_batch_size = final_config['batch_size_per_gpu']\n",
    "else:\n",
    "    print(\"\\n📌 Using single GPU training\")\n",
    "    actual_batch_size = final_config['batch_size_per_gpu']\n",
    "\n",
    "print(f\"   Final batch size: {actual_batch_size}\")\n",
    "print(f\"   Multi-GPU enabled: {use_multi_gpu}\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=final_config['lr'],\n",
    "    weight_decay=final_config['weight_decay']\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=10, T_mult=2, eta_min=1e-5\n",
    ")\n",
    "\n",
    "# Create disk-based samplers\n",
    "print(\"\\n📊 Creating disk-based data samplers...\")\n",
    "train_sampler = DiskBasedSampler(\n",
    "    disk_data,\n",
    "    batch_size=actual_batch_size,\n",
    "    num_neighbors=final_config['num_neighbors']\n",
    ")\n",
    "\n",
    "val_sampler = DiskBasedSampler(\n",
    "    disk_data,\n",
    "    batch_size=actual_batch_size,\n",
    "    num_neighbors=final_config['num_neighbors']\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if final_config['use_amp'] else None\n",
    "\n",
    "# Memory monitoring for two RTX 2060 SUPER GPUs\n",
    "def get_gpu_memory_str():\n",
    "    if use_multi_gpu and num_gpus > 1:\n",
    "        mem_strs = []\n",
    "        for i in range(num_gpus):\n",
    "            alloc = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            mem_strs.append(f\"GPU{i}: {alloc:.1f}/{total:.1f}GB\")\n",
    "        return \" | \".join(mem_strs)\n",
    "    else:\n",
    "        alloc = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        return f\"GPU0: {alloc:.1f}/{total:.1f}GB\"\n",
    "\n",
    "# Training function\n",
    "def train_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    batches_per_epoch = min(800, len(train_idx) // actual_batch_size)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(range(batches_per_epoch), desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_sampler.get_batches(train_idx, shuffle=True)):\n",
    "        if batch_idx >= batches_per_epoch:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            batch = batch.to(device, non_blocking=True)\n",
    "            \n",
    "            # Mixed precision forward\n",
    "            with autocast(enabled=final_config['use_amp']):\n",
    "                out_dict = model(batch.x_dict, batch.edge_index_dict)\n",
    "                \n",
    "                target_mask = batch['paper'].target_mask\n",
    "                if target_mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                paper_out = out_dict['paper'][target_mask][:, :num_classes]\n",
    "                paper_labels = batch['paper'].y[target_mask]\n",
    "                \n",
    "                loss = F.cross_entropy(\n",
    "                    paper_out, \n",
    "                    paper_labels, \n",
    "                    label_smoothing=final_config['label_smoothing']\n",
    "                )\n",
    "                loss = loss / final_config['accumulation_steps']\n",
    "            \n",
    "            # Backward\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            \n",
    "            # Gradient accumulation and step\n",
    "            if (batch_idx + 1) % final_config['accumulation_steps'] == 0:\n",
    "                if scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), final_config['gradient_clip'])\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), final_config['gradient_clip'])\n",
    "                    optimizer.step()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Metrics\n",
    "            batch_size = target_mask.sum().item()\n",
    "            total_loss += float(loss) * batch_size * final_config['accumulation_steps']\n",
    "            total_examples += batch_size\n",
    "            \n",
    "            # Update progress\n",
    "            pbar.update(1)\n",
    "            if batch_idx % 50 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{total_loss/max(1, total_examples):.4f}',\n",
    "                    'lr': f'{optimizer.param_groups[0][\"lr\"]:.6f}',\n",
    "                    'mem': get_gpu_memory_str()\n",
    "                })\n",
    "            \n",
    "            # Periodic cache clearing\n",
    "            if batch_idx % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in batch {batch_idx}: {e}\")\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(\"⚠️  GPU OOM detected. Clearing cache and continuing...\")\n",
    "                torch.cuda.empty_cache()\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    pbar.close()\n",
    "    return total_loss / max(1, total_examples)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    val_batches = min(100, len(val_idx) // actual_batch_size)\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(val_sampler.get_batches(val_idx, shuffle=False), \n",
    "                                  desc='Validating', total=val_batches)):\n",
    "        if i >= val_batches:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            batch = batch.to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast(enabled=final_config['use_amp']):\n",
    "                out_dict = model(batch.x_dict, batch.edge_index_dict)\n",
    "                    \n",
    "                target_mask = batch['paper'].target_mask\n",
    "                \n",
    "                if target_mask.sum() == 0:\n",
    "                    continue\n",
    "                \n",
    "                paper_out = out_dict['paper'][target_mask][:, :num_classes]\n",
    "                paper_labels = batch['paper'].y[target_mask]\n",
    "                loss = F.cross_entropy(paper_out, paper_labels)\n",
    "                \n",
    "                pred = paper_out.argmax(dim=-1)\n",
    "                correct = (pred == paper_labels).sum().item()\n",
    "            \n",
    "            batch_size = target_mask.sum().item()\n",
    "            total_loss += float(loss) * batch_size\n",
    "            total_correct += correct\n",
    "            total_examples += batch_size\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    val_loss = total_loss / max(1, total_examples)\n",
    "    val_acc = total_correct / max(1, total_examples)\n",
    "    return val_loss, val_acc\n",
    "\n",
    "# Checkpoint management\n",
    "def save_checkpoint(epoch, train_loss, val_loss, val_acc, is_best=False):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'config': final_config,\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, os.path.join(final_config['checkpoint_dir'], 'latest.pt'))\n",
    "    \n",
    "    if is_best:\n",
    "        torch.save(checkpoint, os.path.join(final_config['checkpoint_dir'], 'best.pt'))\n",
    "        print(f\"💾 New best model saved! Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "def load_checkpoint():\n",
    "    checkpoint_path = os.path.join(final_config['checkpoint_dir'], 'latest.pt')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"📂 Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        return checkpoint['epoch']\n",
    "    return 0\n",
    "\n",
    "# MAIN TRAINING LOOP\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🏃 STARTING DISK-BASED TRAINING\")\n",
    "print(f\"   Mode: {'Multi-GPU' if use_multi_gpu else 'Single GPU'}\")\n",
    "print(f\"   Batch size: {actual_batch_size}\")\n",
    "print(f\"   GPUs: {num_gpus} RTX 2060 SUPER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to resume from checkpoint\n",
    "start_epoch = load_checkpoint()\n",
    "\n",
    "# Clear GPU cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Training history\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "training_start = datetime.now()\n",
    "\n",
    "for epoch in range(start_epoch + 1, final_config['max_epochs'] + 1):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(epoch)\n",
    "    \n",
    "    # Step scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validate periodically\n",
    "    if epoch % final_config['validation_frequency'] == 0:\n",
    "        val_loss, val_acc = validate()\n",
    "        \n",
    "        # Check if best\n",
    "        is_best = val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, train_loss, val_loss, val_acc, is_best)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f} {'🏆 NEW BEST!' if is_best else ''}\")\n",
    "        print(f\"  Val Accuracy: {val_acc:.4%}\")\n",
    "        print(f\"  Time: {time.time() - epoch_start:.1f}s\")\n",
    "        print(f\"  Memory: {get_gpu_memory_str()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= final_config['early_stopping_patience']:\n",
    "            print(\"🛑 Early stopping triggered!\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Time={time.time() - epoch_start:.1f}s\")\n",
    "    \n",
    "    # Clear cache periodically\n",
    "    if epoch % 5 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Training complete\n",
    "total_time = (datetime.now() - training_start).total_seconds()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✅ TRAINING COMPLETE!\")\n",
    "print(f\"  Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc:.4%}\")\n",
    "print(f\"  Memory: {get_gpu_memory_str()}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Reset CUDA device selection\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}