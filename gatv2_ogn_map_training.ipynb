{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1772293",
   "metadata": {},
   "outputs": [],
   "source": "# OGBN-MAG Minibatch Training - Memory-Efficient Sampling\n# True minibatch training without loading full graph into memory\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import HGTConv\nfrom torch_geometric.data import HeteroData\nimport os\nimport numpy as np\nimport pandas as pd\nimport gzip\nimport psutil\nimport gc\nimport warnings\n\nwarnings.filterwarnings('ignore')\nprint(\"🚀 Starting memory-efficient OGBN-MAG minibatch training...\")\n\n# --- 1. Data Loading (unchanged) ---\ndef ensure_ogbn_data_exists(data_dir):\n    \"\"\"Check if OGBN-MAG raw files exist, download if needed\"\"\"\n    ogbn_dir = os.path.join(data_dir, 'ogbn_mag', 'raw')\n    \n    if not os.path.exists(ogbn_dir):\n        print(\"📦 Downloading OGBN-MAG (this may take a while)...\")\n        from ogb.nodeproppred import PygNodePropPredDataset\n        \n        # Download to a temp location to avoid framework issues\n        temp_dataset = PygNodePropPredDataset('ogbn-mag', root=data_dir)\n        print(\"✅ Download complete!\")\n        del temp_dataset\n        gc.collect()\n    \n    return ogbn_dir\n\ndef load_ogbn_simple(data_dir):\n    \"\"\"Load OGBN-MAG data directly from files - no frameworks!\"\"\"\n    print(\"📥 Loading OGBN-MAG from raw files...\")\n    \n    # Ensure data exists\n    raw_dir = ensure_ogbn_data_exists(data_dir)\n    \n    # Load paper features\n    feat_file = os.path.join(raw_dir, 'node-feat', 'paper', 'node-feat.csv.gz')\n    print(\"  Loading paper features...\")\n    with gzip.open(feat_file, 'rt') as f:\n        paper_features = pd.read_csv(f, header=None).values.astype(np.float32)\n    \n    # Load paper labels\n    label_file = os.path.join(raw_dir, 'node-label', 'paper', 'node-label.csv.gz')\n    print(\"  Loading paper labels...\")\n    with gzip.open(label_file, 'rt') as f:\n        paper_labels = pd.read_csv(f, header=None).values.flatten().astype(np.int64)\n    \n    # Load citation edges\n    cite_file = os.path.join(raw_dir, 'relations', 'paper___cites___paper', 'edge.csv.gz')\n    print(\"  Loading citation edges...\")\n    with gzip.open(cite_file, 'rt') as f:\n        cite_edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n    \n    # Load author-paper edges (for heterogeneous graph)\n    author_file = os.path.join(raw_dir, 'relations', 'author___writes___paper', 'edge.csv.gz')\n    print(\"  Loading author-paper edges...\")\n    with gzip.open(cite_file, 'rt') as f:\n        author_paper_edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n    \n    # Load field-paper edges\n    field_file = os.path.join(raw_dir, 'relations', 'paper___has_topic___field_of_study', 'edge.csv.gz')\n    print(\"  Loading field-paper edges...\")\n    with gzip.open(field_file, 'rt') as f:\n        field_paper_edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n    \n    # Calculate node counts\n    num_papers = len(paper_features)\n    num_authors = author_paper_edges[0].max() + 1\n    num_fields = field_paper_edges[1].max() + 1\n    num_classes = int(paper_labels.max()) + 1\n    \n    # Create train/val/test splits (use official split if available)\n    split_dir = os.path.join(data_dir, 'ogbn_mag', 'split', 'time')\n    if os.path.exists(split_dir):\n        print(\"  Loading official splits...\")\n        train_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'train.csv.gz'), header=None).values.flatten()\n        val_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'valid.csv.gz'), header=None).values.flatten()\n        test_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'test.csv.gz'), header=None).values.flatten()\n    else:\n        print(\"  Creating random splits...\")\n        indices = np.random.RandomState(42).permutation(num_papers)\n        train_size = int(0.8 * num_papers)\n        val_size = int(0.1 * num_papers)\n        train_idx = indices[:train_size]\n        val_idx = indices[train_size:train_size + val_size]\n        test_idx = indices[train_size + val_size:]\n    \n    print(f\"✅ Loaded: {num_papers} papers, {num_authors} authors, {num_fields} fields\")\n    print(f\"   Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n    \n    return {\n        'paper_features': torch.from_numpy(paper_features),\n        'paper_labels': torch.from_numpy(paper_labels),\n        'paper_author_edges': torch.from_numpy(author_paper_edges),\n        'paper_field_edges': torch.from_numpy(field_paper_edges),\n        'paper_cite_edges': torch.from_numpy(cite_edges),\n        'train_idx': torch.from_numpy(train_idx),\n        'val_idx': torch.from_numpy(val_idx),\n        'test_idx': torch.from_numpy(test_idx),\n        'num_papers': num_papers,\n        'num_authors': num_authors,\n        'num_fields': num_fields,\n        'num_classes': num_classes\n    }\n\n# --- 2. Create HeteroData (unchanged) ---\ndef create_pyg_hetero_data(data_dict):\n    \"\"\"Create HeteroData using PyG's standard approach\"\"\"\n    print(\"🔗 Setting up PyG-compatible heterogeneous data...\")\n    \n    data = HeteroData()\n    \n    # Set node features\n    data['paper'].x = data_dict['paper_features']\n    data['paper'].y = data_dict['paper_labels']\n    data['author'].x = torch.randn(data_dict['num_authors'], 128)\n    data['field_of_study'].x = torch.randn(data_dict['num_fields'], 64)\n    \n    # Set edge indices\n    data['author', 'writes', 'paper'].edge_index = data_dict['paper_author_edges'].contiguous()\n    data['paper', 'written_by', 'author'].edge_index = data_dict['paper_author_edges'].flip(0).contiguous()\n    data['paper', 'has_topic', 'field_of_study'].edge_index = data_dict['paper_field_edges'].contiguous()\n    data['field_of_study', 'topic_of', 'paper'].edge_index = data_dict['paper_field_edges'].flip(0).contiguous()\n    data['paper', 'cites', 'paper'].edge_index = data_dict['paper_cite_edges'].contiguous()\n    \n    # Store additional info we need\n    data.num_classes = data_dict['num_classes']\n    data.train_idx = data_dict['train_idx']\n    \n    print(f\"✅ PyG hetero data ready!\")\n    print(f\"   Node types: {data.node_types}\")\n    print(f\"   Edge types: {data.edge_types}\")\n    \n    return data\n\n# --- 3. Memory-Efficient Batch Sampler ---\nclass MemoryEfficientSampler:\n    \"\"\"Memory-efficient sampler that uses PyTorch operations instead of Python dictionaries\"\"\"\n    def __init__(self, data, batch_size=128, num_neighbors=[15, 10]):\n        self.data = data\n        self.batch_size = batch_size\n        self.num_neighbors = num_neighbors\n        print(f\"   Created memory-efficient sampler (batch_size={batch_size})\")\n        \n    def sample_neighbors_tensor(self, node_ids, edge_index, num_samples, node_type='paper'):\n        \"\"\"Sample neighbors using tensor operations - no Python loops or dictionaries\"\"\"\n        if len(node_ids) == 0:\n            return torch.tensor([], dtype=torch.long)\n        \n        # Convert to tensor if needed\n        if isinstance(node_ids, list):\n            node_ids = torch.tensor(node_ids, dtype=torch.long)\n        \n        # Find all edges where our nodes are destinations\n        mask = torch.isin(edge_index[1], node_ids)\n        \n        if not mask.any():\n            return torch.tensor([], dtype=torch.long)\n        \n        # Get source nodes from these edges\n        sources = edge_index[0, mask]\n        destinations = edge_index[1, mask]\n        \n        # Group by destination and sample\n        sampled_neighbors = []\n        for node_id in node_ids:\n            # Get neighbors for this specific node\n            node_mask = destinations == node_id\n            node_neighbors = sources[node_mask]\n            \n            if len(node_neighbors) > 0:\n                # Sample up to num_samples neighbors\n                if len(node_neighbors) > num_samples:\n                    indices = torch.randperm(len(node_neighbors))[:num_samples]\n                    sampled = node_neighbors[indices]\n                else:\n                    sampled = node_neighbors\n                sampled_neighbors.append(sampled)\n        \n        if sampled_neighbors:\n            return torch.unique(torch.cat(sampled_neighbors))\n        else:\n            return torch.tensor([], dtype=torch.long)\n    \n    def create_minibatch(self, target_nodes):\n        \"\"\"Create a minibatch subgraph for target nodes using memory-efficient operations\"\"\"\n        device = self.data['paper'].x.device\n        \n        # Start with target paper nodes\n        target_nodes_tensor = torch.tensor(target_nodes, dtype=torch.long)\n        all_paper_nodes = [target_nodes_tensor]\n        \n        # Sample citation neighbors layer by layer\n        current_nodes = target_nodes_tensor\n        paper_cite_edges = self.data['paper', 'cites', 'paper'].edge_index\n        \n        for num_samples in self.num_neighbors:\n            # Sample neighbors for current layer\n            neighbors = self.sample_neighbors_tensor(\n                current_nodes, paper_cite_edges, num_samples\n            )\n            if len(neighbors) > 0:\n                all_paper_nodes.append(neighbors)\n                current_nodes = neighbors\n        \n        # Combine all paper nodes\n        all_paper_nodes = torch.unique(torch.cat(all_paper_nodes))\n        \n        # Find connected authors and fields (limit to reduce memory)\n        author_paper_edges = self.data['author', 'writes', 'paper'].edge_index\n        field_paper_edges = self.data['paper', 'has_topic', 'field_of_study'].edge_index\n        \n        # Get authors connected to our papers\n        author_mask = torch.isin(author_paper_edges[1], all_paper_nodes)\n        connected_authors = torch.unique(author_paper_edges[0, author_mask])\n        # Limit number of authors to avoid memory explosion\n        if len(connected_authors) > 1000:\n            indices = torch.randperm(len(connected_authors))[:1000]\n            connected_authors = connected_authors[indices]\n        \n        # Get fields connected to our papers\n        field_mask = torch.isin(field_paper_edges[0], all_paper_nodes)\n        connected_fields = torch.unique(field_paper_edges[1, field_mask])\n        # Limit number of fields\n        if len(connected_fields) > 200:\n            indices = torch.randperm(len(connected_fields))[:200]\n            connected_fields = connected_fields[indices]\n        \n        # Create node mappings\n        paper_mapping = {int(old): new for new, old in enumerate(all_paper_nodes.tolist())}\n        author_mapping = {int(old): new for new, old in enumerate(connected_authors.tolist())}\n        field_mapping = {int(old): new for new, old in enumerate(connected_fields.tolist())}\n        \n        # Create batch data\n        batch = HeteroData()\n        \n        # Add node features\n        batch['paper'].x = self.data['paper'].x[all_paper_nodes]\n        batch['paper'].y = self.data['paper'].y[all_paper_nodes]\n        \n        if len(connected_authors) > 0:\n            batch['author'].x = self.data['author'].x[connected_authors]\n        else:\n            batch['author'].x = torch.empty(0, 128)\n            \n        if len(connected_fields) > 0:\n            batch['field_of_study'].x = self.data['field_of_study'].x[connected_fields]\n        else:\n            batch['field_of_study'].x = torch.empty(0, 64)\n        \n        # Add edges (only those within sampled nodes) using tensor operations\n        for edge_type in self.data.edge_types:\n            edge_index = self.data[edge_type].edge_index\n            src_type, _, dst_type = edge_type\n            \n            # Get appropriate node sets and mappings\n            if src_type == 'paper':\n                src_nodes = all_paper_nodes\n                src_mapping = paper_mapping\n            elif src_type == 'author':\n                src_nodes = connected_authors\n                src_mapping = author_mapping\n            else:  # field_of_study\n                src_nodes = connected_fields\n                src_mapping = field_mapping\n                \n            if dst_type == 'paper':\n                dst_nodes = all_paper_nodes\n                dst_mapping = paper_mapping\n            elif dst_type == 'author':\n                dst_nodes = connected_authors\n                dst_mapping = author_mapping\n            else:  # field_of_study\n                dst_nodes = connected_fields\n                dst_mapping = field_mapping\n            \n            # Filter edges efficiently\n            if len(src_nodes) > 0 and len(dst_nodes) > 0:\n                src_mask = torch.isin(edge_index[0], src_nodes)\n                dst_mask = torch.isin(edge_index[1], dst_nodes)\n                edge_mask = src_mask & dst_mask\n                \n                if edge_mask.any():\n                    filtered_edges = edge_index[:, edge_mask]\n                    # Remap indices\n                    new_src = torch.tensor([src_mapping[int(idx)] for idx in filtered_edges[0].tolist()])\n                    new_dst = torch.tensor([dst_mapping[int(idx)] for idx in filtered_edges[1].tolist()])\n                    batch[edge_type].edge_index = torch.stack([new_src, new_dst])\n                else:\n                    batch[edge_type].edge_index = torch.empty(2, 0, dtype=torch.long)\n            else:\n                batch[edge_type].edge_index = torch.empty(2, 0, dtype=torch.long)\n        \n        # Mark target nodes\n        target_mask = torch.zeros(len(all_paper_nodes), dtype=torch.bool)\n        for i, node in enumerate(all_paper_nodes.tolist()):\n            if node in target_nodes:\n                target_mask[i] = True\n        batch['paper'].target_mask = target_mask\n        \n        return batch\n    \n    def get_batches(self, indices, shuffle=True):\n        \"\"\"Generate batches from indices\"\"\"\n        if shuffle:\n            perm = torch.randperm(len(indices))\n            indices = indices[perm]\n        \n        for i in range(0, len(indices), self.batch_size):\n            batch_indices = indices[i:i + self.batch_size]\n            yield self.create_minibatch(batch_indices.tolist())\n\n# --- 4. Load Data ---\nprint(\"\\n📂 Loading OGBN-MAG data...\")\ndata_dict = load_ogbn_simple('./data')\ndata = create_pyg_hetero_data(data_dict)\n\n# Get training indices\ntrain_idx = data.train_idx\nprint(f\"🎯 Training on {len(train_idx)} papers\")\n\n# --- 5. Model (unchanged) ---\nprint(\"\\n🧠 Creating model...\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"   Device: {device}\")\n\nnum_classes = data.num_classes\nhidden_dim = 128\nheads = 4\n\n# Ensure compatibility\nif num_classes % heads != 0:\n    adjusted_classes = ((num_classes + heads - 1) // heads) * heads\n    print(f\"   Adjusting classes: {num_classes} → {adjusted_classes}\")\n    num_classes = adjusted_classes\n\nclass SimpleHGT(torch.nn.Module):\n    def __init__(self, hidden_dim, out_dim, metadata, heads=4):\n        super().__init__()\n        self.conv1 = HGTConv(-1, hidden_dim, metadata, heads=heads)\n        self.conv2 = HGTConv(hidden_dim, out_dim, metadata, heads=heads)\n        self.dropout = torch.nn.Dropout(0.3)\n    \n    def forward(self, x_dict, edge_index_dict):\n        x_dict = self.conv1(x_dict, edge_index_dict)\n        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n        x_dict = {key: self.dropout(x) for key, x in x_dict.items()}\n        x_dict = self.conv2(x_dict, edge_index_dict)\n        return x_dict\n\nmodel = SimpleHGT(hidden_dim, num_classes, data.metadata(), heads=heads).to(device)\n\n# Initialize lazy parameters\nprint(\"   Initializing model parameters...\")\nwith torch.no_grad():\n    dummy_x_dict = {\n        'paper': torch.randn(10, data_dict['paper_features'].shape[1]).to(device),\n        'author': torch.randn(10, 128).to(device),\n        'field_of_study': torch.randn(10, 64).to(device)\n    }\n    dummy_edge_dict = {\n        edge_type: torch.randint(0, 10, (2, 20)).to(device)\n        for edge_type in data.edge_types\n    }\n    _ = model(dummy_x_dict, dummy_edge_dict)\n\nprint(\"✅ Model parameters initialized!\")\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\nprint(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# --- 6. Training with Memory-Efficient Sampling ---\nprint(\"\\n🏃 Training with memory-efficient sampling...\")\n\n# Create memory-efficient sampler\nsampler = MemoryEfficientSampler(data, batch_size=128, num_neighbors=[15, 10])\n\ndef get_memory_usage():\n    \"\"\"Get current memory usage in GB\"\"\"\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / 1024 / 1024 / 1024\n\nprint(f\"Initial memory usage: {get_memory_usage():.2f} GB\")\n\nfor epoch in range(1, 4):\n    print(f\"\\n=== Epoch {epoch} ===\")\n    model.train()\n    total_loss = 0\n    total_examples = 0\n    batch_count = 0\n    \n    for batch in sampler.get_batches(train_idx, shuffle=True):\n        batch_count += 1\n        if batch_count > 50:  # Limit batches for demo\n            break\n            \n        try:\n            # Move batch to device\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            \n            # Forward pass\n            out_dict = model(batch.x_dict, batch.edge_index_dict)\n            \n            # Get target nodes\n            target_mask = batch['paper'].target_mask\n            if target_mask.sum() == 0:\n                continue\n                \n            paper_out = out_dict['paper'][target_mask][:, :data.num_classes]\n            paper_labels = batch['paper'].y[target_mask]\n            \n            # Calculate loss\n            loss = F.cross_entropy(paper_out, paper_labels)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            # Track metrics\n            batch_size = target_mask.sum().item()\n            total_loss += float(loss) * batch_size\n            total_examples += batch_size\n            \n            if batch_count % 10 == 0:\n                avg_loss = total_loss / total_examples\n                memory = get_memory_usage()\n                print(f\"   Batch {batch_count}: Loss={avg_loss:.4f}, Memory={memory:.2f}GB, \"\n                      f\"Papers={len(batch['paper'].x)}, Authors={len(batch['author'].x)}, \"\n                      f\"Fields={len(batch['field_of_study'].x)}\")\n                \n        except Exception as e:\n            print(f\"   Error in batch {batch_count}: {e}\")\n            import traceback\n            traceback.print_exc()\n            continue\n    \n    # Epoch summary\n    if total_examples > 0:\n        epoch_loss = total_loss / total_examples\n        memory = get_memory_usage()\n        print(f\"✅ Epoch {epoch}: Loss={epoch_loss:.4f}, Memory={memory:.2f}GB, Batches={batch_count}\")\n\nprint(f\"\\n🎉 Training complete!\")\nprint(f\"Final memory: {get_memory_usage():.2f}GB\")\n\n# Save model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'hidden_dim': hidden_dim,\n    'num_classes': num_classes,\n    'heads': heads\n}, 'memory_efficient_hgt_model.pt')\n\nprint(\"💾 Model saved!\")\nprint(\"✅ Memory-efficient minibatch training successfully implemented!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}