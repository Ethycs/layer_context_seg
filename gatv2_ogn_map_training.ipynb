{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wvc0hynpbwb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Initializing disk-based OGBN-MAG dataset...\n",
      "📥 Preparing memory-mapped OGBN-MAG data...\n",
      "✅ Memory-mapped data already exists\n",
      "\n",
      "✅ Disk-based data ready!\n",
      "   Papers: 736,389 (on disk)\n",
      "   Authors: 1,134,649\n",
      "   Fields: 59,965\n",
      "   Classes: 349\n",
      "   Memory usage: Minimal - data remains on disk\n"
     ]
    }
   ],
   "source": [
    "# Data Loading Cell - Memory-mapped loading, no data in RAM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HGTConv\n",
    "from torch_geometric.data import HeteroData\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import gc\n",
    "import warnings\n",
    "import h5py\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DiskBasedOGBNMAG:\n",
    "    \"\"\"Memory-efficient OGBN-MAG dataset that keeps data on disk\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='./data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.cache_dir = os.path.join(data_dir, 'ogbn_mag_cache')\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Prepare data files\n",
    "        self._prepare_data()\n",
    "        \n",
    "        # Open memory-mapped files\n",
    "        self._open_mmap_files()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Convert raw OGBN-MAG files to memory-mapped format if needed\"\"\"\n",
    "        print(\"📥 Preparing memory-mapped OGBN-MAG data...\")\n",
    "        \n",
    "        # Check if already prepared\n",
    "        if os.path.exists(os.path.join(self.cache_dir, 'metadata.npz')):\n",
    "            print(\"✅ Memory-mapped data already exists\")\n",
    "            return\n",
    "            \n",
    "        # Ensure raw data exists\n",
    "        ogbn_dir = os.path.join(self.data_dir, 'ogbn_mag', 'raw')\n",
    "        if not os.path.exists(ogbn_dir):\n",
    "            print(\"📦 Downloading OGBN-MAG...\")\n",
    "            from ogb.nodeproppred import PygNodePropPredDataset\n",
    "            temp_dataset = PygNodePropPredDataset('ogbn-mag', root=self.data_dir)\n",
    "            del temp_dataset\n",
    "            gc.collect()\n",
    "        \n",
    "        print(\"🔄 Converting to memory-mapped format...\")\n",
    "        \n",
    "        # Process paper features\n",
    "        feat_file = os.path.join(ogbn_dir, 'node-feat', 'paper', 'node-feat.csv.gz')\n",
    "        with gzip.open(feat_file, 'rt') as f:\n",
    "            paper_features = pd.read_csv(f, header=None).values.astype(np.float32)\n",
    "        \n",
    "        # Save as memory-mapped\n",
    "        mmap_feat = np.memmap(os.path.join(self.cache_dir, 'paper_features.dat'),\n",
    "                             dtype='float32', mode='w+', shape=paper_features.shape)\n",
    "        mmap_feat[:] = paper_features\n",
    "        mmap_feat.flush()\n",
    "        del paper_features, mmap_feat\n",
    "        \n",
    "        # Process labels\n",
    "        label_file = os.path.join(ogbn_dir, 'node-label', 'paper', 'node-label.csv.gz')\n",
    "        with gzip.open(label_file, 'rt') as f:\n",
    "            paper_labels = pd.read_csv(f, header=None).values.flatten().astype(np.int64)\n",
    "        \n",
    "        mmap_labels = np.memmap(os.path.join(self.cache_dir, 'paper_labels.dat'),\n",
    "                               dtype='int64', mode='w+', shape=paper_labels.shape)\n",
    "        mmap_labels[:] = paper_labels\n",
    "        mmap_labels.flush()\n",
    "        \n",
    "        # Process edges in chunks\n",
    "        edge_files = {\n",
    "            'cite': ('paper___cites___paper', 'edge.csv.gz'),\n",
    "            'author': ('author___writes___paper', 'edge.csv.gz'),\n",
    "            'field': ('paper___has_topic___field_of_study', 'edge.csv.gz')\n",
    "        }\n",
    "        \n",
    "        edge_counts = {}\n",
    "        for edge_type, (rel_dir, filename) in edge_files.items():\n",
    "            edge_file = os.path.join(ogbn_dir, 'relations', rel_dir, filename)\n",
    "            with gzip.open(edge_file, 'rt') as f:\n",
    "                edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n",
    "            \n",
    "            # Save edges\n",
    "            mmap_edges = np.memmap(os.path.join(self.cache_dir, f'{edge_type}_edges.dat'),\n",
    "                                  dtype='int64', mode='w+', shape=edges.shape)\n",
    "            mmap_edges[:] = edges\n",
    "            mmap_edges.flush()\n",
    "            edge_counts[edge_type] = edges.shape[1]\n",
    "            del edges, mmap_edges\n",
    "        \n",
    "        # Calculate metadata\n",
    "        num_papers = len(paper_labels)\n",
    "        num_authors = 1134649  # From OGBN-MAG stats\n",
    "        num_fields = 59965\n",
    "        num_classes = int(paper_labels.max()) + 1\n",
    "        feat_dim = mmap_feat.shape[1] if 'mmap_feat' in locals() else 128\n",
    "        \n",
    "        # Load splits\n",
    "        split_dir = os.path.join(self.data_dir, 'ogbn_mag', 'split', 'time')\n",
    "        if os.path.exists(split_dir):\n",
    "            print(\"  Loading official splits...\")\n",
    "            train_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'train.csv.gz'), \n",
    "                                  header=None).values.flatten()\n",
    "            val_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'valid.csv.gz'), \n",
    "                                header=None).values.flatten()\n",
    "            test_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'test.csv.gz'), \n",
    "                                 header=None).values.flatten()\n",
    "        else:\n",
    "            indices = np.random.RandomState(42).permutation(num_papers)\n",
    "            train_size = int(0.8 * num_papers)\n",
    "            val_size = int(0.1 * num_papers)\n",
    "            train_idx = indices[:train_size]\n",
    "            val_idx = indices[train_size:train_size + val_size]\n",
    "            test_idx = indices[train_size + val_size:]\n",
    "        \n",
    "        # Save metadata\n",
    "        np.savez(os.path.join(self.cache_dir, 'metadata.npz'),\n",
    "                num_papers=num_papers,\n",
    "                num_authors=num_authors,\n",
    "                num_fields=num_fields,\n",
    "                num_classes=num_classes,\n",
    "                feat_dim=feat_dim,\n",
    "                train_idx=train_idx,\n",
    "                val_idx=val_idx,\n",
    "                test_idx=test_idx,\n",
    "                edge_counts=edge_counts)\n",
    "        \n",
    "        print(\"✅ Memory-mapped data prepared!\")\n",
    "        gc.collect()\n",
    "    \n",
    "    def _open_mmap_files(self):\n",
    "        \"\"\"Open memory-mapped files for reading\"\"\"\n",
    "        # Load metadata\n",
    "        metadata = np.load(os.path.join(self.cache_dir, 'metadata.npz'), allow_pickle=True)\n",
    "        self.num_papers = int(metadata['num_papers'])\n",
    "        self.num_authors = int(metadata['num_authors'])\n",
    "        self.num_fields = int(metadata['num_fields'])\n",
    "        self.num_classes = int(metadata['num_classes'])\n",
    "        self.feat_dim = int(metadata['feat_dim'])\n",
    "        self.train_idx = torch.from_numpy(metadata['train_idx'])\n",
    "        self.val_idx = torch.from_numpy(metadata['val_idx'])\n",
    "        self.test_idx = torch.from_numpy(metadata['test_idx'])\n",
    "        \n",
    "        # Open memory-mapped arrays (read-only)\n",
    "        self.paper_features = np.memmap(os.path.join(self.cache_dir, 'paper_features.dat'),\n",
    "                                       dtype='float32', mode='r', \n",
    "                                       shape=(self.num_papers, self.feat_dim))\n",
    "        \n",
    "        self.paper_labels = np.memmap(os.path.join(self.cache_dir, 'paper_labels.dat'),\n",
    "                                     dtype='int64', mode='r', shape=(self.num_papers,))\n",
    "        \n",
    "        # Open edge files\n",
    "        edge_counts = metadata['edge_counts'].item()\n",
    "        self.cite_edges = np.memmap(os.path.join(self.cache_dir, 'cite_edges.dat'),\n",
    "                                   dtype='int64', mode='r', shape=(2, edge_counts['cite']))\n",
    "        self.author_edges = np.memmap(os.path.join(self.cache_dir, 'author_edges.dat'),\n",
    "                                     dtype='int64', mode='r', shape=(2, edge_counts['author']))\n",
    "        self.field_edges = np.memmap(os.path.join(self.cache_dir, 'field_edges.dat'),\n",
    "                                    dtype='int64', mode='r', shape=(2, edge_counts['field']))\n",
    "        \n",
    "    def get_paper_batch(self, indices):\n",
    "        \"\"\"Load a batch of papers from disk\"\"\"\n",
    "        # Convert to numpy array for indexing\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.numpy()\n",
    "        \n",
    "        # Load only requested features and labels\n",
    "        features = torch.from_numpy(self.paper_features[indices].copy())\n",
    "        labels = torch.from_numpy(self.paper_labels[indices].copy())\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    def get_edges_for_nodes(self, node_indices, edge_type='cite'):\n",
    "        \"\"\"Get edges connected to specific nodes\"\"\"\n",
    "        if isinstance(node_indices, torch.Tensor):\n",
    "            node_indices = node_indices.numpy()\n",
    "        \n",
    "        # Select appropriate edge array\n",
    "        if edge_type == 'cite':\n",
    "            edges = self.cite_edges\n",
    "        elif edge_type == 'author':\n",
    "            edges = self.author_edges\n",
    "        else:\n",
    "            edges = self.field_edges\n",
    "        \n",
    "        # Find edges involving these nodes (this is still memory intensive for large graphs)\n",
    "        # In production, you'd want an index structure for this\n",
    "        node_set = set(node_indices.tolist())\n",
    "        mask = np.array([edges[0, i] in node_set or edges[1, i] in node_set \n",
    "                        for i in range(edges.shape[1])])\n",
    "        \n",
    "        if mask.any():\n",
    "            return torch.from_numpy(edges[:, mask].copy())\n",
    "        else:\n",
    "            return torch.empty(2, 0, dtype=torch.long)\n",
    "\n",
    "# Create disk-based dataset\n",
    "print(\"🔄 Initializing disk-based OGBN-MAG dataset...\")\n",
    "disk_data = DiskBasedOGBNMAG('./data')\n",
    "\n",
    "# Create a minimal HeteroData structure for compatibility\n",
    "data = HeteroData()\n",
    "data.num_classes = disk_data.num_classes\n",
    "\n",
    "# Store disk dataset reference\n",
    "data._disk_data = disk_data\n",
    "\n",
    "# Training indices\n",
    "train_idx = disk_data.train_idx\n",
    "val_idx = disk_data.val_idx\n",
    "num_classes = disk_data.num_classes\n",
    "\n",
    "print(f\"\\n✅ Disk-based data ready!\")\n",
    "print(f\"   Papers: {disk_data.num_papers:,} (on disk)\")\n",
    "print(f\"   Authors: {disk_data.num_authors:,}\")\n",
    "print(f\"   Fields: {disk_data.num_fields:,}\")\n",
    "print(f\"   Classes: {num_classes}\")\n",
    "print(f\"   Memory usage: Minimal - data remains on disk\")\n",
    "\n",
    "# For compatibility with existing code\n",
    "data_dict = {\n",
    "    'num_papers': disk_data.num_papers,\n",
    "    'num_authors': disk_data.num_authors,\n",
    "    'num_fields': disk_data.num_fields,\n",
    "    'paper_features': disk_data.paper_features,  # This is a memory-mapped array\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k30g6r0wt3h",
   "metadata": {},
   "outputs": [],
   "source": "# Model Definition and Disk-Based Sampler\n\n# Disk-Based Memory-Efficient Sampler\nclass DiskBasedSampler:\n    \"\"\"Sampler that loads data from disk on-demand\"\"\"\n    def __init__(self, disk_data, batch_size=128, num_neighbors=[15, 10]):\n        self.disk_data = disk_data\n        self.batch_size = batch_size\n        self.num_neighbors = num_neighbors\n        \n        # Create edge index for fast neighbor lookup (this does use some memory)\n        # In production, you'd use a graph database or specialized index\n        print(\"Building neighbor index...\")\n        self._build_neighbor_index()\n        \n    def _build_neighbor_index(self):\n        \"\"\"Build a simple neighbor index for citation edges\"\"\"\n        # For true disk-based, this would be saved to disk too\n        # Here we just build a dict for the citation network\n        self.cite_neighbors = {}\n        \n        # Process in chunks to limit memory\n        chunk_size = 1000000\n        num_edges = self.disk_data.cite_edges.shape[1]\n        \n        for start in range(0, num_edges, chunk_size):\n            end = min(start + chunk_size, num_edges)\n            edges_chunk = self.disk_data.cite_edges[:, start:end]\n            \n            for i in range(edges_chunk.shape[1]):\n                src, dst = edges_chunk[0, i], edges_chunk[1, i]\n                if dst not in self.cite_neighbors:\n                    self.cite_neighbors[dst] = []\n                self.cite_neighbors[dst].append(src)\n        \n        print(f\"  Built index for {len(self.cite_neighbors)} nodes\")\n    \n    def sample_neighbors(self, node_id, num_samples):\n        \"\"\"Sample neighbors for a single node\"\"\"\n        if node_id not in self.cite_neighbors:\n            return []\n        \n        neighbors = self.cite_neighbors[node_id]\n        if len(neighbors) <= num_samples:\n            return neighbors\n        \n        # Random sample\n        indices = torch.randperm(len(neighbors))[:num_samples]\n        return [neighbors[i] for i in indices]\n    \n    def create_minibatch(self, target_nodes, force_edges=False):\n        \"\"\"Create a minibatch by loading data from disk\"\"\"\n        # Multi-hop sampling\n        all_paper_nodes = set(target_nodes)\n        current_layer = list(target_nodes)\n        \n        for num_samples in self.num_neighbors:\n            next_layer = set()\n            for node in current_layer:\n                neighbors = self.sample_neighbors(node, num_samples)\n                next_layer.update(neighbors)\n            \n            all_paper_nodes.update(next_layer)\n            current_layer = list(next_layer)\n        \n        # Convert to list for indexing\n        all_paper_nodes = list(all_paper_nodes)\n        num_paper_nodes = len(all_paper_nodes)\n        \n        # Load features and labels from disk\n        paper_features, paper_labels = self.disk_data.get_paper_batch(all_paper_nodes)\n        \n        # Create batch data structure\n        batch = HeteroData()\n        \n        # Paper data\n        batch['paper'].x = paper_features\n        batch['paper'].y = paper_labels\n        \n        # IMPORTANT: Create proper dummy nodes for other types\n        # HGTConv needs all node types to have features\n        num_authors = max(10, num_paper_nodes // 10)  # At least 10 authors\n        num_fields = max(5, num_paper_nodes // 20)    # At least 5 fields\n        \n        batch['author'].x = torch.randn(num_authors, 128)\n        batch['field_of_study'].x = torch.randn(num_fields, 64)\n        \n        # Create node mapping\n        node_mapping = {old: new for new, old in enumerate(all_paper_nodes)}\n        \n        # Build edge indices for the subgraph\n        # Paper-cites-paper edges\n        cite_edges = []\n        for i, node in enumerate(all_paper_nodes):\n            if node in self.cite_neighbors:\n                for neighbor in self.cite_neighbors[node]:\n                    if neighbor in node_mapping:\n                        cite_edges.append([node_mapping[neighbor], node_mapping[node]])\n        \n        if cite_edges:\n            batch['paper', 'cites', 'paper'].edge_index = torch.tensor(cite_edges).T\n        else:\n            # Always provide at least some edges for HGTConv\n            if force_edges and num_paper_nodes >= 2:\n                # Create minimal edges to ensure HGTConv can initialize\n                batch['paper', 'cites', 'paper'].edge_index = torch.tensor([[0, 1], [1, 0]], dtype=torch.long)\n            else:\n                batch['paper', 'cites', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long)\n        \n        # Create edges for other types to avoid torch.cat() errors\n        if force_edges or num_paper_nodes > 0:\n            # Author writes paper (connect authors to papers)\n            author_paper_edges = []\n            for i in range(min(num_authors, num_paper_nodes)):\n                # Each author writes at least one paper\n                author_paper_edges.append([i, i % num_paper_nodes])\n            \n            if author_paper_edges:\n                batch['author', 'writes', 'paper'].edge_index = torch.tensor(author_paper_edges).T\n                # Reverse edges\n                batch['paper', 'written_by', 'author'].edge_index = torch.tensor([[e[1], e[0]] for e in author_paper_edges]).T\n            else:\n                batch['author', 'writes', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long)\n                batch['paper', 'written_by', 'author'].edge_index = torch.empty(2, 0, dtype=torch.long)\n            \n            # Field edges\n            field_paper_edges = []\n            for i in range(min(num_fields, num_paper_nodes)):\n                # Each field has at least one paper\n                field_paper_edges.append([i % num_fields, i])\n            \n            if field_paper_edges:\n                batch['paper', 'has_topic', 'field_of_study'].edge_index = torch.tensor([[e[1], e[0]] for e in field_paper_edges]).T\n                batch['field_of_study', 'topic_of', 'paper'].edge_index = torch.tensor(field_paper_edges).T\n            else:\n                batch['paper', 'has_topic', 'field_of_study'].edge_index = torch.empty(2, 0, dtype=torch.long)\n                batch['field_of_study', 'topic_of', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long)\n        else:\n            # Empty edges for other types\n            batch['author', 'writes', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long)\n            batch['paper', 'written_by', 'author'].edge_index = torch.empty(2, 0, dtype=torch.long)\n            batch['paper', 'has_topic', 'field_of_study'].edge_index = torch.empty(2, 0, dtype=torch.long)\n            batch['field_of_study', 'topic_of', 'paper'].edge_index = torch.empty(2, 0, dtype=torch.long)\n        \n        # Mark target nodes\n        target_mask = torch.zeros(num_paper_nodes, dtype=torch.bool)\n        for node in target_nodes:\n            if node in node_mapping:\n                target_mask[node_mapping[node]] = True\n        batch['paper'].target_mask = target_mask\n        \n        return batch\n    \n    def get_batches(self, indices, shuffle=True):\n        \"\"\"Generate batches from indices\"\"\"\n        if shuffle:\n            perm = torch.randperm(len(indices))\n            indices = indices[perm]\n        \n        for i in range(0, len(indices), self.batch_size):\n            batch_indices = indices[i:i + self.batch_size]\n            yield self.create_minibatch(batch_indices.tolist())\n\n# Research-Optimized HGT Model with better initialization\nclass ResearchOptimalHGT(torch.nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, metadata, heads=8, dropout=0.6, num_layers=3):\n        super().__init__()\n        self.num_layers = num_layers\n        self.dropout = torch.nn.Dropout(dropout)\n        \n        # Store metadata for initialization\n        self.node_types = metadata[0]\n        self.edge_types = metadata[1]\n        \n        # Define input dimensions for each node type\n        self.in_dims = {\n            'paper': 128,  # OGBN-MAG paper features\n            'author': 128,  # Dummy features\n            'field_of_study': 64  # Dummy features\n        }\n        \n        self.convs = torch.nn.ModuleList()\n        self.norms = torch.nn.ModuleList()\n        self.residual_projs = torch.nn.ModuleList()\n        \n        # First layer - use actual dimensions\n        self.convs.append(HGTConv(self.in_dims, hidden_dim, metadata, heads=heads))\n        self.norms.append(torch.nn.LayerNorm(hidden_dim))\n        \n        # Hidden layers\n        for i in range(num_layers - 2):\n            self.convs.append(HGTConv(hidden_dim, hidden_dim, metadata, heads=heads))\n            self.norms.append(torch.nn.LayerNorm(hidden_dim))\n            self.residual_projs.append(torch.nn.Linear(hidden_dim, hidden_dim))\n        \n        # Output layer\n        self.convs.append(HGTConv(hidden_dim, out_dim, metadata, heads=1))\n        \n        self.use_residual = num_layers > 2\n        \n    def forward(self, x_dict, edge_index_dict):\n        # Ensure all node types are present\n        for node_type in self.node_types:\n            if node_type not in x_dict:\n                raise ValueError(f\"Missing node type '{node_type}' in x_dict\")\n        \n        # First layer\n        x_dict = self.convs[0](x_dict, edge_index_dict)\n        x_dict = {key: self.norms[0](x) for key, x in x_dict.items()}\n        x_dict = {key: F.leaky_relu(x, negative_slope=0.2) for key, x in x_dict.items()}\n        x_dict = {key: self.dropout(x) for key, x in x_dict.items()}\n        \n        # Hidden layers with residual\n        for i in range(1, self.num_layers - 1):\n            if self.use_residual:\n                x_dict_res = {k: v.clone() for k, v in x_dict.items()}\n            \n            x_dict = self.convs[i](x_dict, edge_index_dict)\n            x_dict = {key: self.norms[i](x) for key, x in x_dict.items()}\n            x_dict = {key: F.leaky_relu(x, negative_slope=0.2) for key, x in x_dict.items()}\n            x_dict = {key: self.dropout(x) for key, x in x_dict.items()}\n            \n            if self.use_residual:\n                for key in x_dict.keys():\n                    if key in x_dict_res:\n                        residual = self.residual_projs[i-1](x_dict_res[key])\n                        x_dict[key] = x_dict[key] + residual\n        \n        # Output layer\n        x_dict = self.convs[-1](x_dict, edge_index_dict)\n        \n        return x_dict\n\nprint(\"✅ Model and disk-based sampler classes defined!\")\n\n# Define metadata for model initialization\ndata.metadata = lambda: (\n    ['paper', 'author', 'field_of_study'],  # node types\n    [('author', 'writes', 'paper'), \n     ('paper', 'written_by', 'author'),\n     ('paper', 'has_topic', 'field_of_study'),\n     ('field_of_study', 'topic_of', 'paper'), \n     ('paper', 'cites', 'paper')]  # edge types\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6l8sd1f1uvh",
   "metadata": {},
   "outputs": [],
   "source": "# FINAL TRAINING: Multi-GPU with Disk-Based Loading\n# Uses GPUs 1 and 2, loads data from disk on-demand\n\nimport time\nfrom datetime import datetime\nfrom torch.nn.parallel import DataParallel\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\n\nprint(\"🚀 FINAL MULTI-GPU TRAINING WITH DISK-BASED LOADING\")\nprint(\"=\" * 60)\n\n# Force use of GPUs 1 and 2 (RTX 2060s)\nos.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n\n# Disable NCCL for heterogeneous GPUs\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nos.environ['TORCH_USE_CUDA_DSA'] = '0'\n\ntorch.cuda.set_device(0)  # Now device 0 maps to physical GPU 1\n\n# Verify GPU setup\nnum_gpus = torch.cuda.device_count()\nprint(f\"Using {num_gpus} GPUs:\")\nfor i in range(num_gpus):\n    print(f\"  Device {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB)\")\n\n# Since we have heterogeneous GPUs, use single GPU if DataParallel fails\nuse_multi_gpu = True\n\n# FINAL CONFIGURATION\nfinal_config = {\n    # Research-critical parameters\n    'hidden_dim': 256,\n    'heads': 8,\n    'dropout': 0.6,\n    'num_layers': 3,\n    'lr': 0.005,\n    'weight_decay': 5e-4,\n    'gradient_clip': 1.0,\n    'label_smoothing': 0.1,\n    'num_neighbors': [25, 20, 15],\n    \n    # GPU optimization\n    'batch_size_per_gpu': 512,\n    'accumulation_steps': 2,\n    'use_amp': True,\n    \n    # Training parameters\n    'max_epochs': 50,\n    'validation_frequency': 5,\n    'early_stopping_patience': 10,\n    'checkpoint_dir': './final_checkpoints',\n}\n\nos.makedirs(final_config['checkpoint_dir'], exist_ok=True)\n\nprint(\"\\n📊 Configuration:\")\nprint(f\"  Total batch size: {final_config['batch_size_per_gpu'] * (num_gpus if use_multi_gpu else 1)}\")\nprint(f\"  Data loading: From disk on-demand\")\nprint(f\"  Memory usage: Minimal\")\n\n# Create model\ndevice = torch.device('cuda:0')\nprint(\"\\n🧠 Setting up model...\")\n\n# Adjust num_classes if needed\nif num_classes % final_config['heads'] != 0:\n    adjusted_classes = ((num_classes + final_config['heads'] - 1) // final_config['heads']) * final_config['heads']\n    print(f\"   Adjusting classes: {num_classes} → {adjusted_classes}\")\n    num_classes = adjusted_classes\n\n# Create research-optimal model (removed in_dim parameter)\nmodel = ResearchOptimalHGT(\n    in_dim=None,  # Not used anymore, dimensions are hardcoded in the model\n    hidden_dim=final_config['hidden_dim'],\n    out_dim=num_classes,\n    metadata=data.metadata(),\n    heads=final_config['heads'],\n    dropout=final_config['dropout'],\n    num_layers=final_config['num_layers']\n)\n\n# Initialize model with a batch that has edges\nprint(\"   Initializing model with sample batch...\")\ntry:\n    with torch.no_grad():\n        # Get a sample batch with forced edges for initialization\n        sample_sampler = DiskBasedSampler(disk_data, batch_size=100, num_neighbors=[10, 10])\n        sample_batch = sample_sampler.create_minibatch(train_idx[:100].tolist(), force_edges=True)\n        sample_batch = sample_batch.to(device)\n        \n        # Run forward pass to initialize lazy modules\n        _ = model(sample_batch.x_dict, sample_batch.edge_index_dict)\n        print(\"   ✅ Model initialized successfully\")\nexcept Exception as e:\n    print(f\"   ❌ Model initialization failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise\n\n# Move model to GPU\nmodel = model.to(device)\n\n# Try to wrap model for multi-GPU, fall back to single GPU if needed\nif use_multi_gpu and num_gpus > 1:\n    try:\n        print(\"\\n🔧 Setting up DataParallel...\")\n        # Create a simple test to see if DataParallel works\n        test_model = DataParallel(model, device_ids=list(range(num_gpus)))\n        \n        # Quick test forward pass\n        with torch.no_grad():\n            test_sampler = DiskBasedSampler(disk_data, batch_size=4, num_neighbors=[2, 2])\n            test_batch = test_sampler.create_minibatch(train_idx[:4].tolist(), force_edges=True)\n            test_batch = test_batch.to(device)\n            _ = test_model(test_batch.x_dict, test_batch.edge_index_dict)\n        \n        model = test_model\n        print(f\"✅ Model distributed across {num_gpus} GPUs\")\n        actual_batch_size = final_config['batch_size_per_gpu'] * num_gpus\n    except Exception as e:\n        print(f\"⚠️  DataParallel failed: {e}\")\n        print(\"📌 Falling back to single GPU (RTX PRO 6000)\")\n        use_multi_gpu = False\n        actual_batch_size = final_config['batch_size_per_gpu']\nelse:\n    print(\"\\n📌 Using single GPU training\")\n    actual_batch_size = final_config['batch_size_per_gpu']\n\n# Optimizer and scheduler\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=final_config['lr'],\n    weight_decay=final_config['weight_decay']\n)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=10, T_mult=2, eta_min=1e-5\n)\n\n# Create disk-based samplers\nprint(\"\\n📊 Creating disk-based data samplers...\")\ntrain_sampler = DiskBasedSampler(\n    disk_data,\n    batch_size=actual_batch_size,\n    num_neighbors=final_config['num_neighbors']\n)\n\nval_sampler = DiskBasedSampler(\n    disk_data,\n    batch_size=actual_batch_size,\n    num_neighbors=final_config['num_neighbors']\n)\n\n# Mixed precision scaler\nscaler = GradScaler() if final_config['use_amp'] else None\n\n# Memory monitoring\ndef get_gpu_memory_str():\n    if use_multi_gpu:\n        mem_strs = []\n        for i in range(num_gpus):\n            alloc = torch.cuda.memory_allocated(i) / 1024**3\n            reserved = torch.cuda.memory_reserved(i) / 1024**3\n            mem_strs.append(f\"GPU{i}: {alloc:.1f}/{reserved:.1f}GB\")\n        return \" | \".join(mem_strs)\n    else:\n        alloc = torch.cuda.memory_allocated(0) / 1024**3\n        reserved = torch.cuda.memory_reserved(0) / 1024**3\n        return f\"GPU0: {alloc:.1f}/{reserved:.1f}GB\"\n\n# Training function\ndef train_epoch(epoch):\n    model.train()\n    total_loss = 0\n    total_examples = 0\n    \n    batches_per_epoch = min(800, len(train_idx) // actual_batch_size)\n    optimizer.zero_grad()\n    \n    pbar = tqdm(range(batches_per_epoch), desc=f'Epoch {epoch}')\n    \n    for batch_idx, batch in enumerate(train_sampler.get_batches(train_idx, shuffle=True)):\n        if batch_idx >= batches_per_epoch:\n            break\n            \n        try:\n            batch = batch.to(device, non_blocking=True)\n            \n            # Mixed precision forward\n            with autocast(enabled=final_config['use_amp']):\n                out_dict = model(batch.x_dict, batch.edge_index_dict)\n                \n                # Handle DataParallel output\n                if isinstance(out_dict, tuple):\n                    out_dict = out_dict[0]\n                \n                target_mask = batch['paper'].target_mask\n                if target_mask.sum() == 0:\n                    continue\n                \n                paper_out = out_dict['paper'][target_mask][:, :num_classes]\n                paper_labels = batch['paper'].y[target_mask]\n                \n                loss = F.cross_entropy(\n                    paper_out, \n                    paper_labels, \n                    label_smoothing=final_config['label_smoothing']\n                )\n                loss = loss / final_config['accumulation_steps']\n            \n            # Backward\n            if scaler:\n                scaler.scale(loss).backward()\n            else:\n                loss.backward()\n            \n            # Gradient accumulation and step\n            if (batch_idx + 1) % final_config['accumulation_steps'] == 0:\n                if scaler:\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), final_config['gradient_clip'])\n                    scaler.step(optimizer)\n                    scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), final_config['gradient_clip'])\n                    optimizer.step()\n                \n                optimizer.zero_grad()\n            \n            # Metrics\n            batch_size = target_mask.sum().item()\n            total_loss += float(loss) * batch_size * final_config['accumulation_steps']\n            total_examples += batch_size\n            \n            # Update progress\n            pbar.update(1)\n            if batch_idx % 50 == 0:\n                pbar.set_postfix({\n                    'loss': f'{total_loss/max(1, total_examples):.4f}',\n                    'lr': f'{optimizer.param_groups[0][\"lr\"]:.6f}',\n                    'mem': get_gpu_memory_str()\n                })\n                \n        except Exception as e:\n            print(f\"\\nError in batch {batch_idx}: {e}\")\n            if \"NCCL\" in str(e):\n                print(\"⚠️  NCCL error detected. Consider using single GPU training.\")\n            import traceback\n            traceback.print_exc()\n            continue\n    \n    pbar.close()\n    return total_loss / max(1, total_examples)\n\n@torch.no_grad()\ndef validate():\n    model.eval()\n    total_loss = 0\n    total_correct = 0\n    total_examples = 0\n    \n    val_batches = min(100, len(val_idx) // actual_batch_size)\n    \n    for i, batch in enumerate(tqdm(val_sampler.get_batches(val_idx, shuffle=False), \n                                  desc='Validating', total=val_batches)):\n        if i >= val_batches:\n            break\n            \n        try:\n            batch = batch.to(device, non_blocking=True)\n            \n            with autocast(enabled=final_config['use_amp']):\n                out_dict = model(batch.x_dict, batch.edge_index_dict)\n                \n                # Handle DataParallel output\n                if isinstance(out_dict, tuple):\n                    out_dict = out_dict[0]\n                    \n                target_mask = batch['paper'].target_mask\n                \n                if target_mask.sum() == 0:\n                    continue\n                \n                paper_out = out_dict['paper'][target_mask][:, :num_classes]\n                paper_labels = batch['paper'].y[target_mask]\n                loss = F.cross_entropy(paper_out, paper_labels)\n                \n                pred = paper_out.argmax(dim=-1)\n                correct = (pred == paper_labels).sum().item()\n            \n            batch_size = target_mask.sum().item()\n            total_loss += float(loss) * batch_size\n            total_correct += correct\n            total_examples += batch_size\n            \n        except Exception as e:\n            continue\n    \n    val_loss = total_loss / max(1, total_examples)\n    val_acc = total_correct / max(1, total_examples)\n    return val_loss, val_acc\n\n# Checkpoint management\ndef save_checkpoint(epoch, train_loss, val_loss, val_acc, is_best=False):\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss,\n        'val_acc': val_acc,\n        'config': final_config,\n    }\n    \n    torch.save(checkpoint, os.path.join(final_config['checkpoint_dir'], 'latest.pt'))\n    \n    if is_best:\n        torch.save(checkpoint, os.path.join(final_config['checkpoint_dir'], 'best.pt'))\n        print(f\"💾 New best model saved! Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n\ndef load_checkpoint():\n    checkpoint_path = os.path.join(final_config['checkpoint_dir'], 'latest.pt')\n    if os.path.exists(checkpoint_path):\n        print(f\"📂 Loading checkpoint from {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        if hasattr(model, 'module'):\n            model.module.load_state_dict(checkpoint['model_state_dict'])\n        else:\n            model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        return checkpoint['epoch']\n    return 0\n\n# MAIN TRAINING LOOP\nprint(\"\\n\" + \"=\"*60)\nprint(\"🏃 STARTING DISK-BASED TRAINING\")\nprint(f\"   Mode: {'Multi-GPU' if use_multi_gpu else 'Single GPU'}\")\nprint(f\"   Batch size: {actual_batch_size}\")\nprint(\"=\"*60)\n\n# Try to resume from checkpoint\nstart_epoch = load_checkpoint()\n\n# Clear GPU cache\ntorch.cuda.empty_cache()\n\n# Training history\nbest_val_loss = float('inf')\nbest_val_acc = 0.0\npatience_counter = 0\ntraining_start = datetime.now()\n\nfor epoch in range(start_epoch + 1, final_config['max_epochs'] + 1):\n    epoch_start = time.time()\n    \n    # Train\n    train_loss = train_epoch(epoch)\n    \n    # Step scheduler\n    scheduler.step()\n    \n    # Validate periodically\n    if epoch % final_config['validation_frequency'] == 0:\n        val_loss, val_acc = validate()\n        \n        # Check if best\n        is_best = val_loss < best_val_loss\n        if is_best:\n            best_val_loss = val_loss\n            best_val_acc = val_acc\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        # Save checkpoint\n        save_checkpoint(epoch, train_loss, val_loss, val_acc, is_best)\n        \n        # Print summary\n        print(f\"\\n{'='*60}\")\n        print(f\"Epoch {epoch} Summary:\")\n        print(f\"  Train Loss: {train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f} {'🏆 NEW BEST!' if is_best else ''}\")\n        print(f\"  Val Accuracy: {val_acc:.4%}\")\n        print(f\"  Time: {time.time() - epoch_start:.1f}s\")\n        print(f\"  Memory: {get_gpu_memory_str()}\")\n        print(f\"{'='*60}\\n\")\n        \n        # Early stopping\n        if patience_counter >= final_config['early_stopping_patience']:\n            print(\"🛑 Early stopping triggered!\")\n            break\n    else:\n        print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Time={time.time() - epoch_start:.1f}s\")\n    \n    # Clear cache periodically\n    if epoch % 5 == 0:\n        torch.cuda.empty_cache()\n\n# Training complete\ntotal_time = (datetime.now() - training_start).total_seconds()\nprint(f\"\\n{'='*60}\")\nprint(f\"✅ TRAINING COMPLETE!\")\nprint(f\"  Total time: {total_time/3600:.2f} hours\")\nprint(f\"  Best validation loss: {best_val_loss:.4f}\")\nprint(f\"  Best validation accuracy: {best_val_acc:.4%}\")\nprint(f\"  Memory: {get_gpu_memory_str()}\")\nprint(f\"{'='*60}\")\n\n# Reset CUDA device selection\nos.environ['CUDA_VISIBLE_DEVICES'] = '1,2'"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}