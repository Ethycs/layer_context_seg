{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1772293",
   "metadata": {},
   "outputs": [],
   "source": "# OGBN-MAG Minibatch Training - Pragmatic Approach (Fixed Init)\n# Simple data loading + PyG inheritance where needed = Working solution!\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import HGTConv\nfrom torch_geometric.loader import NeighborLoader\nfrom torch_geometric.data import HeteroData\nimport os\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport gzip\nimport psutil\nimport gc\nimport warnings\nimport traceback\n\nwarnings.filterwarnings('ignore')\nprint(\"üöÄ Starting pragmatic OGBN-MAG training...\")\n\n# --- 1. Super Simple Data Loader (Keep This Clean!) ---\ndef ensure_ogbn_data_exists(data_dir):\n    \"\"\"Check if OGBN-MAG raw files exist, download if needed\"\"\"\n    ogbn_dir = os.path.join(data_dir, 'ogbn_mag', 'raw')\n    \n    if not os.path.exists(ogbn_dir):\n        print(\"üì¶ Downloading OGBN-MAG (this may take a while)...\")\n        from ogb.nodeproppred import PygNodePropPredDataset\n        \n        # Download to a temp location to avoid framework issues\n        temp_dataset = PygNodePropPredDataset('ogbn-mag', root=data_dir)\n        print(\"‚úÖ Download complete!\")\n        del temp_dataset\n        gc.collect()\n    \n    return ogbn_dir\n\ndef load_ogbn_simple(data_dir):\n    \"\"\"Load OGBN-MAG data directly from files - no frameworks!\"\"\"\n    print(\"üì• Loading OGBN-MAG from raw files...\")\n    \n    # Ensure data exists\n    raw_dir = ensure_ogbn_data_exists(data_dir)\n    \n    # Load paper features\n    feat_file = os.path.join(raw_dir, 'node-feat', 'paper', 'node-feat.csv.gz')\n    print(\"  Loading paper features...\")\n    with gzip.open(feat_file, 'rt') as f:\n        paper_features = pd.read_csv(f, header=None).values.astype(np.float32)\n    \n    # Load paper labels\n    label_file = os.path.join(raw_dir, 'node-label', 'paper', 'node-label.csv.gz')\n    print(\"  Loading paper labels...\")\n    with gzip.open(label_file, 'rt') as f:\n        paper_labels = pd.read_csv(f, header=None).values.flatten().astype(np.int64)\n    \n    # Load citation edges\n    cite_file = os.path.join(raw_dir, 'relations', 'paper___cites___paper', 'edge.csv.gz')\n    print(\"  Loading citation edges...\")\n    with gzip.open(cite_file, 'rt') as f:\n        cite_edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n    \n    # Load author-paper edges (for heterogeneous graph)\n    author_file = os.path.join(raw_dir, 'relations', 'author___writes___paper', 'edge.csv.gz')\n    print(\"  Loading author-paper edges...\")\n    with gzip.open(author_file, 'rt') as f:\n        author_paper_edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n    \n    # Load field-paper edges\n    field_file = os.path.join(raw_dir, 'relations', 'paper___has_topic___field_of_study', 'edge.csv.gz')\n    print(\"  Loading field-paper edges...\")\n    with gzip.open(field_file, 'rt') as f:\n        field_paper_edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n    \n    # Calculate node counts\n    num_papers = len(paper_features)\n    num_authors = author_paper_edges[0].max() + 1\n    num_fields = field_paper_edges[1].max() + 1\n    num_classes = int(paper_labels.max()) + 1\n    \n    # Create train/val/test splits (use official split if available)\n    split_dir = os.path.join(data_dir, 'ogbn_mag', 'split', 'time')\n    if os.path.exists(split_dir):\n        print(\"  Loading official splits...\")\n        train_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'train.csv.gz'), header=None).values.flatten()\n        val_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'valid.csv.gz'), header=None).values.flatten()\n        test_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'test.csv.gz'), header=None).values.flatten()\n    else:\n        print(\"  Creating random splits...\")\n        indices = np.random.RandomState(42).permutation(num_papers)\n        train_size = int(0.8 * num_papers)\n        val_size = int(0.1 * num_papers)\n        train_idx = indices[:train_size]\n        val_idx = indices[train_size:train_size + val_size]\n        test_idx = indices[train_size + val_size:]\n    \n    print(f\"‚úÖ Loaded: {num_papers} papers, {num_authors} authors, {num_fields} fields\")\n    print(f\"   Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n    \n    return {\n        'paper_features': torch.from_numpy(paper_features),\n        'paper_labels': torch.from_numpy(paper_labels),\n        'paper_author_edges': torch.from_numpy(author_paper_edges),\n        'paper_field_edges': torch.from_numpy(field_paper_edges),\n        'paper_cite_edges': torch.from_numpy(cite_edges),\n        'train_idx': torch.from_numpy(train_idx),\n        'val_idx': torch.from_numpy(val_idx),\n        'test_idx': torch.from_numpy(test_idx),\n        'num_papers': num_papers,\n        'num_authors': num_authors,\n        'num_fields': num_fields,\n        'num_classes': num_classes\n    }\n\n# --- 2. Create HeteroData the PyG Way ---\ndef create_pyg_hetero_data(data_dict):\n    \"\"\"Create HeteroData using PyG's standard approach\"\"\"\n    print(\"üîó Setting up PyG-compatible heterogeneous data...\")\n    \n    data = HeteroData()\n    \n    # Set node features\n    data['paper'].x = data_dict['paper_features']\n    data['paper'].y = data_dict['paper_labels']\n    data['author'].x = torch.randn(data_dict['num_authors'], 128)\n    data['field_of_study'].x = torch.randn(data_dict['num_fields'], 64)\n    \n    # Set edge indices\n    data['author', 'writes', 'paper'].edge_index = data_dict['paper_author_edges'].contiguous()\n    data['paper', 'written_by', 'author'].edge_index = data_dict['paper_author_edges'].flip(0).contiguous()\n    data['paper', 'has_topic', 'field_of_study'].edge_index = data_dict['paper_field_edges'].contiguous()\n    data['field_of_study', 'topic_of', 'paper'].edge_index = data_dict['paper_field_edges'].flip(0).contiguous()\n    data['paper', 'cites', 'paper'].edge_index = data_dict['paper_cite_edges'].contiguous()\n    \n    # Set train/val/test masks\n    num_papers = data_dict['num_papers']\n    train_mask = torch.zeros(num_papers, dtype=torch.bool)\n    val_mask = torch.zeros(num_papers, dtype=torch.bool)\n    test_mask = torch.zeros(num_papers, dtype=torch.bool)\n    \n    train_mask[data_dict['train_idx']] = True\n    val_mask[data_dict['val_idx']] = True\n    test_mask[data_dict['test_idx']] = True\n    \n    data['paper'].train_mask = train_mask\n    data['paper'].val_mask = val_mask\n    data['paper'].test_mask = test_mask\n    \n    # Store additional info we need\n    data.num_classes = data_dict['num_classes']\n    data.train_idx = data_dict['train_idx']\n    \n    print(f\"‚úÖ PyG hetero data ready!\")\n    print(f\"   Node types: {data.node_types}\")\n    print(f\"   Edge types: {data.edge_types}\")\n    \n    return data\n\n# --- 3. Simple Batch Manager (Keep This!) ---\nclass SimpleBatchManager:\n    def __init__(self, initial_size=256, target_memory_gb=6.0):\n        self.batch_size = initial_size\n        self.target_memory = target_memory_gb\n        self.memory_history = []\n    \n    def update(self, current_memory_gb):\n        self.memory_history.append(current_memory_gb)\n        \n        if len(self.memory_history) < 3:\n            return False\n        \n        avg_memory = sum(self.memory_history[-3:]) / 3\n        old_size = self.batch_size\n        \n        if avg_memory > self.target_memory * 0.85:\n            self.batch_size = max(32, int(self.batch_size * 0.8))\n        elif avg_memory < self.target_memory * 0.6:\n            self.batch_size = int(self.batch_size * 1.1)\n        \n        if self.batch_size != old_size:\n            print(f\"üîÑ Batch size: {old_size} ‚Üí {self.batch_size} (memory: {avg_memory:.1f}GB)\")\n            return True\n        return False\n\ndef get_memory_usage():\n    \"\"\"Get current memory usage in GB\"\"\"\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / 1024 / 1024 / 1024\n\n# --- 4. Load Data ---\nprint(\"\\nüìÇ Loading OGBN-MAG data...\")\ndata_dict = load_ogbn_simple('./data')\ndata = create_pyg_hetero_data(data_dict)\n\n# Get training indices\ntrain_idx = data.train_idx\nprint(f\"üéØ Training on {len(train_idx)} papers\")\n\n# --- 5. Simple Model ---\nprint(\"\\nüß† Creating model...\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"   Device: {device}\")\n\nnum_classes = data.num_classes\nhidden_dim = 128\nheads = 4\n\n# Ensure compatibility\nif num_classes % heads != 0:\n    adjusted_classes = ((num_classes + heads - 1) // heads) * heads\n    print(f\"   Adjusting classes: {num_classes} ‚Üí {adjusted_classes}\")\n    num_classes = adjusted_classes\n\nclass SimpleHGT(torch.nn.Module):\n    def __init__(self, hidden_dim, out_dim, metadata, heads=4):\n        super().__init__()\n        self.conv1 = HGTConv(-1, hidden_dim, metadata, heads=heads)\n        self.conv2 = HGTConv(hidden_dim, out_dim, metadata, heads=heads)\n        self.dropout = torch.nn.Dropout(0.3)\n    \n    def forward(self, x_dict, edge_index_dict):\n        x_dict = self.conv1(x_dict, edge_index_dict)\n        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n        x_dict = {key: self.dropout(x) for key, x in x_dict.items()}\n        x_dict = self.conv2(x_dict, edge_index_dict)\n        return x_dict\n\nmodel = SimpleHGT(hidden_dim, num_classes, data.metadata(), heads=heads).to(device)\n\n# Initialize lazy parameters with dummy forward pass\nprint(\"   Initializing model parameters...\")\nwith torch.no_grad():\n    # Create dummy batch with correct shapes\n    dummy_x_dict = {}\n    for node_type in data.node_types:\n        if node_type == 'paper':\n            dummy_x_dict[node_type] = torch.randn(10, data_dict['paper_features'].shape[1]).to(device)\n        elif node_type == 'author':\n            dummy_x_dict[node_type] = torch.randn(10, 128).to(device)\n        else:  # field_of_study\n            dummy_x_dict[node_type] = torch.randn(10, 64).to(device)\n    \n    # Create dummy edge indices\n    dummy_edge_dict = {}\n    for edge_type in data.edge_types:\n        dummy_edge_dict[edge_type] = torch.randint(0, 10, (2, 20)).to(device)\n    \n    # Dummy forward pass to initialize parameters\n    _ = model(dummy_x_dict, dummy_edge_dict)\n\nprint(\"‚úÖ Model parameters initialized!\")\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\nprint(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# --- 6. Fixed Training Function ---\nbatch_manager = SimpleBatchManager(initial_size=128, target_memory_gb=6.0)\n\ndef train_epoch():\n    model.train()\n    total_loss = total_examples = 0\n    \n    print(\"   Creating NeighborLoader...\")\n    try:\n        # Create loader - should work now with proper HeteroData inheritance!\n        loader = NeighborLoader(\n            data,\n            num_neighbors=[15, 10],\n            batch_size=batch_manager.batch_size,\n            input_nodes=('paper', train_idx),\n            shuffle=True,\n            num_workers=0\n        )\n        print(\"   ‚úÖ NeighborLoader created successfully\")\n    except Exception as e:\n        print(f\"   ‚ùå Failed to create NeighborLoader: {e}\")\n        traceback.print_exc()\n        raise e\n    \n    batch_count = 0\n    print(\"   Starting batch iteration...\")\n    \n    for batch in loader:\n        batch_count += 1\n        if batch_count > 5:  # Only process 5 batches for debugging\n            break\n            \n        try:\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            \n            # Forward pass\n            out_dict = model(batch.x_dict, batch.edge_index_dict)\n            \n            # Get the number of target nodes (papers) in this batch\n            if hasattr(batch['paper'], 'batch_size'):\n                batch_size = batch['paper'].batch_size\n            else:\n                # Fallback: count actual target nodes\n                batch_size = batch['paper'].y.size(0)\n            \n            # Loss calculation - Use original class count\n            original_classes = data.num_classes\n            paper_out = out_dict['paper'][:batch_size, :original_classes]\n            paper_labels = batch['paper'].y[:batch_size]\n            \n            loss = F.cross_entropy(paper_out, paper_labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            total_loss += float(loss) * batch_size\n            total_examples += batch_size\n            \n            if batch_count == 1:\n                print(f\"     First batch: Loss={loss:.4f}, Batch size={batch_size}\")\n            \n            # Memory management\n            if batch_count % 10 == 0:\n                gc.collect()\n                if device.type == 'cuda':\n                    torch.cuda.empty_cache()\n        \n        except Exception as e:\n            print(f\"   Error in batch {batch_count}: {e}\")\n            traceback.print_exc()\n            raise e\n    \n    avg_loss = total_loss / max(1, total_examples)\n    print(f\"   Processed {batch_count} batches, {total_examples} examples, avg loss: {avg_loss:.4f}\")\n    return avg_loss, False\n\n# --- 7. Training Loop ---\nprint(\"\\nüèÉ Training...\")\n\nfor epoch in range(1, 4):\n    print(f\"\\n=== Epoch {epoch} ===\")\n    \n    try:\n        loss, _ = train_epoch()\n        memory_usage = get_memory_usage()\n        print(f\"‚úÖ Epoch {epoch}: Loss={loss:.4f}, Memory={memory_usage:.1f}GB\")\n    except Exception as e:\n        print(f\"‚ùå Error in epoch {epoch}: {e}\")\n        break\n\nprint(f\"\\nüéâ Training complete!\")\nprint(f\"Final memory: {get_memory_usage():.1f}GB\")\n\n# Save model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'batch_size': batch_manager.batch_size,\n    'hidden_dim': hidden_dim,\n    'num_classes': num_classes,\n    'heads': heads\n}, 'pragmatic_hgt_model.pt')\n\nprint(\"üíæ Model saved as 'pragmatic_hgt_model.pt'\")\nprint(\"‚úÖ Minibatch training working!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}