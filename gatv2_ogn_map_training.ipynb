{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1772293",
   "metadata": {},
   "outputs": [],
   "source": "# OGBN-MAG Minibatch Training - Simple Sampling Solution\n# Avoiding complex dependencies by implementing our own simple sampling\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import HGTConv\nfrom torch_geometric.data import HeteroData, Data\nimport os\nimport numpy as np\nimport pandas as pd\nimport gzip\nimport psutil\nimport gc\nimport warnings\nimport random\n\nwarnings.filterwarnings('ignore')\nprint(\"ðŸš€ Starting OGBN-MAG minibatch training with simple sampling...\")\n\n# --- 1. Super Simple Data Loader ---\ndef ensure_ogbn_data_exists(data_dir):\n    \"\"\"Check if OGBN-MAG raw files exist, download if needed\"\"\"\n    ogbn_dir = os.path.join(data_dir, 'ogbn_mag', 'raw')\n    \n    if not os.path.exists(ogbn_dir):\n        print(\"ðŸ“¦ Downloading OGBN-MAG (this may take a while)...\")\n        from ogb.nodeproppred import PygNodePropPredDataset\n        \n        # Download to a temp location to avoid framework issues\n        temp_dataset = PygNodePropPredDataset('ogbn-mag', root=data_dir)\n        print(\"âœ… Download complete!\")\n        del temp_dataset\n        gc.collect()\n    \n    return ogbn_dir\n\ndef load_ogbn_simple(data_dir):\n    \"\"\"Load OGBN-MAG data directly from files - no frameworks!\"\"\"\n    print(\"ðŸ“¥ Loading OGBN-MAG from raw files...\")\n    \n    # Ensure data exists\n    raw_dir = ensure_ogbn_data_exists(data_dir)\n    \n    # Load paper features\n    feat_file = os.path.join(raw_dir, 'node-feat', 'paper', 'node-feat.csv.gz')\n    print(\"  Loading paper features...\")\n    with gzip.open(feat_file, 'rt') as f:\n        paper_features = pd.read_csv(f, header=None).values.astype(np.float32)\n    \n    # Load paper labels\n    label_file = os.path.join(raw_dir, 'node-label', 'paper', 'node-label.csv.gz')\n    print(\"  Loading paper labels...\")\n    with gzip.open(label_file, 'rt') as f:\n        paper_labels = pd.read_csv(f, header=None).values.flatten().astype(np.int64)\n    \n    # Load citation edges\n    cite_file = os.path.join(raw_dir, 'relations', 'paper___cites___paper', 'edge.csv.gz')\n    print(\"  Loading citation edges...\")\n    with gzip.open(cite_file, 'rt') as f:\n        cite_edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n    \n    # Load author-paper edges (for heterogeneous graph)\n    author_file = os.path.join(raw_dir, 'relations', 'author___writes___paper', 'edge.csv.gz')\n    print(\"  Loading author-paper edges...\")\n    with gzip.open(author_file, 'rt') as f:\n        author_paper_edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n    \n    # Load field-paper edges\n    field_file = os.path.join(raw_dir, 'relations', 'paper___has_topic___field_of_study', 'edge.csv.gz')\n    print(\"  Loading field-paper edges...\")\n    with gzip.open(field_file, 'rt') as f:\n        field_paper_edges = pd.read_csv(f, header=None).values.T.astype(np.int64)\n    \n    # Calculate node counts\n    num_papers = len(paper_features)\n    num_authors = author_paper_edges[0].max() + 1\n    num_fields = field_paper_edges[1].max() + 1\n    num_classes = int(paper_labels.max()) + 1\n    \n    # Create train/val/test splits (use official split if available)\n    split_dir = os.path.join(data_dir, 'ogbn_mag', 'split', 'time')\n    if os.path.exists(split_dir):\n        print(\"  Loading official splits...\")\n        train_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'train.csv.gz'), header=None).values.flatten()\n        val_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'valid.csv.gz'), header=None).values.flatten()\n        test_idx = pd.read_csv(os.path.join(split_dir, 'paper', 'test.csv.gz'), header=None).values.flatten()\n    else:\n        print(\"  Creating random splits...\")\n        indices = np.random.RandomState(42).permutation(num_papers)\n        train_size = int(0.8 * num_papers)\n        val_size = int(0.1 * num_papers)\n        train_idx = indices[:train_size]\n        val_idx = indices[train_size:train_size + val_size]\n        test_idx = indices[train_size + val_size:]\n    \n    print(f\"âœ… Loaded: {num_papers} papers, {num_authors} authors, {num_fields} fields\")\n    print(f\"   Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n    \n    return {\n        'paper_features': torch.from_numpy(paper_features),\n        'paper_labels': torch.from_numpy(paper_labels),\n        'paper_author_edges': torch.from_numpy(author_paper_edges),\n        'paper_field_edges': torch.from_numpy(field_paper_edges),\n        'paper_cite_edges': torch.from_numpy(cite_edges),\n        'train_idx': torch.from_numpy(train_idx),\n        'val_idx': torch.from_numpy(val_idx),\n        'test_idx': torch.from_numpy(test_idx),\n        'num_papers': num_papers,\n        'num_authors': num_authors,\n        'num_fields': num_fields,\n        'num_classes': num_classes\n    }\n\n# --- 2. Create HeteroData ---\ndef create_pyg_hetero_data(data_dict):\n    \"\"\"Create HeteroData using PyG's standard approach\"\"\"\n    print(\"ðŸ”— Setting up PyG-compatible heterogeneous data...\")\n    \n    data = HeteroData()\n    \n    # Set node features\n    data['paper'].x = data_dict['paper_features']\n    data['paper'].y = data_dict['paper_labels']\n    data['author'].x = torch.randn(data_dict['num_authors'], 128)\n    data['field_of_study'].x = torch.randn(data_dict['num_fields'], 64)\n    \n    # Set edge indices\n    data['author', 'writes', 'paper'].edge_index = data_dict['paper_author_edges'].contiguous()\n    data['paper', 'written_by', 'author'].edge_index = data_dict['paper_author_edges'].flip(0).contiguous()\n    data['paper', 'has_topic', 'field_of_study'].edge_index = data_dict['paper_field_edges'].contiguous()\n    data['field_of_study', 'topic_of', 'paper'].edge_index = data_dict['paper_field_edges'].flip(0).contiguous()\n    data['paper', 'cites', 'paper'].edge_index = data_dict['paper_cite_edges'].contiguous()\n    \n    # Store additional info we need\n    data.num_classes = data_dict['num_classes']\n    data.train_idx = data_dict['train_idx']\n    data.data_dict = data_dict  # Keep original data for sampling\n    \n    print(f\"âœ… PyG hetero data ready!\")\n    print(f\"   Node types: {data.node_types}\")\n    print(f\"   Edge types: {data.edge_types}\")\n    \n    return data\n\n# --- 3. Simple Batch Sampler ---\nclass SimpleBatchSampler:\n    \"\"\"Simple batch sampler that creates mini-batches without complex dependencies\"\"\"\n    def __init__(self, data, batch_size=128, num_neighbors=[10, 5]):\n        self.data = data\n        self.batch_size = batch_size\n        self.num_neighbors = num_neighbors\n        \n        # Build adjacency lists for efficient sampling\n        print(\"   Building adjacency lists...\")\n        self.adj_lists = self._build_adjacency_lists()\n        \n    def _build_adjacency_lists(self):\n        \"\"\"Build adjacency lists for each edge type\"\"\"\n        adj_lists = {}\n        \n        # For each edge type, create adjacency list\n        for edge_type in self.data.edge_types:\n            edge_index = self.data[edge_type].edge_index\n            src_type = edge_type[0]\n            dst_type = edge_type[2]\n            \n            # Create adjacency list: dst_node -> list of src_nodes\n            adj = {}\n            for i in range(edge_index.shape[1]):\n                src = int(edge_index[0, i])\n                dst = int(edge_index[1, i])\n                if dst not in adj:\n                    adj[dst] = []\n                adj[dst].append(src)\n            \n            adj_lists[edge_type] = adj\n            \n        return adj_lists\n    \n    def sample_neighbors(self, nodes, node_type, num_samples):\n        \"\"\"Sample neighbors for given nodes\"\"\"\n        sampled = set()\n        \n        # For each edge type where nodes are destinations\n        for edge_type in self.data.edge_types:\n            if edge_type[2] == node_type:  # If this node type is destination\n                adj = self.adj_lists.get(edge_type, {})\n                \n                for node in nodes:\n                    neighbors = adj.get(int(node), [])\n                    if neighbors:\n                        # Sample up to num_samples neighbors\n                        k = min(num_samples, len(neighbors))\n                        sampled.update(random.sample(neighbors, k))\n        \n        return list(sampled)\n    \n    def create_minibatch(self, target_nodes):\n        \"\"\"Create a minibatch subgraph for target nodes\"\"\"\n        # Start with target paper nodes\n        paper_nodes = set(target_nodes)\n        all_paper_nodes = paper_nodes.copy()\n        \n        # Sample neighbors layer by layer\n        current_papers = list(paper_nodes)\n        for num_samples in self.num_neighbors:\n            # Sample paper neighbors (citations)\n            paper_neighbors = self.sample_neighbors(current_papers, 'paper', num_samples)\n            all_paper_nodes.update(paper_neighbors)\n            current_papers = paper_neighbors\n        \n        # Get all connected authors and fields\n        paper_list = list(all_paper_nodes)\n        author_nodes = set()\n        field_nodes = set()\n        \n        # Find connected authors and fields\n        for edge_type, adj in self.adj_lists.items():\n            if edge_type[2] == 'paper' and edge_type[0] == 'author':\n                for paper in paper_list:\n                    authors = adj.get(int(paper), [])\n                    author_nodes.update(authors[:10])  # Limit authors per paper\n            elif edge_type[2] == 'paper' and edge_type[0] == 'field_of_study':\n                for paper in paper_list:\n                    fields = adj.get(int(paper), [])\n                    field_nodes.update(fields[:5])  # Limit fields per paper\n        \n        # Create node mappings\n        paper_mapping = {old: new for new, old in enumerate(sorted(all_paper_nodes))}\n        author_mapping = {old: new for new, old in enumerate(sorted(author_nodes))}\n        field_mapping = {old: new for new, old in enumerate(sorted(field_nodes))}\n        \n        # Create batch data\n        batch = HeteroData()\n        \n        # Add node features\n        paper_indices = torch.tensor(sorted(all_paper_nodes), dtype=torch.long)\n        batch['paper'].x = self.data['paper'].x[paper_indices]\n        batch['paper'].y = self.data['paper'].y[paper_indices]\n        \n        if author_nodes:\n            author_indices = torch.tensor(sorted(author_nodes), dtype=torch.long)\n            batch['author'].x = self.data['author'].x[author_indices]\n        else:\n            batch['author'].x = torch.empty(0, 128)\n            \n        if field_nodes:\n            field_indices = torch.tensor(sorted(field_nodes), dtype=torch.long)\n            batch['field_of_study'].x = self.data['field_of_study'].x[field_indices]\n        else:\n            batch['field_of_study'].x = torch.empty(0, 64)\n        \n        # Add edges (only those within sampled nodes)\n        for edge_type in self.data.edge_types:\n            edge_index = self.data[edge_type].edge_index\n            src_type = edge_type[0]\n            dst_type = edge_type[2]\n            \n            # Get appropriate mappings\n            if src_type == 'paper':\n                src_mapping = paper_mapping\n            elif src_type == 'author':\n                src_mapping = author_mapping\n            else:  # field_of_study\n                src_mapping = field_mapping\n                \n            if dst_type == 'paper':\n                dst_mapping = paper_mapping\n            elif dst_type == 'author':\n                dst_mapping = author_mapping\n            else:  # field_of_study\n                dst_mapping = field_mapping\n            \n            # Filter edges\n            mask = torch.zeros(edge_index.shape[1], dtype=torch.bool)\n            for i in range(edge_index.shape[1]):\n                src = int(edge_index[0, i])\n                dst = int(edge_index[1, i])\n                if src in src_mapping and dst in dst_mapping:\n                    mask[i] = True\n            \n            if mask.any():\n                filtered_edges = edge_index[:, mask]\n                # Remap indices\n                new_edges = torch.stack([\n                    torch.tensor([src_mapping[int(src)] for src in filtered_edges[0]]),\n                    torch.tensor([dst_mapping[int(dst)] for dst in filtered_edges[1]])\n                ])\n                batch[edge_type].edge_index = new_edges\n            else:\n                batch[edge_type].edge_index = torch.empty(2, 0, dtype=torch.long)\n        \n        # Mark target nodes\n        batch['paper'].batch_size = len(target_nodes)\n        target_mask = torch.zeros(len(paper_mapping), dtype=torch.bool)\n        for node in target_nodes:\n            if node in paper_mapping:\n                target_mask[paper_mapping[node]] = True\n        batch['paper'].target_mask = target_mask\n        \n        return batch\n    \n    def get_batches(self, indices, shuffle=True):\n        \"\"\"Generate batches from indices\"\"\"\n        if shuffle:\n            indices = indices[torch.randperm(len(indices))]\n        \n        for i in range(0, len(indices), self.batch_size):\n            batch_indices = indices[i:i + self.batch_size]\n            yield self.create_minibatch(batch_indices.tolist())\n\n# --- 4. Load Data ---\nprint(\"\\nðŸ“‚ Loading OGBN-MAG data...\")\ndata_dict = load_ogbn_simple('./data')\ndata = create_pyg_hetero_data(data_dict)\n\n# Get training indices\ntrain_idx = data.train_idx\nprint(f\"ðŸŽ¯ Training on {len(train_idx)} papers\")\n\n# --- 5. Simple Model ---\nprint(\"\\nðŸ§  Creating model...\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"   Device: {device}\")\n\nnum_classes = data.num_classes\nhidden_dim = 128\nheads = 4\n\n# Ensure compatibility\nif num_classes % heads != 0:\n    adjusted_classes = ((num_classes + heads - 1) // heads) * heads\n    print(f\"   Adjusting classes: {num_classes} â†’ {adjusted_classes}\")\n    num_classes = adjusted_classes\n\nclass SimpleHGT(torch.nn.Module):\n    def __init__(self, hidden_dim, out_dim, metadata, heads=4):\n        super().__init__()\n        self.conv1 = HGTConv(-1, hidden_dim, metadata, heads=heads)\n        self.conv2 = HGTConv(hidden_dim, out_dim, metadata, heads=heads)\n        self.dropout = torch.nn.Dropout(0.3)\n    \n    def forward(self, x_dict, edge_index_dict):\n        x_dict = self.conv1(x_dict, edge_index_dict)\n        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n        x_dict = {key: self.dropout(x) for key, x in x_dict.items()}\n        x_dict = self.conv2(x_dict, edge_index_dict)\n        return x_dict\n\nmodel = SimpleHGT(hidden_dim, num_classes, data.metadata(), heads=heads).to(device)\n\n# Initialize lazy parameters\nprint(\"   Initializing model parameters...\")\nwith torch.no_grad():\n    dummy_x_dict = {\n        'paper': torch.randn(10, data_dict['paper_features'].shape[1]).to(device),\n        'author': torch.randn(10, 128).to(device),\n        'field_of_study': torch.randn(10, 64).to(device)\n    }\n    dummy_edge_dict = {\n        edge_type: torch.randint(0, 10, (2, 20)).to(device)\n        for edge_type in data.edge_types\n    }\n    _ = model(dummy_x_dict, dummy_edge_dict)\n\nprint(\"âœ… Model parameters initialized!\")\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\nprint(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# --- 6. Training ---\nprint(\"\\nðŸƒ Training with simple sampling...\")\n\n# Create sampler\nsampler = SimpleBatchSampler(data, batch_size=128, num_neighbors=[10, 5])\n\ndef get_memory_usage():\n    \"\"\"Get current memory usage in GB\"\"\"\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / 1024 / 1024 / 1024\n\nfor epoch in range(1, 4):\n    print(f\"\\n=== Epoch {epoch} ===\")\n    model.train()\n    total_loss = 0\n    total_examples = 0\n    batch_count = 0\n    \n    for batch in sampler.get_batches(train_idx, shuffle=True):\n        batch_count += 1\n        if batch_count > 100:  # Limit batches for demo\n            break\n            \n        try:\n            # Move batch to device\n            batch = batch.to(device)\n            optimizer.zero_grad()\n            \n            # Forward pass\n            out_dict = model(batch.x_dict, batch.edge_index_dict)\n            \n            # Get target nodes\n            target_mask = batch['paper'].target_mask\n            paper_out = out_dict['paper'][target_mask][:, :data.num_classes]\n            paper_labels = batch['paper'].y[target_mask]\n            \n            # Calculate loss\n            loss = F.cross_entropy(paper_out, paper_labels)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            # Track metrics\n            batch_size = target_mask.sum().item()\n            total_loss += float(loss) * batch_size\n            total_examples += batch_size\n            \n            if batch_count % 20 == 0:\n                avg_loss = total_loss / total_examples\n                memory = get_memory_usage()\n                print(f\"   Batch {batch_count}: Loss={avg_loss:.4f}, Memory={memory:.1f}GB\")\n                \n        except Exception as e:\n            print(f\"   Error in batch {batch_count}: {e}\")\n            continue\n    \n    # Epoch summary\n    if total_examples > 0:\n        epoch_loss = total_loss / total_examples\n        memory = get_memory_usage()\n        print(f\"âœ… Epoch {epoch}: Loss={epoch_loss:.4f}, Memory={memory:.1f}GB, Batches={batch_count}\")\n\nprint(f\"\\nðŸŽ‰ Training complete!\")\nprint(f\"Final memory: {get_memory_usage():.1f}GB\")\n\n# Save model\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'hidden_dim': hidden_dim,\n    'num_classes': num_classes,\n    'heads': heads\n}, 'simple_sampling_hgt_model.pt')\n\nprint(\"ðŸ’¾ Model saved!\")\nprint(\"âœ… Minibatch training successfully implemented without complex dependencies!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}